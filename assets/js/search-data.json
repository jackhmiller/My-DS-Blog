{
  
    
        "post0": {
            "title": "PEFT with LoRA and QLoRA",
            "content": "Introduction . Since the inception of transfer learning, dozens of works have sought to make model adaptation more parameter- and compute-efficient. Yet it is still the case that one of main drawbacks for LLMs is that have to be fine-tuned for each downstream task, learning a different set of parameters. . [!Question] Why can we use relatively vanilla gradient descent algorithms (e.g., without strong regularization) to tune a model with hundreds of millions of parameters on datasets with only hundreds or thousands of labeled examples? . PEFT: Parameter Efficient Fine Tuning . LoRA . Low-Rank Adaptation, or LoRA1- which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. The most significant benefit comes from the reduction in memory and storage usage. . Refresher on Matrix Decomposition . SVD representation of a matrix is the singular value decomposition of any matrix A so that $A=USV^T$. . $U$ is the left singular vectors obtained by finding an orthonormal set of eigenvectors $A^TA$. | $S$ is a diagonal matrix of singular values, which are the square roots of the eigenvalues of $AA^T$ | V is the right singular vectors obtained by finding an orthonormal set of eigenvectors t$A^TA$ | . You can truncate the SVD of a higher-rank matrix $A$ to get the a low-rank approximation. This is done by setting all but the first $k$ largest singular values to zero, and using the first $k$ rows and columns of $U$ and $V$. The rank is the $k$ chosen. This works because singular values decrease exponentially with rank, with earlier singular values being much larger than later ones. . Low-Rank Parameterized Update Matrices . A neural network contains many dense layers which perform matrix multiplication. The weight matrices in these layers typically have full-rank. When adapting to a specific task, Aghajanyan et al. (2020)2 shows that the pre-trained language models have a low “instrisic dimension” and can still learn efficiently despite a random projection to a smaller subspace. In other words, there exists a low dimension reparameterization that is as effective for fine-tuning as the full parameter space. . [!note] An objective function’s intrinsic dimensionality describes the minimum dimension needed to solve the optimization problem it defines to some precision level. In the context of pretrained language models, measuring intrinsic dimensional will tell us how many free parameters are required to closely approximate the optimization problem that is solved while fine-tuning for each end task. . Inspired by this, the LoRA authors hypothesize the updates to the weights also have a low “intrinsic rank” during adaptation. . For a pretrained weight matrix $W_{0} in mathbb{R}^{d times k}$, the update is constrained by representing its with a low-rank decomposition $W_{0}+ bigtriangleup W = W_{0}+ BA$ where $B in mathbb{R}^{d times r}$, $A in mathbb{R}^{r times k}$, and the rank $r « min(d, k)$. During training, $W_0$ is frozen and does not receive gradient updates, while $A$ and $B$ are trainable parameters. So the forward pass yields: . h=W0x+▽Wx=W0x+BAxh = W_{0x}+ bigtriangledown Wx = W_{0x}+ BAxh=W0x​+▽Wx=W0x​+BAx . In other words, gradients during stochastic gradient descent are passed through the fixed pretrained model weights to the adapter, which is updated to optimize the loss function. LoRA augments a linear projection through an additional factorized projection. . Using the technique shown above, r(n + k) parameters have to be tuned during model adaption. Since r « min(n, k), this is much lesser than the number of parameters that would have to be tuned otherwise (nk). This reduces the time and space required to finetune the model by a large margin. Some numbers from the paper and our experiments are discussed in the sections below. . To take an extreme example, supposed the $W_O$ is of size 512x512. That is 512$^2$ parameters. On the other hand, using two matrices via LoRA to replace $W_O$ where $L_{1} in mathbb{R}^{512 times 1}$ and $L_{2} in mathbb{R}^{1 times 512}$, that is only 1024 total parameters. . Simple LoRA Implementation . import os os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;]=&quot;0&quot; import torch import torch.nn as nn import bitsandbytes as bnb from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM model = AutoModelForCausalLM.from_pretrained( &quot;bigscience/bloom-3b&quot;, torch_dtype=torch.float16, device_map=&#39;auto&#39;, ) tokenizer = AutoTokenizer.from_pretrained(&quot;bigscience/tokenizer&quot;) print(model) for param in model.parameters(): param.requires_grad = False # freeze the model - train adapters later if param.ndim == 1: # cast the small parameters (e.g. layernorm) to fp32 for stability param.data = param.data.to(torch.float32) model.gradient_checkpointing_enable() # reduce number of stored activations model.enable_input_require_grads() class CastOutputToFloat(nn.Sequential): def forward(self, x): return super().forward(x).to(torch.float32) model.lm_head = CastOutputToFloat(model.lm_head) def print_trainable_parameters(model): &quot;&quot;&quot; Prints the number of trainable parameters in the model. &quot;&quot;&quot; trainable_params = 0 all_param = 0 for _, param in model.named_parameters(): all_param += param.numel() if param.requires_grad: trainable_params += param.numel() print( f&quot;trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}&quot; ) # Obtain LoRA Model from peft import LoraConfig, get_peft_model config = LoraConfig( r=8, lora_alpha=16, target_modules=[&quot;query_key_value&quot;], lora_dropout=0.05, bias=&quot;none&quot;, task_type=&quot;CAUSAL_LM&quot; ) model = get_peft_model(model, config) print_trainable_parameters(model) # Load sample data from datasets import load_dataset qa_dataset = load_dataset(&quot;squad_v2&quot;) # Train LoRA import transformers trainer = transformers.Trainer( model=model, train_dataset=mapped_qa_dataset[&quot;train&quot;], args=transformers.TrainingArguments( per_device_train_batch_size=4, gradient_accumulation_steps=4, warmup_steps=100, max_steps=100, learning_rate=1e-3, fp16=True, logging_steps=1, output_dir=&#39;outputs&#39;, ), data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False) ) model.config.use_cache = False # silence the warnings. Please re-enable for inference! trainer.train() . 3 . Where LoRA Falls Short . While LoRA was designed as a PEFT method, most of the memory footprint for LLM finetuning comes from activation gradients and not from the learned LoRA parameters. For example, for a 7B LLaMA model trained on FLAN v2 with a batch size of 1, with LoRA weights equivalent to commonly used 0.2% of the original model weights, the LoRA input gradients have a memory footprint of 567 MB while the LoRA parameters take up only 26 MB. . QLoRA- Efficient Finetuning of Quantized LLMs . QLoRA reduces the average memory needs of finetuning a 65B parameter model from &gt;780GB of GPU RAM to 48GB without sacrificing runtime or predictive performance.4 This is done via an algorithm developed by researchers at the University of Washington that quantizes a pretrained model using to a 4-bit resolution before adding a sparse set of learnable Low-rank Adapter weights modified by backpropagating gradients through the quantized consequences. As a result, QLoRA has made the largest publicly available models to date fine-tunable on a single GPU. . What is Quantization? . Quantization is the process of discretizing an input from a representation that holds more information to a representation with less information. It often means taking a data type with more bits and converting it to fewer bits, for example from 32-bit floats to 8-bit Integers. To ensure that the entire range of the low-bit data type is used, the input data type is commonly rescaled into the target data type range through normalization by the absolute maximum of the input elements, which are usually structured as a tensor. . This is important since to calculate the model size in bytes, one multiplies the number of parameters by the size of the chosen precision in bytes. . Example 1 Let say we want to go from float32 to int8. . array = [-1024, 5, 2048, 256] # as type float32 . To do the conversion, we will multiply each element in the array by a quantization factor c. To calculate c , we first find the max bit number for the desired conversion type (so in this case for int8 it is 127 since the range of int8 is [-127, 127]), and then divide it by the max number of the array. . Then, for each item in our array, we divide the item by the quantization factor and round, thus achieving quantization albeit with an expected loss of precision. . &gt;&gt;&gt; c = 127/max(array) &gt;&gt;&gt; c 0.62 &gt;&gt;&gt; quantized = [round(i/c) for i in array] &gt;&gt;&gt; quantized [-64, 0, 127, 16] . Example 2 - why is this problematic Using the same conversion instead for [0, 100, 100000], you will get 0 for the first two elements of the array, leading to a lot of information loss. The problem with this approach is that if a large magnitude value (i.e., an outlier) occurs in the input tensor, then the quantization bins—certain bit combinations—are not utilized well with few or no numbers quantized in some bins . QLoRA’s Solution . QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimizers to manage memory spikes.   4-bit NormalFloat quantization improves upon quantile quantization by ensuring an equal number of values in each quantization bin. This avoids computational issues and errors for outlier values. During finetuning, QLoRA backpropagates gradients through the frozen 4-bit quantized pretrained language model into the Low-Rank Adapters. QLoRA dequantizes weights from the 4-bit NormalFloat (what the authors call storage data type) to the 16-bit BrainFloat (computation data type) to perform the forward and backward passes, but only computes weight gradients for the LoRA parameters which use 16-bit BrainFloat. The weights are decompressed only when they are needed, therefore the memory usage stays low during training and inference. . Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, &amp; Weizhu Chen (2021). LoRA: Low-Rank Adaptation of Large Language Models_. CoRR, abs/2106.09685. https://arxiv.org/pdf/2106.09685.pdf &#8617; . | Armen Aghajanyan, Luke Zettlemoyer, &amp; Sonal Gupta (2020). Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning_. CoRR, abs/2012.13255, https://arxiv.org/abs/2012.13255. &#8617; . | https://colab.research.google.com/drive/1iERDk94Jp0UErsPf7vXyPKeiM4ZJUQ-a?usp=sharing#scrollTo=kfAO01v-qEPS &#8617; . | Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, &amp; Luke Zettlemoyer. (2023). QLoRA: Efficient Finetuning of Quantized LLMs. &#8617; . |",
            "url": "https://jackhmiller.github.io/My-DS-Blog/2023/06/22/PEFT-LORA-QLORA.html",
            "relUrl": "/2023/06/22/PEFT-LORA-QLORA.html",
            "date": " • Jun 22, 2023"
        }
        
    
  
    
        ,"post1": {
            "title": "Trying to Make Sense of Neural Network Generalization",
            "content": "I am sure that by now you have heard the jokes and seen the memes that AI is essentially just a bunch of if else statements. Take this tweet below: . &quot;Uber will use artificial intelligence to identify drunk passengers. It will use factors like time of day, pickup location, and how long a user interacts with the app before making a decision.&quot;That&#39;s not AI.That&#39;s an if statement. . &mdash; @Nick_Craver@infosec.exchange (@Nick_Craver) June 8, 2018 Obviously this is a gross oversimplification of what AI can do. But because of all of the hype, many have been quick to belittle or poke holes the actual power of AI applications like deep learning. In my opinion, a lot of this stems from the fact that even several years after the mainstream adoption of deep learning applications and capabilities, they still represent a black box system: a model takes millions of data points as inputs and correlates specific data features to produce an output. That process is largely self-directed and is generally difficult for data scientists, programmers and users to interpret. . Similarly, this difficulty can be stated from a more theoretical perspective: On an infinite input domain like $ mathbb{R}^n$, no finite number of samples is enough to uniquely determine a single function, even approximately. Even accounting for bounds and discretization of the input, and even for the symmetries our architectures impose on the output function, our training sets are microscopic compared to the sizes of our input domains. The problem that neural networks successfully solve every day should be impossible. . A Lack of Theory . This lack of explainability has given rise to a subset of deep learning that specifically seeks to address and develop methods to make machine and deep learning systems more understandable. The most notable would be , SHAP (SHapley Additive exPlanations) a game theoretic approach to explain the output of any machine learning model. Yet these efforts have not addressed the underlying yet pronounced lack of theory explaining why deep learning works so well. Take for example, this excerpt from a paper aptly titled “Why does deep and cheap learning work so well?”: . However, it is still not fully understood why deep learning works so well. In contrast to GOFAI (“good old-fashioned AI”) algorithms that are hand-crafted and fully understood analytically, many algorithms using artificial neural networks are understood only at a heuristic level, where we empirically know that certain training protocols employing large data sets will result in excellent performance. . Put even more bluntly in a NeurIPS 2017 presentation: . Batch Norm is a technique that speeds up gradient descent on deep nets… Batch Norm has become a foundational operation for machine learning. It works amazingly well. But we know almost nothing about it. . Clearly, this was a somewhat lighthearted talk that didn’t put heavy emphasis on nuance. The claim that “we know almost nothing” about Batch Norm was probably not meant to be taken literally. However, it’s been taken both seriously and literally by much of our community as a statement about our then-current (i.e. as of 2017) best scientific understanding. But it is hard to give a succinct summary of what Batch Norm does, because there are multiple effects going on, and which ones are important in any particular case will depend on the specific task, optimization algorithm, and architecture. . This example applies to the overall performance of neural networks, where despite their massive size, successful deep artificial neural networks often exhibit a remarkably small difference between training and test performance. The lack of a concise and singular explanation for this ability, coupled with a dearth of theoretical study of individual components, has allowed observers to claim that “deep learning is not as impressive as you think because it’s mere interpolation resulting from glorified curve fitting.” . What is Interpolation (and Extrapolation)? . The success of deep learning has led to a lot of recent interest in understanding the properties of “interpolating” neural network models, that achieve (near-)zero training loss. Interpolation was first defined as predicting the possible realization of a stationary Gaussian process at a time position lying in-between observations. This is not to be confused with “extrapolation”, defined as predicting the future (realization) of a stationary Gaussian process based on past and current realizations. It is critical to distinguish between interpolation, and the idea of an interpolation regime, which occur whenever the model has 0 training loss on the data. . Interpolation and extrapolation provide an intuitive geometrical characterization on the location of new samples with respect to a given dataset. Those terms are commonly used as geometrical proxy to predict a model’s performances on unseen samples and many have reached the conclusion that a model’s generalization performance depends on how a model interpolates. In other words, how accurate is a model within a dataset’s convex-hull determines its generalization performances. . To put it differently, conventional wisdom in machine learning offers the following about generalization: . A model that is too simple will underfit the true patterns in the training data, and thus, it will predict poorly on new data. | A model that is too complicated will overfit spurious patterns in the training data; such a model will also predict poorly on new data. | So then how can overparameterized and overfit deep learning models achieve good generalization? Furthermore, it has been proven that the behavior of a model within a training set’s convex hull barely impacts that model’s generalization performance since new samples lie almost surely outside of that convex hull. This observation holds whether we are considering the original data space, or embeddings. The argument is essentially based on the well-known curse of dimensionality: in high dimension, every point is far from another. For example, in order to maintain a constant probability to be in interpolation regime, the training set size has to increase exponentially with $ mathbb{D}$ regardless of the underlying intrinsic manifold dimension where $ mathbb{D}$ is the dimension of the lowest dimensional affine subspace including the entire data manifold i.e. the convex hull dimension. Thus in high dimension, any new point almost surely lie outside of the training set convex hull. Therefore, finding samples in interpolation regime becomes exponentially difficult with respect to the considered data dimension, and dimensionality reduction methods loose the interpolation/extrapolation information and lead to visual misconceptions skewed towards interpolation. . Is it then a (mis)conception that state-of-the-art algorithms work so well because of their ability to correctly interpolate training data? If this is the case, then how come deep learning models generalize at all? Shouldn’t they just learn an ad hoc mapping between training inputs and targets, like a fancy dictionary? What expectation can we have that this mapping will work for new inputs? . How does this relate to a model’s ability to generalize? . The critical component here that allows us to resolve the apparent contradiction between the inability to interpolate in high dimensional setting and a neural network’s ability to generalize, is the idea of a manifold. A “manifold” is simply a lower-dimensional subspace of some parent space that is locally similar to a linear (Euclidian) space. (i.e. it is continuous and smooth). The fact that this property (latent space = very small subspace + continuous &amp; structured) applies to so many problems is called the manifold hypothesis. More generally, the manifold hypothesis posits that all natural data lies on a low-dimensional manifold within the high-dimensional space where it is encoded. This concept is central to understanding the nature of generalization in ML. . The manifold hypothesis implies that . Machine learning models only have to fit relatively simple, low-dimensional, highly structured subspaces within their potential input space (latent manifolds). | Within one of these manifolds, it’s always possible to interpolate between two inputs, that is to say, morph one into another via a continuous path along which all points fall on the manifold. | . As a result, you can use a small subset of the latent space to fit a curve that approximately matches the latent space. See this tweet of François Chollet from an epic and extremely informative twitter thread: . It&#39;s only with a sufficiently dense sampling of the latent manifold that it becomes possible to make sense of new inputs by interpolating between past training inputs without having to leverage additional priors. pic.twitter.com/SmRvEN2NXS . &mdash; François Chollet (@fchollet) October 19, 2021 So deep learning achieves (local) generalization via interpolation on a learned approximation of the data manifold. But what does this mean, from a practical perspective? . Universal Approximation Theorem . The theorem states, in essence, that under a few conditions, any continuous function f can be modelled with a neural network with one hidden layer and a sufficient number of units. One of the requirements is that the activation function must have some form of nonlinearity. The Universal Approximation Theorem at most implies that neural networks are a reasonable model, but does not suggest that they are superior to other methods (e.g. polynomials, splines, wavelets, Fourier expansions, …). . There’s some further ways to break down the Universal approximation theorem. For example, it requires the activation function be nonconstant, bounded, and monotonically-increasing continuous function. Take the case of the sigmoidal activation, which asymptotes out to 0 as x→−∞ and to 1 as x→+∞. One of the original motivations for using the sigmoidal activation was that it can act like a switch. Allowing the weights to have arbitrary magnitude, i.e. no hard limit on the size of the parameter values, we can effectively shift sigmoids into their “off” states with very negative values and shift sigmoids into their “on” states with very positive values. And training a single hidden layer corresponds to learning a good configuration of parameters. By allowing for the weights to have unbounded size (both in the negative and positive sense), we can interpret the single hidden layer NN as partitioning the domain into sub-spaces where a specific configuration of the sigmoidals are “on” and contribute to the function approximation and the others are switched “off.” Now if we allow ourselves to have a ton of these sigmoids, you start to get some intuition as to why the sigmoidal Universal Approximation Theorem is true. . From the Universal Approximation Theorem, we understand that neural networks may not be ‘intelligent’ in the sense that humans often think of intelligence, but are complex estimators hidden under a guise of multidimensionality, which makes its ability — which would seem ordinary in two or three dimensions — seem impressive. . Another way to think about it is to consider a neural network as a kind of ensemble. Every neural network has a pre-ultimate layer that you can think of as a collection of intermediate representations of your problem. The outputs of this layer are then aggregated (usually using a dense layer) for making the final prediction. This is like ensembling many smaller models. Again, if the smaller models memorized the data, even if each would overfit, by aggregating them, the effects would hopefully cancel out. . Putting it all Together . Each of these above mentioned intermediate layers, where we can learn a via a manifold of the latent space, allows us to separate the input space into exponentially more linear response regions than shallow networks, despite using the same number of computational units. This means that representational capacity scales with depth. Put another way, deep networks are able to identify an exponential number of input neighborhoods by mapping them to a common output of some intermediary hidden layer. The computations carried out on the activations of this intermediary layer are replicated many times, once in each of the identified neighborhoods. . From a geometric perspective, each hidden layer of a deep neural network can be associated with a folding operator. Each hidden layer folds the space of activations of the previous layer. In turn, a deep neural network effectively folds its input-space recursively, starting with the first layer. The consequence of this recursive folding is that any function computed on the final folded space will apply to all the collapsed subsets identified by the map corresponding to the succession of foldings. This means that in a deep model any partitioning of the last layer’s image-space is replicated in all input-space regions which are identified by the succession of foldings. Space foldings are not restricted to foldings along coordinate axes and they do not have to preserve lengths. Instead, the space is folded depending on the orientations and shifts encoded in the input weights W and biases b and on the nonlinear activation function used at each hidden layer. . Consider a ReLU unit assigned to one of the hidden layers of a network, represented as follows: zk=ReLU(wk⋅x+bk)z_{k}= ReLU(w_{k} cdot x + b_k)zk​=ReLU(wk​⋅x+bk​) The area in the parenthesis (input to the ReLU) is the equation of a hyperplane: . $w_K$ is the orientation of the hyperplane | $b_k$ is the offset | . This hyperplane is dividing the input space into two halves, one where the ReLU has a positive output, and the other where the ReLU output is zero. That is only for a single $k$ in a single layer- so for all $k$’s in a layer, it is going to put down multiple hyperplanes in high dimensional space to partition up the input into convex polytopes (polygons in high dimension). The below image shows the different hyperplanes of different $k$’s for a single layer, with the decision boundary as the red line and the different polytopes as shaded regions. . For a second hidden layer, the hyperplanes are folded by the previous layer’s hyperplanes to maintain continuity in mapping, through to the decision boundary: . So what is happening here is not alchemy. Rather, the model is applying a hyperplane for each neuron in each hidden layer, by utilizing subsequent manifolds of the latent space that represent the output of one layer and the input to the subsequent layer. The corresponding multidimensional decision boundaries layer by layer are interpolated together, much akin to how a spline behaves, and then pulled together in the final layer to create a decision boundary. . Source: Berner, Julius &amp; Grohs, Philipp &amp; Kutyniok, Gitta &amp; Petersen, Philipp. (2021). The Modern Mathematics of Deep Learning. . In summary, each neuron splits the space into two affine linear regions separated by a hyperplane. A shallow ReLU network with $n$ neurons in the hidden layer therefore produces a number of regions defined by $n$ hyperplanes. Therefore deepening neural networks correspond to certain folding of the input space. Through this interpretation it can be seen that composing neural networks can lead to a multiplication of the number of regions of the individual networks, resulting in an exponential efficiency of deep neural networks in generating affine linear regions. . . Sources: [^1]: Montúfar, G., Pascanu, R., Cho, K., &amp; Bengio, Y.. (2014). On the Number of Linear Regions of Deep Neural Networks. [^2]: Henry W. Lin, Max Tegmark, &amp; David Rolnick (2017). Why Does Deep and Cheap Learning Work So Well?. Journal of Statistical Physics, _168(6), 1223–1247. [^3]: P. H. P. Savarese, I. Evron, D. Soudry, and N. Srebro, “How do infinite width bounded norm networks look in function space?” in Conference on Learning Theory, COLT 2019, 25-28 June 2019, Phoenix, AZ, USA, 2019, pp. 2667–2690 [^4]: A Spline Perspective of Deep Learning - Richard Baraniuk - FFT Feb 28th, 2022 https://www.youtube.com/watch?v=5AMxhdj-96Q [^6]: https://github.com/slundberg/shap [^8]: Ali Rahimi. Back when we were young. NeurIPS Test of Time Talk, 2017. [^9]: Wiener, N. (1949). Extrapolation, interpolation, and smoothing of stationary time series, with engineering applications. [^10]: Chatterji, N. S., Long, P. M., and Bartlett, P. L. (2021). When does gradient descent with logistic loss find interpolating two-layer networks? Journal of Machine Learning Research, 22(159):1–48. [^11]: Balestriero, R., Pesenti, J., &amp; LeCun, Y.. (2021). Learning in High Dimension Always Amounts to Extrapolation. [^12]: A feedforward network with a linear output layer and at least one hidden layer with any “squashing” activation function (such as the logistic sigmoid activation function) can approximate any function from one finite-dimensional space to another with any desired non-zero amount of error, provided that the network is given enough hidden units .",
            "url": "https://jackhmiller.github.io/My-DS-Blog/2021/12/11/Interpolation.html",
            "relUrl": "/2021/12/11/Interpolation.html",
            "date": " • Dec 11, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Neural Tangent Kernel",
            "content": "Introduction . Much of the research on deep learning theory over the past few years addresses the common theme of analyzing neural networks in the infinite-width limit. At first, this limit may seem impractical and even pointless to study. However, it turns out that neural networks in this regime simplify to linear models with a kernel called the neural tangent kernel. These results are significant as they give a way of understanding why neural networks converge to a optimal solution. Gradient descent is therefore very simple to study, leads to a proof of convergence of gradient descent to 0 training loss. Neural networks are know to be highly non-convex objects and so understanding their convergence under training is highly non-trivial. . In the post, we will do a deep dive into the motivation and definition of NTK and how it can be used to explain the evolution of neural networks during training via gradient descent. . Motivation . Following the popularization of deep learning beginning in the late 2010s, a series of papers were published where it was shown that overparametrized neural networks could converge linearly to zero training loss with their parameters hardly varying.1 This culminated in a a 2020 paper titled “On Lazy Training in Differentiable Programming,” where the authors coin the phrase lazy training, which corresponds to the model behaving like its linearization around the initialization.2 This can be proven quantitatively, by looking at the relative change in the norm of the weight vector from initialization: . ∥w(n)−w0∥2∥w0∥2 frac{ left | w(n)-w_0 right |_2}{ left | w_0 right |_2}∥w0​∥2​∥w(n)−w0​∥2​​ . Lets look at a simple 2-hidden layer ReLU network, with varying widths. . As we can see from the results above, training loss goes to zero for all networks, yet for the widest network, the weights barely move! It appears the approximation is a linear model in the weights. Does that mean that minimizing the least squares loss reduces to just doing linear regression? . Well no, since the model function is still non-linear in the input, because finding the gradient of the model is definitely not a linear operation. In fact, this is just a linear model using a feature map $ phi(x)$ which is the gradient vector at initialization. This feature map naturally induces a kernel on the input, which is called the neural tangent kernel (NTK). . What is a Kernel? . A kernel is essentially a similarity function between two data points. It describes how sensitive the prediction for one data sample is to the prediction for the other; or in other words, how similar two data points are. Depending on the problem structure, some kernels can be decomposed into two feature maps, one corresponding to one data point, and the kernel value is an inner product of these two features: . K(x,x′)=⟨ϕ(x),ϕ(x′)⟩K(x, x&amp;#x27;) = left langle phi(x), phi(x&amp;#x27;) right rangleK(x,x′)=⟨ϕ(x),ϕ(x′)⟩ . Kernel methods are a type of non-parametric, instance-based machine learning algorithms. . A Simple Illustration . Consider a linear function $f(x, theta) = theta_{1}x + theta_2$. Like in the case of a neural network, we will initialize our parameters $ theta$s. We will then conduct a forward pass, calculate the loss function, and then propagate backwards in order to adjust our parameters $ theta$. Since our function $f$ is not parametrized as lookup tables of individual function values, changes our $ theta s$ based on a single training iteration will change the parameters for all of our observations. . This is why the neural tangent kernel is useful; at its core, it explains how updating the model parameters on one data sample affects the predictions for other samples. . Theoretical Justification for Linear Approximation? . The linearized model is great for analysis, only if it’s actually an accurate approximation of the non-linear model. Chizat and Bach3 defined the condition where the local approximation applies, leading to the kernel regime: . ∥y(w0)−yˉ∥∥▽w2y(w0)∥∥▽wy(w0)∥2≪1(1) left | y(w_0) - bar{y} right | frac{ left | bigtriangledown_w^2 y(w_0) right |}{ left | bigtriangledown _w y(w_0) right |^2} ll 1 tag{1}∥y(w0​)−yˉ​∥∥▽w​y(w0​)∥2∥∥∥​▽w2​y(w0​)∥∥∥​​≪1(1) . In words, if the Hessian divided by the squared norm of the gradient is less than 1, the gradient dynamics track very closley with gradient dynamics on a kernel machine. Put another way, if the condition above holds, it means that is little to no movement in the weights as there is no negative curvature. So how much the Hessian changes is very small relative to how much the gradient is changing. This is key, as we only change parameters slightly (if at all) and achieve a large change in predictions. This means that we obtain a linear behavior in a small region around initialization.4 . Based on this equation, a key condition can be summarized as follows: the amount of change in $w$ to produce a change of ∥y(w0)−yˉ∥ left | y(w_0) - bar{y} right |∥y(w0​)−yˉ​∥ in $y$ causes a negligible change in the Jacobian $ bigtriangledown _w y(w_0)$. What is key now is to understand how the quantity in (1) which we will now represent as $k(w_0)$ changes with the hidden width $m$ of our network. . Well, it turns out based on the research5 that $k rightarrow 0$ as $m rightarrow infty$ . This means that the model is very close to its linear approximation. . An intuitive explanation for why this happens is as follows: a large width means that there are a lot more neurons affecting the output. A small change in all of these neuron weights can result in a very large change in the output, so the neurons need to move very little to fit the data. If the weights move less, the linear approximation is more accurate. As the width increases, this amount of neuron budging decreases, so the model gets closer to its linear approximation. . Neural Tangent Kernel (NTK) . So if the model is close to its linear approximation, ($k(w_{0}) ll 1$), the Jacobian of the model outputs does not change as training progresses. This means that $ bigtriangledown y(w(t)) approx bigtriangledown y(w_0)$. This is referred to as the kernel regime, because the tangent kernel stays constant during training. The training dynamics now reduces to a very simple linear ordinary differential equation: $ bigtriangledown y(w) = -H(w_{0})(y(w)- bar{y})$ where $H$ is the NTK $ bigtriangledown y(w)^{T} bigtriangledown y(w)$ . Lets try and derive it step by step. . Our loss function can be represented as: . L(θ)=1N∑i=1Nl(f(xi,θ);yi)(2)L( theta) = frac{1}{N} sum_{i=1}^{N} l(f(x^{i}, theta);y^i) tag{2}L(θ)=N1​i=1∑N​l(f(xi,θ);yi)(2) . And its using the chain rule the gradient can be represented as . ▽θL(θ)=1N∑i=1N▽θf(xi;θ)▽fl(f,yi)(3) bigtriangledown_ theta L( theta) = frac{1}{N} sum_{i=1}^{N} bigtriangledown_{ theta}f(x^{i}; theta) bigtriangledown_{f}l(f, y^i) tag{3}▽θ​L(θ)=N1​i=1∑N​▽θ​f(xi;θ)▽f​l(f,yi)(3) . Where the size of the first gradient term is $P$x$n_L$ where $P$ is the number of parameters in the network and $n_L$ is the number of layers. The size of the second gradient term is of size $n_L$x$1$. . If we take the learning rate to be infinitesimally small, we can look at the evolution of the weight vectors over time, and write down this differential equation: . dθdt=−▽θL(θ)(4) frac{ mathrm{d theta} }{ mathrm{d} t} = - bigtriangledown_ theta L( theta) tag{4}dtdθ​=−▽θ​L(θ)(4) . This is called a gradient flow. In essence, it is a continuous time equivalent of standard gradient descent. The main point is that the trajectory of gradient descent in parameter space closely approximates the trajectory of the solution of this differential equation if the learning rate is small enough. Again this is because when tracking how the network parameter $ theta$ evolves in time, each gradient descent update introduces a small incremental change of an infinitesimal step size. . Now we can express how the network output evolves over time according to the derivative: . df(x;θ)dt=df(x;θ)dθdθdt=−1N∑i=1N▽θf(x;θ)T▽θf(xi;θ)▽fl(f,yi)(5) frac{ mathrm{df(x; theta)} }{ mathrm{d} t} = frac{ mathrm{df(x; theta)} }{ mathrm{d} theta} frac{ mathrm{d theta} }{ mathrm{d} t} = - frac{1}{N} sum_{i=1}^{N} { color{Red} bigtriangledown_{ theta}f(x; theta)^{T} bigtriangledown_{ theta}f(x^i; theta)} bigtriangledown_{f}l(f, y^i) tag{5}dtdf(x;θ)​=dθdf(x;θ)​dtdθ​=−N1​i=1∑N​▽θ​f(x;θ)T▽θ​f(xi;θ)▽f​l(f,yi)(5) . Where the red component in (5) is the NTK, $K(x, x’; theta) = bigtriangledown_{ theta}f(x; theta)^{T} bigtriangledown_{ theta}f(x’; theta)$. This means that the feature map of one input $x$ is $ phi(x) = bigtriangledown_{ theta}f(x; theta)$. This is because the NTK matrix corresponding to this feature map is obtained by taking pairwise inner products between the feature maps of all the data points. . Further, since our model is over-parameterized, the NTK matrix is always positive definite. By performing a spectral decomposition on the positive definite NTK, we can decouple the trajectory of the gradient flow into independent 1-d components (the eigenvectors) that decay at a rate proportional to the corresponding eigenvalue. The key thing is that they all decay (because all eigenvalues are positive), which means that the gradient flow always converges to the equilibrium where train loss is 0. . Why is this interesting? . It turns out the neural tangent kernel becomes particularly useful when studying learning dynamics in infinitely wide feed-forward neural networks. Why? Because in this limit, two things happen: . First: if we initialize $ theta_0$ randomly from appropriately chosen distributions, the initial NTK of the network $k_{ theta_0}$ approaches a deterministic kernel as the width increases. This means, that at initialization, $k_{ theta_0}$ doesn’t really depend on $ theta_0$ but is a fixed kernel independent of the specific initialization. | Second: in the infinite limit the kernel $k_{ theta_0}$ stays constant over time as we optimize $ theta_0$. This removes the parameter dependence during training. | Further reading: . https://rajatvd.github.io/NTK/ | https://lilianweng.github.io/posts/2022-09-08-ntk/ | Simon S. Du, Xiyu Zhai, Barnabás Póczos, and Aarti Singh. Gradient descent provably optimizes over-parameterized neural networks. In International Conference on Learning Representations, 2019; Simon S. Du, Lee Jason D., Li Haochuan, Wang Liwei, and Zhai Xiyu. Gradient descent finds global minima of deep neural networks. In International Conference on Machine Learning (ICML), 2019; Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-parameterization. In Proceedings of the 36th International Conference on Machine Learning, volume 97, pages 242–252, 2019; Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient descent on structured data. In Advances in Neural Information Processing Systems, pages 8167–8176, 2018. &#8617; . | Chizat, Lenaic, and Francis Bach. “A note on lazy training in supervised differentiable programming.” arXiv preprint arXiv:1812.07956 (2018). &#8617; . | Lenaic Chizat, &amp; Francis Bach. (2018). On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport. &#8617; . | https://www.youtube.com/watch?v=l0im8AJAMco &#8617; . | Jacot, Arthur, Franck Gabriel, and Clément Hongler. “Neural tangent kernel: Convergence and generalization in neural networks.” Advances in neural information processing systems. 2018; - Chizat, Lenaic, and Francis Bach. “A note on lazy training in supervised differentiable programming.” arXiv preprint arXiv:1812.07956 (2018); - Arora, Sanjeev, et al. “On exact computation with an infinitely wide neural net.” arXiv preprint arXiv:1904.11955 (2019); - Li, Zhiyuan, et al. “Enhanced Convolutional Neural Tangent Kernels.” arXiv preprint arXiv:1911.00809 (2019); Lee &amp; Xiao, et al. “Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent.” NeuriPS 2019. &#8617; . |",
            "url": "https://jackhmiller.github.io/My-DS-Blog/2021/10/02/NTK.html",
            "relUrl": "/2021/10/02/NTK.html",
            "date": " • Oct 2, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "The Log-Sum-Exp Trick",
            "content": ". The Log-Sum-Exp Trick . Problem Setting . When I was switching deep learning frameworks form Tensorflow to Pytorch, I noticed something interesting when building classification models; the sigmoid (for binary classification) or softmax (for multiclass classification) in the last layer of the neural network was not applied in the forward() method. In this post, we will understand why that is the case. . In statistical modeling and machine learning, work in a logarithmic scale is often preferred. For example, when $x$ and $y$ are small numbers, multiplying them together can cause underflow. However, if we convert to a logarithmic scale, we can convert multiplication to addition: log(xy) = log(x) + log(y) tag{1} This is just one reason that working with quantities such as log likelihoods and log probabilities is often preferred. For a more detailed example, consider computing a matrix determinant. This is a routine computation in many standard libraries like SciPy. To compute the determinant of matrix $ sum$ , these libraries use the fact that for a $D$x$D$ matrix $M$ with eigenvalues $ lambda_{1,}…, lambda_{D}$, the determinant is equal to the product of the eigenvalues or: det(M) = prod_{d=1}^D lambda_d tag{2} However, computing the determinant this way can be numerically unstable, since if $ lambda_n$ is small, the computed determinant might be rounded to 0 due to our computer’s floating point precision. And taking the log of 0 will result in -inf. . Lets do this in code: . &gt;&gt;&gt;import numpy as np &gt;&gt;&gt;A = np.ones(100) * 1e-5 &gt;&gt;&gt;np.linalg.det(np.diag(A)) 0.0 . Floating Point Precision with Log Likelihoods . Recalling equation 2, where the determinant of a diagonal matrix M is the product of the elements along its diagonal, we can take the log of this, resulting in the following: . log(det(M)) = log( prod_{d=1}^D lambda_{d}) tag{3} which is equal to = sum_{d}^{D} log( lambda_i) tag{4} If we compute equation 4 instead of 3, we might avoid an issue with floating point precision because we’re taking the log of much bigger numbers and then adding them together. Lets take a look in python: . &gt;&gt;&gt;A = np.ones(100) * 1e-5 &gt;&gt;&gt;np.linalg.det(np.diag(A)) 0.0 &gt;&gt;&gt;np.log(A).sum() -1151.2925464970228 . We can check ourselves to see if our calculations give us the same number: . &gt;&gt;&gt;A = np.ones(100) * 2 &gt;&gt;&gt;np.log(np.linalg.det(np.diag(A))) 69.31471805599459 &gt;&gt;&gt;np.log(A).sum() 69.31471805599453 . The Solution . With the above concepts in mind, consider the log-sum-exp operation: LSE(x_1,...,x_{N)}= log( sum_{n=1}^Nexp(x_n)) tag{5} . Consider the softmax activation function: p_{i}= frac{exp(x_i)}{ sum limits_{n=1}^{N}exp(x_{n})} tag{6} where $ sum limits_{n=1}^{N} p_{n}= 1$. Since each $x_n$ is a log probability that might be very large and either negative or positive, then exponentiating might result in under or overflow. Therefore, we will seek to rewrite the denominator to avoid this issue. First, lets rewrite the summation term in equation 6 as exp(x_{i}) = p_i sum_{n=1}^Nexp(x_n) tag{7} x_{i}= log(p_{i})+ log( sum_{n=1}^{N}exp(x_n)) tag{8} log(p_{i})= x_{i}- log( sum_{n=1}^{N}exp(x_n)) tag{9} $$ p_i= exp(x_{i}- underset{LSE}{log( sum_{n=1}^{N}exp(x_n))}) . tag{10} WecanseethatwehavetheLSEfromequation5.Soagain,whatwehavedoneisperformthenormalizationin(6)usingthelog−sum−expin(5).Whatisniceabout(5)whichwedidnotmentionisthatitcanbeshowntoequal:We can see that we have the LSE from equation 5. So again, what we have done is perform the normalization in (6) using the log-sum-exp in (5). What is nice about (5) which we did not mention is that it can be shown to equal:WecanseethatwehavetheLSEfromequation5.Soagain,whatwehavedoneisperformthenormalizationin(6)usingthelog−sum−expin(5).Whatisniceabout(5)whichwedidnotmentionisthatitcanbeshowntoequal: y=log( sum_{n=1}^Nexp(x_n)) tag{5} e^{y}= sum_{n=1}^Nexp(x_n) e^{y} = e^c sum_{n=1}^Nexp(x_n-c) y=c+log sum_{n=1}^{N}exp(x_n-c) tag{11} $$ This means, you can shift the center of the exponential sum by an arbitrary constant c while still computing the same final value. Critically, we’ve been able to create a term that doesn’t involve a log or exp function. Now all that’s left is to pick a good value for c that works in all cases. It turns out $c = max(x_{1}, …, x_{N})$ works really well. . Lets try it out! Here is an example of how we can use the log-sum-exp to deal with a case of overflow: . &gt;&gt;&gt;x=np.array([1000, 1000, 1000]) &gt;&gt;&gt;np.exp(x) array([inf, inf, inf]) . Now for the log-sum-exp: . def logsumexp(x): c = x.max() return c + np.log(np.sum(np.exp(x-c))) &gt;&gt;&gt;logsumexp(x) 1001.0986122886682 &gt;&gt;&gt;np.exp(x-logsumexp(x)) array([0.33333333, 0.33333333, 0.33333333]) .",
            "url": "https://jackhmiller.github.io/My-DS-Blog/2021/03/01/Log-Sum-Exp-Trick.html",
            "relUrl": "/2021/03/01/Log-Sum-Exp-Trick.html",
            "date": " • Mar 1, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Tweedie Distribution",
            "content": "Is that a typo? . The Tweedie distribution is a three-parameter family of distributions that is a special case of exponential dispersion models, but is a generalization of several familiar probability distributions, including the normal, gamma, inverse Gaussian and Poisson distributions. The distribution along with exponential dispersion models were introduced by Jørgensen in 1987.1 According to its Wikipedia page, the unusually named distribution was named as such by Jørgensen after Maurice Tweedie, a statistician and medical physicist at the University of Liverpool, UK, who presented the first thorough study of these distributions in 1984.2 . Ok… So what? . You are now asking yourself; How did I come across such an oddly-named distribution, and why are you writing about it? . Well, the application of the Tweedie distribution primarily involves regression problems with an extreme class imbalance. How extreme? The majority class is at least 90% of the data. So that is really imbalanced. Having dealt with class-imbalance problems in a classification setting, I was interested in a regression use-case. But why do we need a special distribution? Couldn’t a Poisson distribution work, or even a zero-inflated hurdle model? . As it turns out, the answer to those questions is “yes, but you can do better with the Tweedie distribution.” Let’s find out why. . Clarifying our use-case . Imagine a not so far-fetched scenario were we have a cluster of data items at zero, and a right long-tailed distribution. We can clearly not fit a normal distribution to it as the data would consist of a large peak at zero and continuous positive values. Poisson is also not a suitable candidate as we are not dealing with the count data alone. On the other hand, Gamma distribution does not take zero values. This is where the Tweedie distribution shines. . This particular property makes it useful for modeling premiums in the insurance industry. Consider the properties these measures exhibit, which would need to be approximated by the probability distribution used to describe them: they are most often zero, as most policies incur no loss; where they do incur a loss, the distribution of losses tends to be highly skewed. As such, the pdf would need to have most of its mass at zero, and the remaining mass skewed to the right. This model can also be applied in other use cases across industries where you find a mixture of zeros and non-negative continuous data points. . Here are some of the first illustrations of a Tweedie Distribution where p = 1.5, from one of Jørgensen’s original analyses:3 . And a practical example of insurance claim payments:4 . Math time: deriving the Tweedie . The Tweedie distribution has three parameters: . mean $ mu$ | dispersion $ phi$ | power parameter $p$ | . The distribution is characterized by a unique mean-variance relationship $var(Y) = phi mu^p$. Setting p=1 gives a quasi-Poisson distribution, while p=2 gives a gamma distribution. In practice, $p$ is a float between 1 and 2, and can be optimized as a hyperparameter via grid search given the relatively small search space. Technically speaking the distribution is not defined outside of the ranges $1 &lt; p &lt; 2$, but when p=0, we have a normal distribution and when p=3, we have an inverse Gaussian distribution. Thus we can directly see how the Tweedie distribution combines the Poisson and Gamma distributions. Thanks to the Tweedie distribution, our choices in modeling are not restricted to the moderately-skewed gamma distribution and the extreme skewness of the inverse Gaussian. The Tweedie provides a continuum of distributions between those two by simply setting the value of p to be between 2 (gamma) and 3 (inverse Gaussian). . Again, the area of the p parameter space we are most interested in is between 1 and 2. At the two ends of that range are Poisson, which is a good distribution for modeling frequency, and gamma, which is good for modeling magnitude. Between 1 and 2, Tweedie becomes a neat combination of Poisson and gamma, which is great for modeling the combined effects. In this way the Tweedie distribution may be thought of as a “Poisson-distributed sum of gamma distributions.”5 . Let $N$ be a random variable with Poisson distribution and $Z_1, Z_2, …$ be independent identically distributed random variables with Gamma distribution. Define a random variable $Z$ by . Z={0, mboxif N=0Z1+Z2+...+ZN, mboxif N&gt;0Z = begin{cases}0, &amp; mbox{if} N = 0 Z_1 + Z_2 + ... + Z_N, &amp; mbox{if} N &gt; 0 end{cases}Z={0,Z1​+Z2​+...+ZN​,​ mboxif N=0 mboxif N&gt;0​ . The resulting distribution of $Z$ is called a compound Poisson distribution. In the case of insurance premium prediction $ $ refers to the number of claims, $Z_i$ refers to the amount of $i$-th claim. The PDF can be written as follows: . fZ(z∣θ,ϕ)=a(z,ϕ)exp{zθ−κ(θ)ϕ}f_Z(z| theta, phi) = a(z, phi) text{exp} left { frac{z theta - kappa( theta)}{ phi} right }fZ​(z∣θ,ϕ)=a(z,ϕ)exp{ϕzθ−κ(θ)​} . Where $a( cdot )$ and $ kappa( cdot )$ are given functions. . The compound Poisson distribution is a special case of Tweedie model, if we re-parameterize the compound Poisson by . λ=1ϕμ2−ρ2−ρ,α=2−ρρ−1,γ=ϕ(ρ−1)μρ−1 lambda = frac{1}{ phi} frac{ mu^{2- rho}}{2- rho}, alpha = frac{2- rho}{ rho-1}, gamma= phi( rho-1) mu^{ rho-1}λ=ϕ1​2−ρμ2−ρ​,α=ρ−12−ρ​,γ=ϕ(ρ−1)μρ−1 . then it will have the form of a Tweedie model Tw($ mu, phi, rho$) with the PDF: . fTw(z∣μ,ϕ,ρ):=a(z,ϕ,ρ)exp(1ϕ(zu1−ρ1−ρ−μ2−ρ2−ρ))f_{Tw}(z| mu, phi, rho) := a(z, phi, rho) text{exp} left ( frac{1}{ phi}(z frac{u^{1- rho}}{1- rho} - frac{ mu^{2- rho}}{2- rho}) right )fTw​(z∣μ,ϕ,ρ):=a(z,ϕ,ρ)exp(ϕ1​(z1−ρu1−ρ​−2−ρμ2−ρ​)) . The log-likelihood of the PDF can be written as . p(z)=1ϕ(zμ1−ρ1−ρ−μ2−ρ2−ρ)+ap(z) = frac{1}{ phi} left(z frac{ mu^{1- rho}}{1- rho} - frac{ mu^{2- rho}}{2- rho} right) + ap(z)=ϕ1​(z1−ρμ1−ρ​−2−ρμ2−ρ​)+a where $a, phi, mu$ and $1&lt; rho &lt; 2$ are some constants. . To convert Tweedie distribution to a loss function, we need to maximize the likelihood of sample data through model training. A common way to do this is through the negative log-likelihood. For computational stability instead of optimizing $ mu$ parameter of Tweedie distribution directly, we will optimize $ log{ mu}$ . So changing our notation a bit, the Tweedie loss is given by the following formula: . L=∑i=1nwi(−yiexp⁡(F(xi)(1−ρ))1−ρ+exp⁡(F(xi)(2−ρ))2−ρ)L = sum_{i=1}^n w_i left(- frac{y_i exp{(F(x_i)(1- rho))}}{1 - rho} + frac{ exp{(F(x_i)(2- rho))}}{2 - rho} right)L=i=1∑n​wi​(−1−ρyi​exp(F(xi​)(1−ρ))​+2−ρexp(F(xi​)(2−ρ))​) . where $w_i$ are object weights, $y_i$ is target, $F(x_i)$ is current object prediction, $ rho$ is the obligatory variance power. Variance power must belong to the interval $[1, 2]$. . Coding a Tweedie loss . The loss function of Tweedie is created for the aim of maximizing the negative log likelihood of the Tweedie distribution. Lightgbm computes Tweedie loss as follows: . def tweedie_eval(y_pred, y_true, p=1.5): y_true = y_true.get_label() a = y_true*np.exp(y_pred, (1-p)) / (1-p) b = np.exp(y_pred, (2-p))/(2-p) loss = -a + b return loss . A Tensorflow implementation would look as follows: . import tensorflow as tf def tweedie_loss_func(p): def tweedie_loglikelihood(y, y_hat): loss = - y * tf.pow(y_hat, 1 - p) / (1 - p) + tf.pow(y_hat, 2 - p) / (2 - p) return tf.reduce_mean(loss) return tweedie_loglikelihood . Scikit-learn has an implementation of a Tweedie Regressor.6 H20 even has a gradient boosting implementation with a Tweedie distribution!7 Here is a general framework for how it can be used: . import h2o from h2o.estimators.gbm import H2OGradientBoostingEstimator from h2o.grid.grid_search import H2OGridSearch gbm = H2OGradientBoostingEstimator(distribution=&quot;tweedie&quot;, tweedie_power = 1.2, seed =1234) gbm.train(x = predictors, y = response, training_frame = train, validation_frame = valid) # Example of values to grid over for `tweedie_power` # select the values for tweedie_power to grid over hyper_params = {&#39;tweedie_power&#39;: [1.2, 1.5, 1.7, 1.8]} gbm_2 = H2OGradientBoostingEstimator(distribution = &quot;tweedie&quot;, seed = 1234,) grid = H2OGridSearch(model = gbm_2, hyper_params = hyper_params, search_criteria = {&#39;strategy&#39;: &quot;Cartesian&quot;}) grid.train(x = predictors, y = response, training_frame = train, validation_frame = valid) sorted_grid = grid.get_grid(sort_by = &#39;mse&#39;, decreasing = False) print(sorted_grid) . And my favorite code implementation: CatBoost! . from catboost import CatBoostRegressor, Pool train_pool = Pool(df_train[features], label=df_train[target], cat_features=cat_features) test_pool = Pool(df_test[features], label=df_test[target], cat_features=cat_features) cb_tweedie = CatBoostRegressor(loss_function=&#39;Tweedie:variance_power=1.9&#39;, n_estimators=500, silent=True) cb_tweedie.fit(train_pool, eval_set=test_pool) . Jørgensen, B. (1987), Exponential Dispersion Models. Journal of the Royal Statistical Society: Series B (Methodological), 49: 127-145. https://doi.org/10.1111/j.2517-6161.1987.tb01685.x &#8617; . | Tweedie, M.C.K. (1984). “An index which distinguishes between some important exponential families”. In Ghosh, J.K.; Roy, J (eds.). Statistics: Applications and New Directions. Proceedings of the Indian Statistical Institute Golden Jubilee International Conference. Calcutta: Indian Statistical Institute. pp. 579–604. MR 0786162 &#8617; . | Kelly, Jorgensen (1996). Analyzing Accident Benefit Data Using Tweedie’s Compound Poisson Model. &#8617; . | Heller et al. (2007) Mean and dispersion modelling for policy claims costs. &#8617; . | M. Goldburd, A. Khare, D. Tevet, D. Guller (2016). Generalized linear models for insurance rating, _Casualty Actuarial Society, CAS Monographs Series. &#8617; . | https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.TweedieRegressor.html &#8617; . | https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/algo-params/tweedie_power.html &#8617; . |",
            "url": "https://jackhmiller.github.io/My-DS-Blog/2021/01/31/tweedie.html",
            "relUrl": "/2021/01/31/tweedie.html",
            "date": " • Jan 31, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Alternating Method of Multipliers- Theory and Industry Example Application",
            "content": "In this blog post we will solve the following optimization problem using the scaled form of alternating direction method of multipliers (ADMM). . $$ min frac{1}{2} left | Ax-b right |_{2}^{2} + lambda_1 left | x right |_1 + frac{ lambda_2}{2} left | x right |_{2}^{2}min frac{1}{2} left | Ax-b right |_{2}^{2} + lambda_1 left | x right |_1 + frac{ lambda_2}{2} left | x right |_{2}^{2}$$ . Background on ADMM . The alternating direction method of multipliers (ADMM) is an algorithm that solves convex optimization problems by breaking them into smaller pieces, each of which are then easier to handle. Namely, it is intended to blend the decomposability of dual ascent with the superior convergence properties of the method of multipliers. The original paper can be found here: https://stanford.edu/~boyd/papers/pdf/admm_distr_stats.pdf. . ADMM received lots of attention a few years ago due to the tremendous proliferation and subsequentdemand from large-scale and data-distributed machine learning applications. ADMM a fairly simple computational method for optimization proposed in 1970s. It stemmed from the augmented Lagrangian method (also known as the method of multipliers) dating back to late 1960s. The theoretical aspects of ADMM have been studied since the 1980s, and its global convergence was established in the literature (Gabay, 1983; Glowinski &amp; Tallec, 1989;Eckstein &amp; Bertsekas, 1992). As reviewed in the comprehensive paper (Boyd et al., 2010), with the ability of dealing with objective functions separately and synchronously , ADMM turned out to be a natural fit in the field of large-scale data-distributed machine learning and big-data related optimization, and therefore received significant amount of attention beginning in 2015. Considerable work was conducted thereafter. . On the theoretical side, ADMM was shown to have an O(1/N) rate of convergence for convex problems. . The algorithm solves the problem in the form: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; minimize $f(x) + g(z)$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; subject to $Ax+ Bz = c$ . with variables $x in R^n$ and $z in R^m$, where $A in R^{pxn}$, $B in R^{pxm}$ and $C in R^{p}$. . The only difference from the general linear equality-constrained problem is that the variable x has been split into two parts, called x and z, with the objective function separable across this splitting. The optimal value of the problem is now denoted by: $$ p^* = inf left { f(x) + g(x) | Ax + Bz = c right } $$ Which forms the augmented Lagrangian: $$ L_p(x, z, y) = f(x) + g(z) + y^T(Ax+Bz-c) + frac{ rho}{2} left | Ax + Bz -c right |_{2}^{2} $$ . Finally, we have our ADMM which consists of the following iterations: $$ x^{k+1} = underset{x}{argmin} L_ rho(x, z^k, y^k)$$ $$ z^{k+1} = underset{x}{argmin} L_ rho(x^{k+1}, z, y^k) $$ $$ y^{k+1} = y^k + rho(Ax^{k+1} +Bz^{k+1}-c) $$ $$s.t. rho&gt;0$$ . The algorithm is very similar to dual ascent and the method of multipliers: it consists of an x-minimization step, a z-minimization step, and a dual variable update. As in the method of multipliers, the dual variable update uses a step size equal to the augmented Lagrangian parameter. However, while with the method of multipliers the augmented Lagrangian is minimized jointly with respect to the two primal variables, in ADMM, on the other hand, x and z are updated in an alternating or sequential fashion, which accounts for the term alternating direction. . Simple examples show that ADMM can be very slow to converge to high accuracy. However, it is often the case that ADMM converges to modest accuracy—sufficient for many applications—within a few tens of iterations. This behavior makes ADMM similar to algorithms likethe conjugate gradient method, for example, in that a few tens of iterations will often produce acceptable results of practical use. However, the slow convergence of ADMM also distinguishes it from algorithms such as Newton’s method (or, for constrained problems, interior-point methods), where high accuracy can be attained in a reasonable amount of time. While in some cases it is possible to combine ADMM with a method for producing a high accuracy solution from a low accuracy solution, in the general case ADMM will be practically useful mostly in cases when modest accuracy is sufficient. Fortunately, this is usually the case for large-scale industrial applications. Also, in the case of machine learning problems, solving a parameter estimation problem to very high accuracy often yields little to no improvement in actual prediction performance, the real metric of interest in applications. . Our Optimization Problem . First we will write the augmented Lagrangian function (the scaled form) and drive the ADMM updates. . Scaled form of the augmented Lagrangian . $$ L(x, z, u: rho) = min frac{1}{2} left | Ax-b right |_{2}^{2} + lambda_{1} left | z right |_{1} + frac{ lambda_{2}}{2} left | z right |_{2}^{2} + frac{ rho}{2} left | x-z+ w right |_{2}^{2} + frac{ rho}{2} left | w right |_{2}^{2} $$ . $$ x_{k} = underset{x}{argmin} frac{1}{2} left | Ax-b right |_{2}^{2} + frac{ rho}{2} left | x-z_{k-1}+ w_{k-1} right |_{2}^{2} $$ . $$ x_k = ((A^TA+ rho I))^{-1}( rho(z_{k-1}-w_{k-1})+A^Tb) $$ . $$ z_{k} = underset{z}{argmin} ( lambda_{1} left | z right |_{1} + frac{ lambda_{2}}{2} left | z right |_{2}^{2}) + frac{ rho}{2} left | x_{k}-z+ w_{k-1} right |_{2}^{2} $$ . if $z&gt;0$ $$ z_{k} = frac{ rho(w_{k-1}+x_{k})}{ lambda_2- rho}- lambda_1$$ . if $z&lt;0$ $$ z_{k} = frac{ rho(w_{k-1}+x_{k})}{ lambda_2- rho}+ lambda_1$$ . $$ w_{k} = w_{k-1} + x_k -z_k $$ . Practical Application . Now, we will implement a regression algorithm using our augmented lagrangian. The dataset is the performance decay over time of a ship&#39;s Gas Turbine (GT) compressor. We split our test and train data 20:80. The range of decay of compressor has been sampled with a uniform grid of precision 0.001 so to have a good granularity of representation. For the compressor decay state discretization the kMc coefficient has been investigated in the domain [0.95,1]. Ship speed has been investigated sampling the range of feasible speed from 3 knots to 27 knots with a granularity of representation equal to tree knots. A series of measures (13 features) which indirectly represents of the state of the system subject to performance decay has been acquired and stored in the dataset over the parameter’s space. . The A 13-feature vector containing the GT measures at steady state of the physical asset: . Lever position (lp) | Ship speed (v) | Gas Turbine (GT) shaft torque (GTT) | GT rate of revolutions (GTn) | Gas Generator rate of revolutions (GGn) | Port Propeller Torque (Tp) | Hight Pressure (HP) Turbine exit temperature (T48) | GT Compressor outlet air temperature (T2) | HP Turbine exit pressure (P48) | GT Compressor outlet air pressure (P2) | GT exhaust gas pressure (Pexh) | Turbine Injection Control (TIC) | Fuel flow (mf) | GT Compressor decay state coefficient | . import pandas as pd import numpy as np import matplotlib.pyplot as plt %matplotlib inline from scipy.io import loadmat . ship_test = pd.read_csv(&quot;Shiptest-2.csv&quot;, header=None) ship_train = pd.read_csv(&quot;Shiptrain-2.csv&quot;, header=None) . X_train = ship_train.iloc[:,:-1] y_train = ship_train.iloc[:,-1:] X_test = ship_test.iloc[:,:-1] y_test = ship_test.iloc[:,-1:] . lambda_1 = 0.1 lambda_2 = 0.9 . iterations = 100 rho = 0.1 w = 0 z = np.random.rand(13).reshape(-1,1) A = X_train.values b = y_train.values loss = [] for i in range(iterations): x = (np.linalg.inv(np.dot(A.T,A) + (rho*np.eye(13)))).dot(rho*(z-w) + np.dot(A.T, b)) for i in range(len(z)): if np.sign(z[i])&gt;0: z = rho*(w+x)/(lambda_2-rho) - lambda_1 else: z = rho*(w+x)/(lambda_2-rho) + lambda_1 w = w + rho*(x-z) loss.append(np.sum(0.5*(np.dot(A,x)-b)**2)) . plt.figure(figsize=(10,5)) plt.plot(loss) plt.title(&#39;Obj function vs iterations&#39;) plt.xlabel(&#39;Number of iterations&#39;) plt.ylabel(&#39;Obj&#39;) . Text(0, 0.5, &#39;Obj&#39;) . print(&quot;Coefficients&quot;) x . Coefficients . array([[-5.05840315e-02], [ 1.03456745e-02], [ 1.21482112e-05], [-2.86479882e-04], [-1.49933306e-05], [-1.52302544e-03], [-9.13559793e-04], [ 1.36474482e-03], [ 1.85080531e-01], [ 5.00636758e-02], [ 6.72727845e-01], [-1.62063399e-04], [-1.76737055e-01]]) . Sum absolute errors . abs_ms_errors = [] abs_errors = [] for A, b in zip(X_test.values, y_test.values): abs_ms_errors.append(abs(np.sum(0.5*(np.dot(A,x)-b)**2))) abs_errors.append(abs(np.sum(np.dot(A,x)-b))) print(&quot;Sum absolute mean-squared errors:&quot;, round(sum(abs_ms_errors), 2)) print(&quot;Sum absolute errors:&quot;, round(sum(abs_errors), 2)) . Sum absolute mean-squared errors: 0.14 Sum absolute errors: 21.1 .",
            "url": "https://jackhmiller.github.io/My-DS-Blog/2020/12/17/ADMM.html",
            "relUrl": "/2020/12/17/ADMM.html",
            "date": " • Dec 17, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Variational Autoencoders (VAEs)",
            "content": "Background . In Bayesian models, the latent variables help govern the distribution of the data, where our ultimate goal is to is to approximate a conditional density of latent variables given observed variables.1 A Bayesian model draws the latent variables from a prior density $p(z)$ and then relates them to the observations through likelihood $p(x|z)$. Inference in Bayesian modeling amounts to conditioning on data and computing the posterior $p(z|x)$. In complex Bayesian models, this computation often requires approximate inference.2 For decades, the dominant paradigm for approximate inference was Markov chain Monte Carlo (MCMC). However, as data sets have become more complex and larger, we need an approximate conditional faster than a simple MCMC algorithm can produce. In these settings, variational inference provides a good alternative approach to approximate Bayesian inference. . [!note] MCMC and variational inference are different approaches to solving the same problem. MCMC algorithms sample a Markov chain; variational algorithms solve an optimization problem. MCMC algorithms approximate the posterior with samples from the chain; variational algorithms approximate the posterior with the result of the optimization. . Variational inference is suited to large data sets and scenarios where we want to quickly explore many models; MCMC is suited to smaller data sets and scenarios where we happily pay a heavier computational cost for more precise samples. . The Main Idea of Variational Inference . Rather than use sampling, the main idea behind Variational Bayesian (VB) inference is to use optimization. . VB methods allow us to re-write statistical inference problems (i.e. infer the value of a random variable given the value of another random variable) as optimization problems (i.e. find the parameter values that minimize some objective function). . First, we posit a family of approximate densities $ vartheta$, which is a set of densities over the latent variables. Then we try to find a member of that family that minimizes the Kullback-Leibler (KL) divergence to the exact posterior, . q∗(z)=arg min q(z)∈ϑKL(q(z)∣∣p(z∣x))q^*(z) = underset{q(z) in vartheta }{ text{arg min }}KL(q(z)||p(z|x))q∗(z)=q(z)∈ϑarg min ​KL(q(z)∣∣p(z∣x)) . Finally, we approximate the posterior with the optimized member of the family $q^*( cdot )$. One of the key ideas is to choose $ vartheta$ that is flexible enough to capture a density close to $p(z | x)$ but simply enough for efficient optimization. | . The Deep Learning Connection . An Autoencoder is a neural network designed to learn an identity function in an unsupervised way to reconstruct the original input while compressing the data in the process so as to discover a more efficient and compressed representation. . It consists of two networks: . Encoder network: It translates the original high-dimension input into the latent low-dimensional code. The input size is larger than the output size. | Decoder network: The decoder network recovers the data from the code, likely with larger and larger output layers. | . The encoder network essentially accomplishes the dimensionality reduction. In addition, the autoencoder is explicitly optimized for the data reconstruction from the code. A good intermediate representation not only can capture latent variables, but also benefits a full decompression process. . The drawback with autoencoders is that their latent space may not have a well-defined structure or exhibit smooth transitions between classes that exist within the data. What if we wanted a smoother latent space that allowed for meaningful interpolation even between latent points? . Towards (Deep) Generative Modeling . Generative Modeling involves creating models of distributions over data points in some potentially high dimensional space. The job of the generative model is to capture dependencies between data points; the more complicated the dependencies, the more difficult the model is to train. Ultimately, we are aiming to maximize the probability of each point X in the training set under the entire generative process according to: . P(X)=∫P(X∣z;θ)P(z)dzP(X) = int P(X|z; theta)P(z)dzP(X)=∫P(X∣z;θ)P(z)dz . where $z$ is a vector of latent variables in a high dimensional space $Z$ that we can sample over according to a PDF. Then we have a family of deterministic functions $f(z; theta)$ parameterized by $ theta$ where we want to optimize $ theta$ such that we can sample $z$ from $P(z)$ with a high probability that $f(z: theta)$ will be like the X’s in our dataset. So before we can say that our model is representative of the dependencies in our data and thus representative of our dataset, we need to make sure that for ever point X there is a set of latent variables that causes the model to generate something very similar to X. . It is important to note that in the above equation $f(z; theta)$ is represented by a distribution $P(X | z; theta)$ which enabled use to use the law of probability and the maximum likelihood framework to optimize our models through gradient descent. This would not be possible if we used $X = f(z; theta)$ since it is deterministic and thus not differentiable. | . Deep Variational Inference: VAEs . VAEs take an unusual approach to dealing with this problem: they assume that there is no simple interpretation of the dimensions of $z$, and instead assert that samples of $z$ can be drawn from a simple distribution, i.e., N (0, I), where I is the identity matrix. With powerful function approximators like neural networks, we can simply learn a function which maps our independent, normally-distributed z values to whatever latent variables might be needed for the model, and then map those latent variables to X. All that remains is then to maximize $P(X)$ where $P(z) = N(z|0, I)$, using gradients. . Instead of mapping the input into a fixed vector, we want to map it into a distribution. . In practice, for most $z$, $P(X | z)$ will be nearly zero and thus contribute nothing to our estimate of $P(X)$. It is also a large and expensive undertaking to search the entire space of possible $z$’s. So the key idea with VAEs is to attempt to sample values of $z$ that are likely to have produced X, and compute $P(X)$ just from those. This means we need a new function $Q(z | X)$ which can take a value of X and give us a distribution over $z$ values that are likely to produce X. The hope is that the space of $z$ values that are likely under $Q$ will be much smaller than the space of all $z$’s that are likely under the prior $P(z)$. | . . So in our VAE setting, we have: . $p_ theta(x | z)$ defines the generative model, playing the role of the decoder in a traditional autoencoder architecture | . | $q_ phi(z | x)$ is the probabilistic encoder, playing the role of encoder in a traditional autoencoder architecture. | . | The connection to deep learning, and why a VAE is an autoencoder, is that we use neural networks as our function approximators for $q$ . Our goal should be to make the estimated posterior $q_ phi(z | x)$ as similar to the real one $p_ theta(x | z)$ as possible (noted by the dashed blue line in the figure above). | . VAE Evaluation . Loss Function Derivation . So how do we do approximate inference of the latent variable $z$? We want to maximize the log-likelihood of generating real data ($log p_ theta(x)$) while minimizing the difference between the real and estimated posterior distributions. We can use Kullback-Leibler (KL) divergence to quantify the distance between these two distributions. But more on that in a second. . So exact inference is not possible, and to see this we may use Bayes’ theorem to find an expression for the posterior: . pθ(z∣x)=pθ(x∣z)pθ(z)pθ(x)(1)p_ theta(z|x) = frac{p_ theta(x|z)p_ theta(z)}{p_ theta(x)} tag{1}pθ​(z∣x)=pθ​(x)pθ​(x∣z)pθ​(z)​(1) . The marginal term, or our denominator: . pθ(x)=∫z0...∫zDpθ(x∣z)dz0...dzD(2)p_ theta(x) = int_{z_0}... int_{z_D} p_ theta(x|z)dz_0...dz_D tag{2}pθ​(x)=∫z0​​...∫zD​​pθ​(x∣z)dz0​...dzD​(2) . is intractable. So how do we get our posterior? . Instead, we define an approximation $q_{ theta}(z | x)$ to the intractable posterior. To ensure that $p_{ theta}(z | x)$ is similar to $q_{ phi}(z | x)$, we could minimize the KL divergence between the two discrete distributions as an optimization problem: | . θ∗,ϕ∗=argminθ∗,ϕ∗ DKL(qϕ(z∣x)∣∣pθ(z∣x))(3) theta^*, phi^* = text{arg} underset{ theta^*, phi^*}{min} text{ D}_{KL}(q_ phi(z|x)||p_ theta(z|x)) tag{3}θ∗,ϕ∗=argθ∗,ϕ∗min​ DKL​(qϕ​(z∣x)∣∣pθ​(z∣x))(3) . DKL(qϕ(z∣x)∣∣pθ(z∣x))=Eqϕ(z∣x)[logeqϕ(z∣x)pθ(z∣x)](4a) text{D}_{ text{KL}}(q_ phi(z|x)||p_ theta(z|x)) = text{E}_{q_{ phi(z|x)}} begin{bmatrix} log_e frac{q_ phi(z|x)}{p_ theta(z|x)} end{bmatrix} tag{4a}DKL​(qϕ​(z∣x)∣∣pθ​(z∣x))=Eqϕ(z∣x)​​[loge​pθ​(z∣x)qϕ​(z∣x)​​](4a) . Or in the continuous case (and simplified notation): . DKL(qϕ(z∣x)∣∣pθ(z∣x))=∫qϕ(z)logqϕ(z)pθ(z∣x)(4b) text{D}_{ text{KL}}(q_ phi(z|x)||p_ theta(z|x)) = int{q_{ phi(z)}} log frac{q_ phi(z)}{p_ theta(z|x)} tag{4b}DKL​(qϕ​(z∣x)∣∣pθ​(z∣x))=∫qϕ(z)​logpθ​(z∣x)qϕ​(z)​(4b) . To see how we can start to minimize the KL divergence $ text{D}{ text{KL}}(q phi(z | x) |   | p_ theta(z | x)) $, we will reformulate eq. 4 above: | . =Eqϕ(z∣x)[logeqϕ(z∣x)pθ(z∣x)](5a)= text{E}_{q_{ phi(z|x)}} begin{bmatrix} log_e frac{q_ phi(z|x)}{p_ theta(z|x)} end{bmatrix} tag{5a}=Eqϕ(z∣x)​​[loge​pθ​(z∣x)qϕ​(z∣x)​​](5a) . =Eqϕ(z∣x)[logeqϕ(z∣x)]−Eqϕ(z∣x)[logepθ(z∣x)](5b)= text{E}_{q_{ phi(z|x)}} begin{bmatrix} log_eq_ phi(z|x) end{bmatrix} - text{E}_{q_{ phi(z|x)}} begin{bmatrix} log_ep_ theta(z|x) end{bmatrix} tag{5b}=Eqϕ(z∣x)​​[loge​qϕ​(z∣x)​]−Eqϕ(z∣x)​​[loge​pθ​(z∣x)​](5b) . =Eqϕ(z∣x)[logeqϕ(z∣x)]−Eqϕ(z∣x)[logepθ(z,x)pθ(x)](5c)= text{E}_{q_{ phi(z|x)}} begin{bmatrix} log_eq_ phi(z|x) end{bmatrix} - text{E}_{q_{ phi(z|x)}} begin{bmatrix} log_e frac{p_ theta(z, x)}{p_ theta(x)} end{bmatrix} tag{5c}=Eqϕ(z∣x)​​[loge​qϕ​(z∣x)​]−Eqϕ(z∣x)​​[loge​pθ​(x)pθ​(z,x)​​](5c) . =Eqϕ(z∣x)[logeqϕ(z∣x)pθ(z,x)]+Eqϕ(z∣x)[logepθ(x)](5d)= text{E}_{q_{ phi(z|x)}} begin{bmatrix} log_e frac{q_ phi(z|x)}{p_ theta(z, x)} end{bmatrix} + text{E}_{q_{ phi(z|x)}} begin{bmatrix} log_ep_ theta(x) end{bmatrix} tag{5d}=Eqϕ(z∣x)​​[loge​pθ​(z,x)qϕ​(z∣x)​​]+Eqϕ(z∣x)​​[loge​pθ​(x)​](5d) . =Eqϕ(z∣x)[logeqϕ(z∣x)pθ(z,x)]+logepθ(x)(5e)= text{E}_{q_{ phi(z|x)}} begin{bmatrix} log_e frac{q_ phi(z|x)}{p_ theta(z, x)} end{bmatrix} + log_ep_ theta(x) tag{5e}=Eqϕ(z∣x)​​[loge​pθ​(z,x)qϕ​(z∣x)​​]+loge​pθ​(x)(5e) . So looking at eq. 5e we can see that the KL divergence depends on the intractable marginal likelihood $p_ theta(x)$ (note that we can drop the expectation for the second term since $q$ is not present). There’s no way we can minimize the above equation if we can’t write down $p_ theta(x)$ in closed form. However, we can get around this: we’ll minimize the KL divergence, but not directly. Instead, we try to find a quantity which we can maximize, and show that in turn this minimizes the KL divergence. The trick is not obvious, but is simply done by finding a lower bound on the log marginal likelihood. . Given that the second term in eq. 5e is fixed (the evidence term) as it does not depend on our surrogate $q$, we will derive a lower bound on the first term as a way of minimizing KL divergence . ELBO . In Variational Bayesian methods, this loss function is known as the variational lower bound, or evidence lower bound. The “lower bound” part in the name comes from the fact that KL divergence is always non-negative. . Recall the first term in eq. 5e, simply by rearraigning terms we get: . =−logeEqϕ(z∣x)[qϕ(z∣x)pθ(x,z)](6)= -log_e text{E}_{q_ phi(z| x)} begin{bmatrix} frac{q_ phi(z|x)}{p_ theta(x,z)} end{bmatrix} tag{6}=−loge​Eqϕ​(z∣x)​[pθ​(x,z)qϕ​(z∣x)​​](6) . So adding eq. 6 back in to our KL divergence as $L(q)=L( theta, phi; x)$, we get we get: . DKL(qϕ(z∣x)∣∣pθ(z∣x))=−L(θ,ϕ;x)+logepθ(x)(7) text{D}_{ text{KL}}(q_ phi(z|x)||p_ theta(z|x)) = -L( theta, phi; x) + log_ep_ theta(x) tag{7}DKL​(qϕ​(z∣x)∣∣pθ​(z∣x))=−L(θ,ϕ;x)+loge​pθ​(x)(7) . Since KL divergence is a distance metric, we know that it must be greater than or equal to zero. Thus the evidence must be larger than $L$, hence the evidence-based lower bound. Finally, we can write the expression: . L(θ,ϕ;x)=logpθ(x) iif KL[q(z)∣∣p(z∣x)]=0(8)L( theta, phi; x) = log p_{ theta}(x) text{ iif } KL[q(z)||p(z|x)] = 0 tag{8}L(θ,ϕ;x)=logpθ​(x) iif KL[q(z)∣∣p(z∣x)]=0(8) . Which if true means that we have found the true posterior! . Maximizing the ELBO (to the point where eq. 8 holds) is equivalent to minimizing the KL divergence. That is the essence of variational inference. . ELBO (closed form) . Since the KL divergence is the negative of the ELBO up to an additive constant (with respect to q), minimizing the KL divergence is equivalent to maximizing the ELBO. Now we will seek to write the ELBO in closed-form, after which we will be able to implement the variational autoencoder. . L(θ,ϕ;x)=Eqϕ(z,x)[logepθ(x,z)qϕ(z∣x)](9a)L( theta, phi; x) = text{E}_{q_ phi(z, x)} begin{bmatrix}log_e frac{p_ theta(x,z)}{q_ phi(z|x)} end{bmatrix} tag{9a}L(θ,ϕ;x)=Eqϕ​(z,x)​[loge​qϕ​(z∣x)pθ​(x,z)​​](9a) . =−Eqϕ(z,x)[logeqϕ(z∣x)pθ(x,z)](9b)= - text{E}_{q_ phi(z, x)} begin{bmatrix}log_e frac{q_ phi(z|x)}{p_ theta(x,z)} end{bmatrix} tag{9b}=−Eqϕ​(z,x)​[loge​pθ​(x,z)qϕ​(z∣x)​​](9b) . =−Eqϕ(z,x)[logeqϕ(z∣x)pθ(x∣z)pθ(z)](9c)= - text{E}_{q_ phi(z, x)} begin{bmatrix}log_e frac{q_ phi(z|x)}{p_ theta(x|z)p_ theta(z)} end{bmatrix} tag{9c}=−Eqϕ​(z,x)​[loge​pθ​(x∣z)pθ​(z)qϕ​(z∣x)​​](9c) . =−Eqϕ(z,x)[logeqϕ(z∣x)pθ(z)]+Eqϕ(z,x)[logepθ(x∣z)](9d)= - text{E}_{q_ phi(z, x)} begin{bmatrix}log_e frac{q_ phi(z|x)}{p_ theta(z)} end{bmatrix} + text{E}_{q_ phi(z, x)} begin{bmatrix}log_ep_ theta(x|z) end{bmatrix} tag{9d}=−Eqϕ​(z,x)​[loge​pθ​(z)qϕ​(z∣x)​​]+Eqϕ​(z,x)​[loge​pθ​(x∣z)​](9d) . =−DKL(qϕ(z∣x)∣∣pθ(z))+Eqϕ(z,x)[logepθ(x∣z)](9e)= -D_{KL}(q_ phi(z|x)||p_ theta(z)) + text{E}_{q_ phi(z, x)} begin{bmatrix}log_ep_ theta(x|z) end{bmatrix} tag{9e}=−DKL​(qϕ​(z∣x)∣∣pθ​(z))+Eqϕ​(z,x)​[loge​pθ​(x∣z)​](9e) . Thus for a single data point $x^i$ the ELBO can be represented as: . LELBO(θ,ϕ;xi)=−DKL(qϕ(z∣xi)∣∣pθ(z))+Eqϕ(z,x)[logepθ(xi∣z)](10)L_{ text{ELBO}}( theta, phi; x^{i})= -D_{KL}(q_ phi(z|x^i)||p_ theta(z)) + text{E}_{q_ phi(z, x)} begin{bmatrix}log_ep_ theta(x^i|z) end{bmatrix} tag{10}LELBO​(θ,ϕ;xi)=−DKL​(qϕ​(z∣xi)∣∣pθ​(z))+Eqϕ​(z,x)​[loge​pθ​(xi∣z)​](10) . The second term is the expected L2 reconstruction error under the encoder model. The firm term is thought of as a regularization term. . A Quick Detour- KL and Reverse KL . Hopefully you know that KL divergence is not symmetric: $KL(q(z)||p(z)) neq KL(p(z)||q(z))$. Thus there is the forward KL $KL(q(z)||p(z))$ and the reverse KL $KL(p(z)||q(z))$ (forward and reverse are arbitrary here). . *So why are we choosing $KL(q(z) |   | p(z))?$* The key is to understand the difference in behavior between the two different directions of the KL divergence. | . There are two main differences: . Mode seeking vs mode covering . | Zero avoiding vs zero forcing . | Given that we are dealing with variational inference, and we are exploring a family of distributions $q$, we want to keep our search space as small as possible to maximize computational efficiency. Thus we choose $KL(q(z) |   | p(z))$ that has zero avoiding behavior- it does not place significant probability mass in regions of variable space that have very low probability, as seen above in Bishop’s Figure 10.2. | . Similarly, we want to avoid mode-seeking behavior, like (a) in Bishop’s 10.3 above, since the center of that distribution falls between the two modes, thus assigning high probability to a region with low density (this means it is a bad representation of $p(z)$ ). This is significant since n practical applications, the true posterior distribution will often be multimodal, with most of the posterior mass concentrated in some number of relatively small regions of parameter space. These multiple modes may arise through non-identifiability in the latent space or through complex nonlinear dependence on the parameters. $KL(q(z) |   | p(z))$ tends to find one of these modes, while if we were to minimize the reverse instead, the resulting approximations would average across all of the modes and, in the context of the mixture model, would lead to poor predictive distributions (because the average of two good parameter values is typically itself not a good parameter value).3 | . Optimizing Our Loss Function . Why backpropogation does not work with our loss’s current form . Just as a refresher, let us recall that to minimize a function say $ mathbb{E}_{p(z)}[f(z)]$ we take the derivative, or in other words the expectation of the integral of the function. This works if our parameters are know. But say we had an equation like this with unknown parameters for $p$: . Epθ[f(z)] mathbb{E}_{p_{ color{Red} theta}}[f(z)]Epθ​​[f(z)] . Minimizing this above function results in . ▽Epθ[f(z)]=▽∫zpθ(z)f(z)dz(11) bigtriangledown mathbb{E}_{p_{ color{Red} theta}}[f(z)] = bigtriangledown int_{z}p_{ color{Red} theta}(z)f(z)dz tag{11}▽Epθ​​[f(z)]=▽∫z​pθ​(z)f(z)dz(11) . So we have the gradient of two functions so using basic calculus we get: . =∫zf(z)▽pθ(z)dz+Epθ(z)▽f(z)(12)= int_zf(z) bigtriangledown p_{ color{Red} theta}(z)dz + mathbb{E}_{p_{ color{Red} theta}(z)} bigtriangledown f(z) tag{12}=∫z​f(z)▽pθ​(z)dz+Epθ​(z)​▽f(z)(12) . The second term is not a problem since we can calculate the expectation of p with respect to the unknown parameters via sampling. What we can’t do is calculate the derivative with respect to unknown parameters like in the first term. In other words, we cannot swap the gradient and the expectation, since the expectation is being taken with respect to the distribution that we are trying to differentiate. . So instead of framing our equation in terms of these unknown parameters, we might try to frame it in terms of a known equation: . Epθ[f(z)]=Epϵ[f((g(ϵ,x)))](13) mathbb{E}_{p_{ color{Red} theta}}[f(z)] = mathbb{E}_{p_{ color{Green} epsilon}}[f((g({ color{Green} epsilon}, x)))] tag{13}Epθ​​[f(z)]=Epϵ​​[f((g(ϵ,x)))](13) . If we know the distribution of $ epsilon$ , we can take its derivate thus allowing us to compute the first term in eq. 12. In other words, we avoid evaluating the gradient in terms of $ theta$ and instead evaluate it in terms of $ epsilon$. This is the key contribution of the original VAE paper: the reparameterization trick..4 . The Reparameterization Trick . Ok, so we have our loss function that we want to use to optimize our VAE. Recall eq. 10 . L(θ,ϕ;xi)=−DKL(qϕ(z∣xi)∣∣pθ(z))+Eqϕ(z,x)[logepθ(xi∣z)](10)L( theta, phi; x^{i})= -D_{KL}(q_ phi(z|x^i)||p_ theta(z)) + text{E}_{q_ phi(z, x)} begin{bmatrix}log_ep_ theta(x^i|z) end{bmatrix} tag{10}L(θ,ϕ;xi)=−DKL​(qϕ​(z∣xi)∣∣pθ​(z))+Eqϕ​(z,x)​[loge​pθ​(xi∣z)​](10) . Rearraigning (and simplifying a bit) we get: . logpθ(x)=KL(qθ(z∣x)∣∣pθ(z∣x))+ELBO(ϕ,θ)(14) text{log}p_{ theta(x)}= KL(q_ theta(z|x)||p_ theta(z|x)) + text{ELBO}( phi, theta) tag{14}logpθ(x)​=KL(qθ​(z∣x)∣∣pθ​(z∣x))+ELBO(ϕ,θ)(14) . So we want to minimize the KL divergence and maximize the ELBO. And we have two parameters: $ phi$ and $ theta$ so we need $ bigtriangledown_ phi$ and $ bigtriangledown_ theta$. . We want to optimize the parameters of our model $ theta$. The gradients of the ELBO (the second term in eq. 10) w.r.t. the $ theta$ are straightforward to obtain since we known the joint distribution $p_{ theta(x,z)}$. Our gradient of the ELBO just evaluates to $ bigtriangledown_{ theta}L_{ELBO} = bigtriangledown_{ theta}( text{log}p_ theta(x,z))$. Here our $z$ is just a random sample from $q_ phi(z | x)$. | . The other term (the first term in eq. 10) is our KL divergence. Recall from eq. 6 that the KL divergence here can be expressed as . =−logeEqϕ(z∣x)[qϕ(z∣x)pθ(x,z)](6)= -log_e text{E}_{q_ phi(z| x)} begin{bmatrix} frac{q_ phi(z|x)}{p_ theta(x,z)} end{bmatrix} tag{6}=−loge​Eqϕ​(z∣x)​[pθ​(x,z)qϕ​(z∣x)​​](6) . Which can be rewritten as: . =▽ϕEqϕ(z∣x)[logpθ(x,z)−logqϕ(z∣x)](15)= bigtriangledown_ phi text{E}_{q_ phi(z| x)} begin{bmatrix} text{log}p_ theta(x,z)- text{log}q_{ phi}(z|x) end{bmatrix} tag{15}=▽ϕ​Eqϕ​(z∣x)​[logpθ​(x,z)−logqϕ​(z∣x)​](15) . We need to take the derivative of this term w.r.t. $ phi$ in order to get the gradients of the variational parameters ($ phi$). But like in the previous section, we cannot move the gradient inside of the expectation since we are taking the expectation w.r.t the distribution $q_ phi(z | x)$ which is a function of $ phi$. | . So like we hinted in the previous section, our solution is to introduce the reparameterization trick: to express the random variable $z$ as a deterministic variable. . ▽ϕEqϕ(z∣x)[f(z)]=▽ϕEp(ϵ)[f(z)](16a) bigtriangledown_{ phi} mathbb{E}_{q_{ phi}(z|x)}[f(z)] = bigtriangledown_{ phi} mathbb{E}_{p( epsilon)}[f(z)] tag{16a}▽ϕ​Eqϕ​(z∣x)​[f(z)]=▽ϕ​Ep(ϵ)​[f(z)](16a) . =Ep(ϵ)[▽ϕf(z)](16b)= mathbb{E}_{p( epsilon)}[ bigtriangledown_{ phi}f(z)] tag{16b}=Ep(ϵ)​[▽ϕ​f(z)](16b) . ∼▽ϕf(z)(16c) sim bigtriangledown_{ phi}f(z) tag{16c}∼▽ϕ​f(z)(16c) . . So to summarize: . Instead of directly sampling from the approximate posterior distribution, which involves a non-differentiable stochastic operation, the trick involves reparameterizing the sampling process to make it differentiable. By applying the reparameterization trick, the gradients of the ELBO with respect to the parameters of the approximate posterior can be estimated, allowing for efficient optimization of the VAE. . Choosing the variational family for $q( cdot)$ . The complexity of the family determines the complexity of the optimization; it is more difficult to optimize over a complex family than a simple family. . A common choice of the form $q_ phi(z | x^i)$ is a multivariate Gaussian with a diagonal covariance structure. Therefore, we will assume that the posterior is a k-dimensional Gaussian with diagonal covariance. So the posterior should take the form: | . z∼qϕ(z∣xi)=N(z;μi,σ2(i)I)z sim q_ phi(z|x^{i}) = N(z; mu^{i}, sigma^{2(i)}I)z∼qϕ​(z∣xi)=N(z;μi,σ2(i)I) . z=μ+σ⊙ϵz = mu + sigma odot epsilonz=μ+σ⊙ϵ . where $ epsilon sim N(0, I)$ and $ odot$ is the element-wise product. Recall that the multivariate normal density is defined as: . 1(2π)kdet∑exp⁡(−12(x−μ)T∑−1(x−μ)) frac{1}{ sqrt{(2 pi)^{k} text{det} sum}} exp(- frac{1}{2}(x- mu)^{T} sum text{}^{-1}(x- mu))(2π)kdet∑ . ​1​exp(−21​(x−μ)T∑−1(x−μ)) . and that our $D_{KL}$ can be defined as: . DKL(q(z)∣∣p(x))=∫q(z)log(q(z)p(x))dxD_{KL}(q(z)||p(x)) = int q(z) log( frac{q(z)}{p(x)})dxDKL​(q(z)∣∣p(x))=∫q(z)log(p(x)q(z)​)dx . Given that we have defined $q(z)$ as belonging to a multivariate Gaussian, we can use the multivariate normal density as our definition of $q(z)$. So just plugging the following into our equation for $D_{KL}$ . q(z)=1(2π)kdet∑exp⁡(−12(z−μ)T∑−1(z−μ))q(z) = frac{1}{ sqrt{(2 pi)^{k} text{det} sum}} exp(- frac{1}{2}(z- mu)^{T} sum text{}^{-1}(z- mu))q(z)=(2π)kdet∑ . ​1​exp(−21​(z−μ)T∑−1(z−μ)) . log q(z)=−k2log(2π)−12log(det∑)−12(z−μ)T∑−1(z−μ) text{log }q(z) = - frac{k}{2} text{log}(2 pi) - frac{1}{2} text{log} left( text{det} sum right)- frac{1}{2}(z- mu)^{T} sum text{}^{-1}(z- mu)log q(z)=−2k​log(2π)−21​log(det∑)−21​(z−μ)T∑−1(z−μ) . log p(z)=−k2log(2π)−12log(det∑)−12(x−μ)T∑−1(x−μ) text{log }p(z) = - frac{k}{2} text{log}(2 pi) - frac{1}{2} text{log} left( text{det} sum right)- frac{1}{2}(x- mu)^{T} sum text{}^{-1}(x- mu)log p(z)=−2k​log(2π)−21​log(det∑)−21​(x−μ)T∑−1(x−μ) . The result: . DKL(qϕ(z∣xi)∣∣pθ(z))=12[∑j=1kσj2(i)+∑j=1kμj2(i)−k−ln∏j=1kσj2(i)](17)D_{KL}(q_ phi(z|x^{i})||p_ theta(z))= frac{1}{2} begin{bmatrix} sum_{j=1}^{k} sigma_j^{2(i)} + sum_{j=1}^{k} mu_j^{2(i)} - k-ln prod_{j=1}^{k} sigma_j^{2(i)} end{bmatrix} tag{17}DKL​(qϕ​(z∣xi)∣∣pθ​(z))=21​[∑j=1k​σj2(i)​+∑j=1k​μj2(i)​−k−ln∏j=1k​σj2(i)​​](17) . The general form of the KL divergence for k-dimensional Gaussians is: . DKL(N0∣∣N1)=12[tr(∑1−1∑0)+(μ1−μ0)T∑1−1(μ1−μ0)−k+ln(det∑1det∑0)](18)D_{KL}(N_0||N_{1)}= frac{1}{2} begin{bmatrix} text{tr}( sum_1^{-1} sum_0)+( mu_1- mu_0)^T sum_1^{-1}( mu_1- mu_0)-k+ text{ln}( frac{ text{det} sum_1}{ text{det} sum_0}) end{bmatrix} tag{18}DKL​(N0​∣∣N1)​=21​[tr(∑1−1​∑0​)+(μ1​−μ0​)T∑1−1​(μ1​−μ0​)−k+ln(det∑0​det∑1​​)​](18) . More Complexity: Normalizing Flows at a High Level . Normalizing flows deserves its own post, but lets briefly explain the concept. . Variational inference searches for the best posterior approximation within a parametric family of distributions. Hence, the true posterior distribution can only be recovered exactly if it happens to be in the chosen family. Above, we used a simple variational family of diagonal covariance Gaussian distributions. We know that the better the variational approximation to the posterior the tighter the ELBO. However, with such simple variational distributions the ELBO will be fairly loose, resulting in biased maximum likelihood estimates of the model parameters $ theta$. See the figure below: . . Since more complex variational families enable better posterior approximations, resulting in improved model performance, designing tractable and more expressive variational families is an important problem in variational inference. As a result, Rezende and Mohamed introduced a general framework for constructing more flexible variational distributions, called normalizing flows. Normalizing flows transform a base density through a number of invertible parametric transformations with tractable Jacobians into more complicated distributions.5 . Some final intuition behind the two-term loss . For standard autoencoders, we simply need to learn an encoding which allows us to reproduce the input. This means we only need to minimize the reconstruction loss, which should achieve class separation of the input samples. However, with VAEs that learn a smooth latent state representations of the input data, we want to avoid areas in latent space which don’t represent any of the observed data. So we cannot rely only on reconstruction loss. . On the other hand, if we only focus only on ensuring that the latent distribution is similar to the prior distribution (through our KL divergence loss term), we end up describing every observation using the same unit Gaussian, which we subsequently sample from to describe the latent dimensions visualized. This effectively treats every observation as having the same characteristics; in other words, we’ve failed to describe the original data. . However, when the two terms are optimized simultaneously, we’re encouraged to describe the latent state for an observation with distributions close to the prior but deviating when necessary to describe salient features of the input. Equilibrium is reached by the cluster-forming nature of the reconstruction loss, and the dense packing nature of the KL loss, forming distinct clusters the decoder can decode. This is great, as it means when randomly generating, if you sample a vector from the same prior distribution of the encoded vectors, N(0, I), the decoder will successfully decode it. And if you’re interpolating, there are no sudden gaps between clusters, but a smooth mix of features a decoder can understand. . def vae_loss(input_img: np.array, output: np.array) -&gt; float: reconstruction_loss = np.sum(np.square(output-input_img)) kl_loss = -0.5 * np.sum(1 + log_stddev - np.square(mean) - np.square(np.exp(log_stddev)), axis=-1) total_loss = np.mean(reconstruction_loss + kl_loss) return total_loss . Other Useful References . Diederik P. Kingma, &amp; Max Welling (2019). An Introduction to Variational Autoencoders_. Foundations and Trends in Machine Learning, 12(4), 307–392. https://arxiv.org/abs/1906.02691 | Carl Doersch. (2021). Tutorial on Variational Autoencoders. https://arxiv.org/abs/1606.05908 | Vlodymyr Kuleshov &amp; Stefano Ermon’s notes form Stanford CS228 on VAE: https://ermongroup.github.io/cs228-notes/extras/vae/ | Rianne van den Berg’s 2021 talk on Variational Inference and VAEs- https://www.youtube.com/watch?v=-hcxTS5AXW0. | https://blog.evjang.com/2016/08/variational-bayes.html | David M. Blei, Alp Kucukelbir, &amp; Jon D. McAuliffe (2017). Variational Inference: A Review for Statisticians_. Journal of the American Statistical Association, 112(518), 859–877. &#8617; . | Approximate inference is needed since the evidence (denominator in equation for Bayes rule) $p(x)$ is usually intractable. &#8617; . | Bishop, C. (2006). Pattern Recognition and Machine Learning. &#8617; . | Kingma, D., &amp; Welling, M. (2014). Auto-Encoding Variational Bayes. https://arxiv.org/abs/1312.6114. &#8617; . | Danilo Jimenez Rezende, &amp; Shakir Mohamed. (2016). Variational Inference with Normalizing Flows. &#8617; . |",
            "url": "https://jackhmiller.github.io/My-DS-Blog/2020/02/01/vae.html",
            "relUrl": "/2020/02/01/vae.html",
            "date": " • Feb 1, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://jackhmiller.github.io/My-DS-Blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Learning Rate Schedulers",
            "content": "Introduction . Many students and practitioners primarily focus on optimization algorithms for how to update the weight vectors rather than on the rate at which they are being updated. Nonetheless, adjusting the learning rate is often just as important as the actual algorithm. Learning rate ($ eta$) (LR) as a global hyperparameter determines the size of the steps which a GD optimizer takes along the direction of the slope of the surface derived from the loss function downhill until reaching a (local) minimum (valley). . Choosing a proper LR can be difficult. A too small LR may lead to slow convergence, while a too large learning rate can deter convergence and cause the loss function to fluctuate and get stuck in a local minimum or even to diverge. Due to the difficulty of determining a good LR policy, the constant LR is a baseline default LR policy for training DNNs in several deep learning frameworks (e.g., TensorFlow, PyTorch). Empirical approaches are then used manually in practice to find good LR values through trials and errors. Moreover, due to the lack of relevant systematic studies and analyses, the large search space for LR parameters often results in huge costs for this hand-tuning process, impairing the efficiency and performance of DNN training. . But what if we wanted a learning rate that was a bit more dynamic than a fixed floating point number? . Numerous efforts have been engaged to enhance the constant LR by incorporating a multistep dynamic learning rate schedule, which attempts to adjust the learning rate during different stages of DNN training by using a certain type of annealing techniques.1 This is especially challenging, given that good LR schedules need to adapt to the characteristics of different datasets and/or different neural network models. Further, as seen below different LR policies will result in different optimization paths, since even though initially different LR functions product similar results, as the number of iterations increases the accumulated impact of the LR updates could also lead to sub-optimal results.2 It might be that high LRs introduce high “kinetic energy” into the optimization and thus model parameters are bouncing around chaotically. . ![[LR_policies.png]] . In this post, we will review some learning rate functions and their associated LR policies by examining their range parameters, step parameters, and value update parameters. We will divide the LR schedulers into 3 categories: fixed, decaying, and cyclic. . Fixed Schedulers . The simplest LR scheduler is time-based. For example, step decay is scheduler that adjusts the learning rate after a fixed number of steps, reducing the learning rate by a specified factor. This is useful for situations where the learning rate needs to decrease over time to allow the model to converge. . The mathematical form of time-based decay is as follows: . lr=lr0/(1+kt)lr = lr_0/(1+kt)lr=lr0​/(1+kt) . Where $lr$, $k$ are hyperparameters and $t$ is the iteration number. In the Keras source code, the SGD optimizer takes a decay and lr arguments to update the LR by a decreasing factor each epoch3: . lr *= (1. / (1. + self.decay * self.iterations)) . Here is an implementation of a similar fixed schedule called step decay, with the following mathematical formulation: . lr=lr0∗dropfloor(epochepochs drop)lr = lr_0 * drop^{floor( frac{epoch}{ text{epochs drop}})}lr=lr0​∗dropfloor(epochs dropepoch​) . class StepLR: def __init__(self, optimizer, step_size, gamma): self.optimizer = optimizer self.step_size = step_size self.gamma = gamma self.last_step = 0 def step(self, current_step): if current_step - self.last_step &gt;= self.step_size: for param_group in self.optimizer.param_groups: param_group[&#39;lr&#39;] *= self.gamma self.last_step = current_step optimizer = # SGD, Adam, etc. scheduler = StepLR(optimizer, step_size=30, gamma=0.1) for epoch in range(num_epoch): # train... scheduler.step(epoch) . Decaying Schedules . One of the most widely used decaying schedulers is exponential decay: This scheduler adjusts the learning rate by a specified factor after each iteration. The learning rate decreases exponentially over time, which is useful for models that require a gradually decreasing learning rate. A use-case of this would be a larger learning rate to explore the loss surface and find one or more minima, where a slowing LR would help the loss function settle into the minimum rather than oscillating. It can be calculated as follows: . lr=lr0∗e(−kt)lr = lr_0 * e^{(−kt)}lr=lr0​∗e(−kt) . Where $lr$, $k$, are hyperparameters and $t$ is again the iteration number. . In the code below, in each epoch the step method updates the learning rate of the optimizer by multiplying it with the decay rate raised to the power of the epoch number. . import math class ExponentialLR: def __init__(self, optimizer, gamma, last_epoch=-1): self.optimizer = optimizer self.gamma = gamma self.last_epoch = last_epoch def step(self, epoch): self.last_epoch = epoch for param_group in self.optimizer.param_groups: param_group[&#39;lr&#39;] *= param_group[&#39;lr&#39;] * self.gamma ** (epoch + 1) optimizer = # SGD, Adam, etc. scheduler = ExponentialLR(optimizer, gamma=0.95) for epoch in range(num_epoch): # train... scheduler.step(epoch) . Cyclic Schedules . Cyclic schedules set the LR of each parameter group according to cyclical learning rate policy where the policy cycles the LR between two boundaries with a constant frequency, as detailed in the paper by Leslie Smith.4 . A classic example of such a scheduler is Cosine Annealing. This scheduler adjusts the learning rate according to a cosine annealing schedule, which starts high and decreases over time to zero. This is useful for models that require a gradually decreasing learning rate but with a more gradual decline in the latter stages of training. . import math class CosineAnnealingLR: def __init__(self, optimizer, T_max, eta_min=0): &quot;&quot;&quot; T_max:: the maximum number of steps over which the learning rate will decrease from its initial value to eta_min eta_min:: the minimum value of the learning rate LR equation: eta_min + (1 - eta_min) * (1 + cos(pi * current_step / T_max)) / 2 &quot;&quot;&quot; self.optimizer = optimizer self.T_max = T_max self.eta_min = eta_min self.current_step = 0 def step(self): self.current_step += 1 lr = self.eta_min + (1 - self.eta_min) * (1 + math.cos(math.pi * self.current_step / self.T_max)) / 2 for param_group in self.optimizer.param_groups: param_group[&#39;lr&#39;] = lr optimizer = # SGD, Adam, etc. scheduler = CosineAnnealingLR(optimizer, T_max=100, eta_min=0.00001) for epoch in range(num_epoch): # train... scheduler.step(epoch) . For example see Akhilesh Gotmare, Nitish Shirish Keskar, Caiming Xiong, &amp; Richard Socher (2018). A Closer Look at Deep Learning Heuristics: Learning rate restarts, Warmup and Distillation_. CoRR, abs/1810.13243. &#8617; . | Wu Y, Liu L, Bae J, et al (2019). Demystifying Learning Rate Policies for High Accuracy Training of Deep Neural Networks. https://arxiv.org/pdf/1908.06477.pdf &#8617; . | https://github.com/keras-team/keras/blob/master/keras/optimizers/sgd.py &#8617; . | Leslie N. Smith (2015). No More Pesky Learning Rate Guessing Games_. CoRR, abs/1506.01186. &#8617; . |",
            "url": "https://jackhmiller.github.io/My-DS-Blog/2020/01/06/LR.html",
            "relUrl": "/2020/01/06/LR.html",
            "date": " • Jan 6, 2020"
        }
        
    
  
    
  
    
        ,"post10": {
            "title": "Proving Proximal Gradient Method's Convergence Rate and a Code Demonstration",
            "content": "Proximal Gradient Descent . A proximal algorithm is an algorithm for solving a convex optimization problem that uses the proximal operators of the objective terms. It is called such since it consists of a gradient step followed by a proximal mapping. There are three main benefits to the application of proximal algorithms: . They work under extremely general conditions, including cases where the functions are nonsmooth and extended real-valued | | They can be fast, since there can be simple proximal operators for functions that are otherwise challenging to handle in an optimization problem | | They are amenable to distributed optimization, so they can be used to solve very large scale problems | | . The proximal operator is defined as $$ prox_f(x) = argmin left { f(u) + frac{1}{2} left | u-x right |^2: u in mathbb{R}^n right }, forall x in mathbb{R}^n $$ with the goal being to $$minimize left { f(u) + h(u): u in mathbb{R}^n right }$$ where h is a proper lower semi-continuous function and f is a smooth convex function on dom(h). . Some important assumptions before we begin: . We assume that f has L-Lipschitz continuous gradient, i.e., $$ left | bigtriangledown f(x) - bigtriangledown f(y) right | leq L left | x-y right |, forall x, y in dom(h)$$ and hence for every $x, y in dom(h)$, $$ f(x) leq l_f(x; y) + frac{L}{2} left | x-y right |^2$$ where $l_f(x; y) := f(y) + left langle bigtriangledown f(y), x-y right rangle$. . Recal that PGM with a constant prox stepsize is recursive in nature and iterates according to : $$x_{k+1}=prox_{ lambda h}(x_k- lambda nabla f(x_k)).$$ . Let&#39;s get started! . First, we will derive a single iteration of PGM and prove that it is strong convex. . $$ x_{k+1} = argmin left { h(u) + frac{1}{2} left | u-(x_k- bigtriangledown f(x_k)) right |^2 right }$$ . $$ x_{k+1} = argmin left { f(x_k) + left langle bigtriangledown f(x_k), u-x_k right rangle +h(u) + frac{1}{2} left | x-x_k right |^2 right }$$ . $$x_{k+1}= argmin left { ell_f(u;x_k)+h(u) + frac{1}{2 lambda}||u-x_k||^2 right }, $$ . And proving strong convexity $ left langle bigtriangledown h(u) - bigtriangledown h(x), u - x right rangle geq lambda left | u-x right |^{2}$: . $$ left langle prox_{ lambda h}(u) - prox_{ lambda h}(x), (u- frac{1}{ lambda} bigtriangledown h(u)) -(x- frac{1}{ lambda} bigtriangledown h(x)) right rangle geq left | prox_{ lambda h}(u)-prox_{ lambda h}(x) right |^{2} $$ . $$ left langle (u - frac{1}{ lambda} bigtriangledown h(u)) - (x - frac{1}{ lambda} bigtriangledown h(x)), (u- frac{1}{ lambda} bigtriangledown h(u)) -(x- frac{1}{ lambda} bigtriangledown h(x)) right rangle geq left | (u - frac{1}{ lambda} bigtriangledown h(u)) - (x - frac{1}{ lambda} bigtriangledown h(x)) right |^{2} $$ . Using the definition of $x_{k+1}$ and the strong convexity, we obtain upon rearranging terms that: . $$ h(x_{k+1}) leq h(x) + left langle - bigtriangledown f(u), x^{k+1}-x right rangle + frac{1}{2} left | u-x right |^2 - frac{1}{2} left | u-x^{k+1} right |^2 - frac{1}{2} left | x^{k+1}-x right |^2 $$ . Due to the Lipschitz continuity: $$ f(x_{k+1}) leq f(u) + left langle - bigtriangledown f(u), u-x^{k+1} right rangle + frac{1}{2} left | u-x_{k+1} right |^2 $$ . Adding the two: $$ f(x_{k+1}) + h(x_{k+1}) leq f(u) + h(x) + left langle bigtriangledown f(u), u-x right rangle - frac{1}{2} left | x_{k+1}-x right |^2 + frac{1}{2} left | u-x right |^2 $$ . Using definition for $ ell_f(u;x_k)$ $$ ell_f(u;x_k)+h(u) + frac{1}{2}||u-x_k||^2 geq ell_f(x_{k+1};x_k)+h(x_{k+1})+ frac{1}{2}||x_{k+1}-x_k||^2 + frac{1}{2}||u-x_{k+1}||^2$$ . Similarly for any x in int(dom(f)): $$ f(x_*) leq f(x) + left langle bigtriangledown f(x), x_*-x right rangle + frac{1}{2 lambda} left | x_*-x right |^2 $$ It holds that $$ (f+h)(x)-(f+h)(x_*) geq frac{1}{2 lambda} left | x-x_* right |^2 $$ . Consider $$ g(u) = f(x_{k+1})+ left langle bigtriangledown f(x_{k+1}), u-x_{k+1} right rangle + g(u)+ frac{1}{2 lambda} left | u-x_{k+1} right |^2$$ . $ x_* = argmin_g(u) $ $$ g(x)-g(x_*) geq frac{1}{2 lambda} left | x-x_* right |^2 $$ . Since $$ g(x_*) = f(x_{k+1}) + left langle bigtriangledown f(x_{k+1}),x_*-x_{k+1} right rangle + frac{1}{2 lambda} left | x_*-x_{k+1} right |^2 + h(x_*) $$ $$ geq f(x_*)+h(x_*) = (f+h)(x_*) $$ . This implies that $$ h(x_{k+1})-(f+h)(x_*) geq frac{1}{2 lambda} left | x_{k+1}-x_* right |^2 $$ . Plugging for g(u) into above inequality . $$ f(x_{k+1}) + left langle bigtriangledown f(x_{k+1}), x-x_{k+1} right rangle + h(x)+ frac{1}{2 lambda} left | x-x_{k+1} right |^2 -(f+h)(x_*) geq frac{1}{2 lambda} left | x-x_* right |^2$$ . Which is equal to . $$ (f+h)(x_{k+1})-(f+h)(x_*) geq frac{1}{2 lambda} left | x_{k+1}-x_* right |^2 - frac{1}{2 lambda} left | x-x_{k+1} right |^2 +f(x_{k+1}) + ell_f(x;x_{k+1}) $$ . $$(f+h)(x_*)+ frac{1}{2 lambda}||x_k-x_*||^2 geq (f+h)(x_{k+1})+h(x_{k+1})+ frac{1}{2 lambda}||x_{k+1}-x_*||^2$$ . Using $$ frac{1}{2 lambda}((f+h)(x_*)-(f+h)(x_{k+1})) geq left | x_*-x_{k+1} right |^2 - left | x_*-x_k right |^2 + frac{1}{2 lambda} ell_f(x_*,x_k)$$ . $$ frac{1}{2 lambda}((f+h)(x_*)-(f+h)(x_{k+1})) geq left | x_*-x_{k+1} right |^2 - left | x_*-x_k right |^2 $$ . Sum over all n from 0 to k to obtain: $$ frac{1}{2 lambda} sum_{}^{k}(f+h)(x_*)-(f+h)(x_{k+1}) geq left | x_*-x_k right |^2- left | x_*-x_0 right |^2 $$ . Thus $$ sum_{}^{k}((f+h)(x_{k+1})-(f+h)(x_*)) leq frac{1}{2 lambda} left | x_*-x_0 right |^2- frac{1}{2 lambda} left | x_*-x_k right |^2 leq frac{1}{2 lambda} left | x_*-x_0 right |^2 $$ . Given the monotonicity of $(f+h)(x_n)$ for $n geq 0$ $$ k((f+h)(x_k)-(f+h)(x_*)) leq sum_{}^{k}((f+h)(x_{k+1})-(f+h)(x_*)) leq frac{1}{2 lambda} left | x_*-x_0 right |^2 $$ . Thus $$ sum_{i=1}^k (f+h)(x_i)-k(f+h)(x_*) leq frac{||x_0-x_*||^2}{2 lambda} $$ . Proving PGM has the descent property: . $$(f+h)(x_k) geq (f+h)(x_{k+1}), forall k geq 0 $$ . $$ frac{1}{2 lambda}((f+h)(x_*)-(f+h)(x_{k+1})) geq left | x_*-x_{k+1} right |^2 - left | x_*-x_k right |^2 + frac{1}{2 lambda} ell_f(x_*,x_k)$$ . Along with the relationship: $$ left | x_{k+1} -x_* right | leq left | x_k-x_* right |$$ . It follows that: $$ (f+h)(x_*)-(f+h)(x_{k+1}) leq (f+h)(x_*)-(f+h)(x_{k}) leq 0$$ . Thus for all k $ geq 0$ $$(f+h)(x_{k+1}) leq (f+h)(x_{k}) $$ . Finally, given the above: $$ k((f+h)(x_k)-(f+h)(x_*)) leq sum_{}^{k}((f+h)(x_{k+1})-(f+h)(x_*)) leq frac{1}{2 lambda} left | x_0-x_* right |^2 $$ Consequently $$ (f+h)(x_i)-(f+h)(x_*) leq frac{1}{k2 lambda}||x_0-x_*||^2 $$ . Hence we obtain the $O( frac{1}{k})$ convergence rate . . Code Example . Here we will employ proximal gradient descent with stochastic schemes. In general, when the loss function we are trying to minimize can be wwritten in the form $ sum_{i=1}^{m}g_i( theta )$ where each $g_i( theta)$ is the loss sample at i, and the training time is long, then stochastic schemes should be considered. We will optimize $$f( theta) = underset{ theta in mathbb{R}^d}{min} frac{1}{m} sum_{i=1}^{m} left [ log(1+exp(x_i theta)) -y_ix_i theta right ] + lambda left | theta right |_1$$ We decompose $f( theta)$ into a convex and differentiable function g and a convex but not differentiable function h: $$ g( theta) = frac{1}{m} sum_{i=1}^{m}log(1+exp(x_i theta)) $$ $$ h( theta) = frac{1}{m} sum_{i=1}^{m} -y_ix_i theta + lambda left | theta right |_1$$ . The data we are using is from the classic MNIST machine learning dataset. There are two classes, 0 and 1, and we have a total of 14,780 images; a training set of 12,665 and a test set of 2,115. Each image is 28x28. Each image is vectorized and stacked to form a training and test matrix, with the label appended to the last column of each matrix. Thus, our classifier will learn $ theta$ on the train set to predict the labels for the test set. . import numpy as np from sklearn.metrics import accuracy_score . x_train = train[:, :-1] y_train = train[:, -1 :] x_test = test[:, :-1] y_test = test[:, -1 :] x = x_train y = y_train . def predict_labels(X, weights): return 1/(1+np.exp(-X.dot(weights))) def soft_threshold(x,t): pos = np.maximum(x - t, 0) neg = np.minimum(x + t, 0) return pos+neg def log_loss(X, theta): return np.sum(np.log(1 + np.exp(X.dot(theta)))) / X.shape[0] def h(X, y, lam=10, lr=0.01): return (1/len(X))*(-y.T.dot(X)) + lam*lr def evaluate_gradient(X, theta, y=None): return np.sum((X*np.exp(X.dot(theta))) / (1 + np.exp(X.dot(theta))), axis=0)/m . n = 100 lam = 10 lr= 0.01 max_iters=1000 tol= 1e-3 N, D = x.shape theta_current = np.zeros(shape=(D, 1)) losses = [log_loss(x, theta_current)] thetas = [theta_current] iterations = 1 while (loss &gt; tol) or (iterations &gt; max_iters): theta_current = thetas[-1] # Stochastic number_of_rows = x.shape[0] random_indices = np.random.choice(number_of_rows, size=n, replace=False) x_temp, y_temp = x[random_indices, :], y[random_indices, :] for it in range(n): # Proximal GD grad = evaluate_gradient(x_temp, theta_current).reshape(-1,1) theta_new_grad = theta_current - (lr * grad) theta_new = soft_threshold(theta_new_grad, h(x_temp, y_temp)) theta_current = theta_new loss = log_loss(x, theta_current) losses.append(loss) thetas.append(theta_current) iterations += 1 . n = 100 lam = 10 lr= 0.01 max_iters=1000 tol= 1e-5 N, D = x.shape theta_current = np.zeros(shape=(D, 1)) loss_1 = log_loss(x, theta_current) losses = [loss_1] thetas = [theta_current] iterations = 1 #while losses[-1] &gt; tol: for i in range(200): theta_current = thetas[-1] grad = evaluate_gradient(x, theta_current).reshape(-1,1) theta_new_grad = theta_current - (lr * grad) theta_new = soft_threshold(theta_new_grad, h(x, y).T) theta_current = theta_new loss = log_loss(x, theta_current) losses.append(loss) thetas.append(theta_current) #iterations += 1 . predict_labels(x, thetas[-1]) accuracy_score(y_test, predict_labels(x_test, thetas[-1])) . Overall, this stochastic implementation achieves an accuracy of 93.76 on the training set. .",
            "url": "https://jackhmiller.github.io/My-DS-Blog/2019/12/05/PGD.html",
            "relUrl": "/2019/12/05/PGD.html",
            "date": " • Dec 5, 2019"
        }
        
    
  
    
        ,"post11": {
            "title": "Accelerated Gradient Descent and Newton's Method",
            "content": "Nonconvex optimization problems are ubiquitous in modern machine learning. While it is NP-hard to find global minima of a nonconvex function in the worst case, in the setting of machine learning it has proved useful to consider a less stringent notion of success, namely that of convergence to a first-order stationary point. However, architectures such as deep neural networks induce optimization surfaces that can be teeming with highly suboptimal saddle points. . In this setting, the glaring limitation of gradient descent is that it can get stuck in flat areas or bounce around if the objective function returns noisy gradients. Therefore second-order descent methods are needed for optimization. Momentum is an approach that accelerates the progress of the search to skim across flat areas and smooth out bouncy gradients. In some cases, the acceleration of momentum can cause the search to miss or overshoot the minima at the bottom of basins or valleys. Nesterov momentum is an extension of momentum that involves calculating the decaying moving average of the gradients of projected positions in the search space rather than the actual positions themselves. This has the effect of harnessing the accelerating benefits of momentum whilst allowing the search to slow down when approaching the optima and reduce the likelihood of missing or overshooting it. Further, Nesterov’s accelerated gradient descent (AGD), an instance of the general family of “momentum methods,” provably achieves faster convergence rate than gradient descent (GD) in the convex setting. . $$ L( alpha, beta|y_1,...,y_n) = prod_{i=1}^{n} frac{ Gamma ( alpha+ beta)}{ Gamma( alpha) Gamma( beta)}y_i^{ alpha-1}(1-y_i)^{ beta-1} $$ . Given the above likelihood function, where $ Gamma$ is the gamma function, and $ alpha$ and $ beta$ are greater than zero. First, we convert our likelihood function to the log likelihood. . $$ log(L( alpha, beta | y_{1}, ..., y_{n})) = nlog( Gamma(a+b)) -nlog( Gamma(a)) -nlog( Gamma(b)) +(a-1) sum_{i=1}^{n}log(y_{i}) +(b-1) sum_{i=1}^{n}log(1-y_{i}) $$ . And its maximum likelihood formulation. . $$ widehat{ alpha}, widehat{ beta} = underset{i}{argmax}(log( widehat{L}( alpha, beta | y_{1}, ..., y_{n}))) $$ . And our gradient and Hessian from the log likelihood function: . &emsp; Gradient . $$ bigtriangledown log(L)) = begin{bmatrix} n psi( alpha+ beta) -n psi( alpha) + sum_{i=1}^{n}log(y_{i}) n psi( alpha+ beta) -n psi( beta) + sum_{i=1}^{n}log(1-y_{i}) end{bmatrix} $$ &emsp; Hessian . $$ bigtriangledown^{2} log(L) = begin{bmatrix} n psi&#39;( alpha+ beta) -n psi&#39;( alpha) &amp; n psi&#39;( alpha+ beta) n psi&#39;( alpha+ beta) &amp; n psi&#39;( alpha+ beta) -n psi&#39;( beta) end{bmatrix} $$ Let&#39;s code it up. . For digamma $ psi(x) = frac{ Gamma&#39;(x)}{ Gamma(x)}$ and trigamma $ psi&#39;(x)$, we will cheat and use built-in functions from scipy. . import matplotlib.pyplot as plt import numpy as np import pandas as pd import math from scipy.io import loadmat import seaborn as sns import scipy from sklearn.model_selection import KFold, cross_val_score from random import randrange from scipy.interpolate import BSpline from scipy.ndimage import gaussian_filter %matplotlib inline from scipy.special import digamma, polygamma . Loading the data and coding our objective function, gradient, and Hessian. . Y = loadmat(&#39;Sample_data.mat&#39;)[&#39;y&#39;] . def objective(data, x): n = len(data) return n*math.log(math.gamma(x[0]+x[1]))-n*math.log(math.gamma(x[0]))-n*math.log(math.gamma(x[1])) + (x[0]-1)*sum([math.log(i) for i in data]) + (x[1]-1)*sum([math.log(1-i) for i in data]) def gradient(data, x): n = len(data) return np.array([n* digamma(x[0]+x[1]) - n*digamma(x[0]) + sum([math.log(i) for i in data]), n* digamma(x[0]+x[1]) - n*digamma(x[1]) + sum([math.log(1-i) for i in data])]) def Hessian(data, x): n = len(data) return np.array([[n* polygamma(1, x[0]+x[1]) - n*polygamma(1, x[0]), n* polygamma(1, x[0]+x[1])],[n* polygamma(1, x[0]+x[1]), n* polygamma(1, x[0]+x[1]) - n*polygamma(1, x[1])]]) . &emsp; Newton&#39;s Method . alpha = 1 lam = 1e-10 epsilon = 0.01 x0 = np.random.rand(2) f0 = objective(Y, x0) # initialize fL = [f0] g0 = gradient(Y, x0) # initialize gL = [g0] H0 = Hessian(Y, x0) # initialize Omega = -np.linalg.solve(H0 + lam*np.eye(2),g0) stopping = np.linalg.norm(Omega) iterations = 1 params = [] while stopping &gt; epsilon: x = x0 + alpha*Omega fval = objective(Y, x) while fval &lt; f0: alpha *= 0.1 x = x0 + alpha*Omega fval = objective(Y, x) alpha = alpha**0.5 params.append(x) x0 = x f0 = fval fL.append(fval) g0 = gradient(Y, x0) Omega = -np.linalg.solve(Hessian(Y, x0)+lam*np.eye(2),g0) stopping = np.linalg.norm(Omega) iterations += 1 results = x0 iters = iterations plt.figure(figsize=(20,10)) plt.plot(fL) plt.title(&#39;Newton Method&#39;) plt.xlabel(&#39;Number of iterations&#39;) plt.ylabel(&#39;Log-Likelihood&#39;) . Text(0, 0.5, &#39;Log-Likelihood&#39;) . fig, ax = plt.subplots(figsize=(15,10)) ax.plot([x[0] for x in params], lw=3, label=&#39;Alpha&#39;) ax.plot([x[1] for x in params], lw = 3, label=&#39;Beta&#39;) plt.title(&#39;Alpha &amp; Beta Values vs Number of Iterations&#39;) plt.xlabel(&#39;Number of iterations&#39;) plt.ylabel(&#39;Parameter Value&#39;) ax.legend(loc=&#39;best&#39;) . &lt;matplotlib.legend.Legend at 0x21414a0d940&gt; . print(f&quot; Newton&#39;s Method optimized parameter values- Alpha: {round(x0[0], 3)}, Beta: {round(x0[1], 3)}&quot;) . Newton&#39;s Method optimized parameter values- Alpha: 4.97, Beta: 12.173 . &emsp; Accelerated Gradient Descent . x = np.random.rand(2) beta = np.random.rand(2) x0 = x convergence = gradient(Y, beta)@gradient(Y, beta) convergence_results = [convergence] loss = [objective(Y, x)] alpha = 0.1 epsilon = 0.01 iterations = 1 while convergence &gt; epsilon: x = beta + alpha*gradient(Y, x) beta = x - (iterations-1)/(iterations+2)*(x-x0) convergence = gradient(Y, x)@gradient(Y, x) convergence_results.append(convergence) try: loss.append(objective(Y, x)) except: pass x0 = x iterations += 1 results = x0 iters = iterations plt.figure(figsize=(20,10)) plt.plot(loss) plt.title(&#39;Accelerated Gradient Descent&#39;) plt.xlabel(&#39;Iterations&#39;) plt.ylabel(&#39;Log-Likelihood&#39;) .",
            "url": "https://jackhmiller.github.io/My-DS-Blog/2019/11/28/NM.html",
            "relUrl": "/2019/11/28/NM.html",
            "date": " • Nov 28, 2019"
        }
        
    
  
    
        ,"post12": {
            "title": "Dealing with Class Imbalance in Classification",
            "content": "Introduction . With the rapid increase of large-scale, real-world datasets, it becomes critical to address the problem of long-tailed data distribution (i.e., a few classes account for most of the data, while most classes are under-represented). Solutions have typically adopted class re-balancing strategies such as re-sampling and re-weighting based on the number of observations for each class. Many boosting algorithms that were part of the machine learning boom back 2016-17, which are natural fits for dealing with class imbalances due to their role as ensembles of weak learners, have hyperparameters related to class weights. . So from my perspective, with the initial ML boon in the mid to late 2020s, you had two methods available to you to deal with class imbalances that so often occur in industry: data-level methods and algorithm-level methods. . Data-Level Methods . Data-level methods alter the class distribution in the original data by employing re-sampling strategies to balance the dataset. The simplest forms of resampling include random over-sampling and random undersampling. The former, like the Synthetic Minority Oversampling Technique (SMOTE) for example, handles class imbalance by duplicating the instances in the rare minority class and thus, augmenting the minority class, whereas the latter randomly drops instances from the majority class to match the cardinality of minority class. Experiments suggest that data sampling strategies have little effect on classification performance, however, other results in demonstrate that random oversampling leads to performance improvements.1 . While sampling strategies are widely adopted, these methods manipulate the original class representation of the given domain and introduce drawbacks. Particularly, over-sampling can potentially lead to overfitting and may aggravate the computational burden while under-sampling may eliminate useful information that could be vital for the induction process. For example, Castro et al. demonstrates that strategies adopted to mitigate effects of class imbalance such as undersampling adversely affect probability calibration of minority classes.2 Moreover, a classifier developed by employing sampling methods to artificially balance data may not be applicable to a population with a much difference prevalence rate since the classifier is trained to perform well on balanced data. . Algorithm-Level Methods . Algorithm-level approach involve adjusting the classifier, and can further be categorized into ensemble methods and cost-sensitive methods. The most widely used methods include bagging and boosting ensemble-based methods. For example, with XGBoost, scale_pos_weight value is used to scale the gradient for the positive class. This has the effect of scaling errors made by the model during training on the positive class and encourages the model to over-correct them. In turn, this can help the model achieve better performance when making predictions on the positive class. Pushed too far, it may result in the model overfitting the positive class at the cost of worse performance on the negative class or both classes. As such, the scale_pos_weight can be used to train a class-weighted or cost-sensitive version of XGBoost for imbalanced classification. . A sensible default value to set for the scale_pos_weight hyperparameter is the inverse of the class distribution. For example, for a dataset with a 1 to 100 ratio for examples in the minority to majority classes, the scale_pos_weight can be set to 100. This will give classification errors made by the model on the minority class (positive class) 100 times more impact, and in turn, 100 times more correction than errors made on the majority class. The XGBoost documentation suggests a fast way to estimate this value using the training dataset as the total number of examples in the majority class divided by the total number of examples in the minority class: scale_pos_weight = total_negative_examples / total_positive_examples. . A New Set of Tools Emerges: Optimization-Level Methods . As the ML boom of the mid 2010s turned into the DL revolution of the late 2010s and early 2020s, synergies began to emerge across different subject areas and frameworks. Knowledge and practices invented for DL could be applied to ML settings, while theoretical knowledge gained from the study of ML systems like gradient behaviors could be used to inform the early theoretical work being done on DL. In the theme of our current topic of class imbalance, unique approaches were being developed for specific issues that could simultaneously be applied to a range of problems. For example, I have written in a previous post about Focal Loss, originally designed to solve object detection problems in a computer vision setting. This loss function was quickly applied to other areas to address the common occurrence of class imbalances in binary classification. . These approaches were being developed for deep learning frameworks in the binary classification setting, often manipulating the cross-entropy loss that has become the de-facto loss function for deep binary classification tasks. Although not new from a theoretical or statistical perspective (a class-balanced loss assigns sample weights inversely proportionally to the class frequency3), cost sensitive learning methods seek to reinforce the sensitivity of the classification algorithm towards the under-represented class by building on advancements in ML/DL by incorporating class-wise costs into the objective function of the classification algorithm during training process. So instead of each instance being either correctly or incorrectly classified, each class (or instance) is given a misclassification cost. Thus, instead of trying to optimize the accuracy, the problem is then to minimize the total misclassification cost. . So building off my earlier post about Focal Loss, I would like to present another work that was published around the same time at Google. Like the Focal Loss paper, this novel paper seeks to address a specific issue in DL, but provides a tool that is applicable to any practitioner involves in classification tasks. . Class-Balanced Loss Based on Effective Number of Samples . In a single sentence, proposes a class-wise re-weighting scheme for most frequently used losses (softmax-cross-entropy, focal loss, etc.) giving a quick boost of accuracy, especially when working with data that is highly class imbalanced.4 Their Class-Balanced Loss is designed to address the problem of training from imbalanced data by introducing a weighting factor that is inversely proportional to the effective number of samples. The class-balanced loss term can be applied to a wide range of deep networks and loss functions. . The authors begin the paper by noting that in the context of deep feature representation learning using CNNs, re-sampling may either introduce large amounts of duplicated samples, which slows down the training and makes the model susceptible to overfitting when oversampling, or discard valuable examples that are important for feature learning when under-sampling. I have also found this to be in the case with other neural network architectures as well as gradient-boosting algorithms. Due to these disadvantages of applying re-sampling for CNN training, the authors focus on re-weighting approaches, namely, how to design a better class-balanced loss. . Due to highly imbalanced data, directly training the model or re-weighting the loss by inverse number of samples cannot yield satisfactory performance. Intuitively, the more data, the better. However, since there is information overlap among data, as the number of samples increases, the marginal benefit a model can extract from the data diminishes. . In light of this, the authors propose a novel theoretical framework to characterize data overlap and calculate the effective number of samples in a model and loss-agnostic manner. A class-balanced re-weighting term that is inversely proportional to the effective number of samples is added to the loss function. Extensive experimental results indicate that this class-balanced term provides a significant boost to the performance of commonly used loss functions for training CNNs on long-tailed datasets. . Their theoretical framework is inspired by the random covering problem, where the goal is to cover a large set by a sequence of i.i.d. random small sets. The idea is to capture the diminishing marginal benefits by using more data points of a class. Due to intrinsic similarities among real-world data, as the number of samples grows, it is highly possible that a newly added sample is a near-duplicate of existing samples. . Data Sampling as Random Covering . Given a class, denote the set of all possible data in the feature space of the class as $S$. The volume of $S$ is $N$ and $N geq 1$. The data sampling process is a random covering problem where the more data being sampled, the better the coverage of $S$. The expected total volume of sampled data increases as the number of data increases and is bounded by $N$. . Therefore, the authors define what they call the effective number of samples as an exponential function of the number of samples. with a hyperparameter $ beta in [0, 1)$ controlling how fast the expected volume of samples grows. This expected volume of samples is denoted as the effective number of samples $E_n$, where $n$ is the number of samples, and can be defined as follows: . En=(1−βn)1−β=∑j=1nβj−1(1)E_n = frac{(1- beta^n)}{1- beta} = sum_{j=1}^n beta^{j-1} tag{1}En​=1−β(1−βn)​=j=1∑n​βj−1(1) . This means that the $j^{th}$ sample contributes $ beta^{j-1}$ to the effective number. The total volume N for all possible data in the class can then be calculated as: . N=lim⁡n→∞∑j=1nβj−1=11−β(2)N = lim_{n rightarrow infty} sum_{j=1}^n beta^{j-1} = frac{1}{1- beta} tag{2}N=n→∞lim​j=1∑n​βj−1=1−β1​(2) . . How it Works in Practice . For an input sample $x$ with label $y in {1, 2, 3, …, C}$ where $C$ is the total number of classes, the loss of the model can be denoted as $L(p, y)$ where $p in [0, 1]$ are the estimated class probabilities $p = [p_{1}, p_{2}, …, p_{C}]^T$. Supposing the number of samples for class $i$ is $n_i$, based on (1) then the number of samples for class $i$ is . Eni=1−βini1−βi(3)E_{n_{i}}= frac{1- beta_i^{n_i}}{1- beta_{i}} tag{3}Eni​​=1−βi​1−βini​​​(3) . where $ beta_{i} = frac{N_{i}- 1}{N_i}$. To balance the loss, a normalized weighting factor $ alpha_i$ is introduced that is inversely proportional to the effective number of samples for class $i$: $ alpha_{i} propto frac{1}{E_{n_i}}$. . TLDR: given a sample from class $i$ that contains $n_i$ samples in total, the authors propose to add a weighting factor $ frac{1- beta}{1- beta^{n_i}}$ where $ beta in [0, 1)$ is a hyperparameter. . Finally, the class-balanced (CB) loss can be written as: . CB(p,y)=1EnyL(p,y)=1−β1−βnyL(p,y)(4) text{CB}(p, y) = frac{1}{E_{n_{y}}}L(p,y) = frac{1- beta}{1- beta^{n_{y}}}L(p,y) tag{4}CB(p,y)=Eny​​1​L(p,y)=1−βny​1−β​L(p,y)(4) . where $n_i$ is the number of samples in the ground-truth class $y$. Note that $ beta=0$ corresponds to no re-weighting and $ beta rightarrow 1$ corresponds to re-weighting by inverse class frequency. The proposed novel concept of effective number of samples enables us to use a hyperparameter $ beta$ to smoothly adjust the class-balanced term between no re-weighting and re-weighing by inverse class frequency. . Applications . The proposed class-balanced term is model-agnostic and loss-agnostic in the sense that it’s independent to the choice of loss function and predicted class probabilities. Here is how to apply class-balanced term to two commonly used loss functions: softmax cross-entropy loss and focal loss: . CEsoftmax(z,y)=−log(exp(zy)∑j=1Cexp(zj))(5a) text{CE}_{ text{softmax}}(z, y) = - text{log}( frac{ text{exp}(z_y)}{ sum_{j=1}^{C} text{exp}(z_{j})}) tag{5a}CEsoftmax​(z,y)=−log(∑j=1C​exp(zj​)exp(zy​)​)(5a) . CBsoftmax(z,y)=−1−β1−βnylog(exp(zy)∑j=1Cexp(zj))(5b) text{CB}_{ text{softmax}}(z, y) = - frac{1- beta}{1- beta^{n_{y}}} text{log}( frac{ text{exp}(z_y)}{ sum_{j=1}^{C} text{exp}(z_{j})}) tag{5b}CBsoftmax​(z,y)=−1−βny​1−β​log(∑j=1C​exp(zj​)exp(zy​)​)(5b) . FL(z,y)=−∑i=1C(1−pit)γlog(pit)(6a) text{FL}(z, y) = - sum_{i=1}^{C}(1-p_i^t)^{ gamma} text{log}(p_i^t) tag{6a}FL(z,y)=−i=1∑C​(1−pit​)γlog(pit​)(6a) . CBfocal(z,y)=−1−β1−βny∑i=1C(1−pit)γlog(pit)(6a) text{CB}_{ text{focal}}(z, y) = - frac{1- beta}{1- beta^{n_{y}}} sum_{i=1}^{C}(1-p_i^t)^{ gamma} text{log}(p_i^t) tag{6a}CBfocal​(z,y)=−1−βny​1−β​i=1∑C​(1−pit​)γlog(pit​)(6a) . Code Implementation . import numpy as np import torch import torch.nn.functional as F def focal_loss(labels, logits, alpha, gamma): &quot;&quot;&quot;Compute the focal loss between `logits` and the ground truth `labels`. Focal loss = -alpha_t * (1-pt)^gamma * log(pt) where pt is the probability of being classified to the true class. pt = p (if true class), otherwise pt = 1 - p. p = sigmoid(logit). Args: labels: A float tensor of size [batch, num_classes]. logits: A float tensor of size [batch, num_classes]. alpha: A float tensor of size [batch_size] specifying per-example weight for balanced cross entropy. gamma: A float scalar modulating loss from hard and easy examples. Returns: focal_loss: A float32 scalar representing normalized total loss. &quot;&quot;&quot; BCLoss = F.binary_cross_entropy_with_logits(input=logits, target=labels, reduction=&quot;none&quot;) if gamma == 0.0: modulator = 1.0 else: modulator = torch.exp(-gamma * labels * logits - gamma * torch.log(1 + torch.exp(-1.0 * logits))) loss = modulator * BCLoss weighted_loss = alpha * loss focal_loss = torch.sum(weighted_loss) focal_loss /= torch.sum(labels) return focal_loss def CB_loss(labels, logits, samples_per_cls, no_of_classes, loss_type, beta, gamma): &quot;&quot;&quot;Compute the Class Balanced Loss between `logits` and the ground truth `labels`. Class Balanced Loss: ((1-beta)/(1-beta^n))*Loss(labels, logits) where Loss is one of the standard losses used for Neural Networks. Args: labels: A int tensor of size [batch]. logits: A float tensor of size [batch, no_of_classes]. samples_per_cls: A python list of size [no_of_classes]. no_of_classes: total number of classes. int loss_type: string. One of &quot;sigmoid&quot;, &quot;focal&quot;, &quot;softmax&quot;. beta: float. Hyperparameter for Class balanced loss. gamma: float. Hyperparameter for Focal loss. Returns: cb_loss: A float tensor representing class balanced loss &quot;&quot;&quot; effective_num = 1.0 - np.power(beta, samples_per_cls) weights = (1.0 - beta) / np.array(effective_num) weights = weights / np.sum(weights) * no_of_classes labels_one_hot = F.one_hot(labels, no_of_classes).float() weights = torch.tensor(weights).float() weights = weights.unsqueeze(0) weights = weights.repeat(labels_one_hot.shape[0],1) * labels_one_hot weights = weights.sum(1) weights = weights.unsqueeze(1) weights = weights.repeat(1,no_of_classes) if loss_type == &quot;focal&quot;: cb_loss = focal_loss(labels_one_hot, logits, weights, gamma) elif loss_type == &quot;sigmoid&quot;: cb_loss = F.binary_cross_entropy_with_logits(input=logits, target=labels_one_hot, weights=weights) elif loss_type == &quot;softmax&quot;: pred = logits.softmax(dim = 1) cb_loss = F.binary_cross_entropy(input=pred, target=labels_one_hot, weight=weights) return cb_loss . no_of_classes = 5 logits = torch.rand(10,no_of_classes).float() labels = torch.randint(0,no_of_classes, size = (10,)) beta = 0.9999 gamma = 2.0 samples_per_cls = [2,3,1,2,2] loss_type = &quot;focal&quot; cb_loss = CB_loss(labels, logits, samples_per_cls, no_of_classes, loss_type, beta, gamma) print(cb_loss) . M. Buda, A. Maki, and M. A. Mazurowski, “A systematic study of the class imbalance problem in convolutional neural networks,” Neural Networks, vol. 106, pp. 249–259, 2018. &#8617; . | C. L. Castro and A. P. Braga, “Novel cost-sensitive approach to improve the multilayer perceptron performance on imbalanced data,” IEEE transactions on neural networks and learning systems, vol. 24, no. 6, pp. 888–899, 2013. &#8617; . | C. Huang, Y. Li, C. Change Loy, and X. Tang. Learning deep representation for imbalanced classification. In CVPR, 2016; N. Sarafianos, X. Xu, and I. A. Kakadiaris. Deep imbalanced attribute classification using visual attention aggregation. In ECCV, 2018. &#8617; . | Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, &amp; Serge J. Belongie (2019). Class-Balanced Loss Based on Effective Number of Samples_. CoRR, abs/1901.05555. &#8617; . |",
            "url": "https://jackhmiller.github.io/My-DS-Blog/2019/03/16/Class-Balanced-Loss.html",
            "relUrl": "/2019/03/16/Class-Balanced-Loss.html",
            "date": " • Mar 16, 2019"
        }
        
    
  
    
        ,"post13": {
            "title": "Focal Loss",
            "content": "Motivation . Focal loss (FL) was introduced by Tsung-Yi Lin et al., in their 2018 paper “Focal Loss for Dense Object Detection”.1 Given an image, the object detection algorithms usually have to propose a good number of regions in which potential objects might sit. In R-CNN and Fast R-CNN algorithms, the number of regions proposed is limited intentionally to several thousand. In Faster R-CNN models and other models with CNN region proposal mechanisms, the number of regions proposed could be as high as several hundred thousand. Of course, most of the regions proposed are negative examples where there is no object inside. In R-CNN and Fast R-CNN, because the model is not end-to-end but rather consists of several distinct models, the class imbalanced problem could be solved by sampling more minor class samples or removing major class samples. . However, in state of the art object detection models beginning with Faster R-CNN, the authors believed that the extreme foreground-background class imbalance hinders the first-stage detector from achieving a better performance, where balanced sampling cannot be easily implemented. As such, FL is designed to address scenarios with extreme imbalanced classes, such as one-stage object detection where the imbalance between foreground and background classes can be, for example, 1:1000. . Solution . Therefore, they improve the traditional cross entropy (CE) loss function and devise Focal Loss (FL), which focuses training on hard examples, avoiding the vast number of easy negative examples from overwhelming the detector during training. Hard example here refers to the examples in the training set that are poorly predicted, i.e. being mislabeled by the current version of the classifier. Easy example is exactly the opposite. . How it Works . The default loss function for most classification tasks is CE: . CE(p,y)=−(ylogp+(1−y)log(1−p))(1) text{CE}(p, y) = -(y text{log}p + (1-y) text{log}(1-p)) tag{1}CE(p,y)=−(ylogp+(1−y)log(1−p))(1) . Adding a modulating factor related to $p$ and $ gamma$ and a balanced factor $ alpha$ to CE, FL is obtained as follows: . FL(p,y)=−(αy+(1−α)(1−y))⋅((1−(yp+(1−y)(1−p)))γ)⋅(ylogp+(1−y)log(1−p))(2) text{FL}(p, y) = -( alpha y + (1- alpha)(1-y)) cdot ((1-(yp + (1-y)(1-p)))^ gamma) cdot (y text{log}p + (1-y) text{log}(1-p)) tag{2}FL(p,y)=−(αy+(1−α)(1−y))⋅((1−(yp+(1−y)(1−p)))γ)⋅(ylogp+(1−y)log(1−p))(2) . Or more compactly: . FL(y,p)=−yα(1−p)γlog(p)−(1−y)(1−α)pγlog(1−p)(3)FL(y, p) = -y alpha(1-p)^ gamma text{log}(p) - (1-y)(1- alpha)p^ gamma text{log}(1-p) tag{3}FL(y,p)=−yα(1−p)γlog(p)−(1−y)(1−α)pγlog(1−p)(3) . FL(pt)=−αt(1−pt)γlog(pt)(4) text{FL}(p_t) = - alpha_t(1-p_t)^ gamma log(p_t) tag{4}FL(pt​)=−αt​(1−pt​)γlog(pt​)(4) . where $y in [0, 1]$ is the ground truth label, $p_{t} in [0,1]$ is the model’s predicted probability for the class with label $y=1$, and $ alpha_t$ is defined as: . αt={α∈[0,1]if y=11−αotherwise alpha_t = left { begin{matrix} alpha in [0, 1]&amp; text{if } y=1 1- alpha &amp; text{otherwise} end{matrix} right.αt​={α∈[0,1]1−α​if y=1otherwise​ So we are adding the weight term $ alpha_{t}(1-p_t)^ gamma$ in addition to the cross entropy loss. So when $ gamma$ is 0 then FL is simply the cross entropy loss. As $ gamma$ increases, the focal loss values of easy examples ($p_{t}&gt; 0.5$) are reduced, while hard examples (where $p_{t}« 0.5$) are reduced to a maximum of one quarter, making the classifier concentrate on hard samples. Quoting from the authors:  . “with $ gamma$ = 2, an example classified with $p_t$ = 0.9 would have 100 × lower loss compared with CE and with $p_t$ ≈ 0.968 it would have 1000 × lower loss.”” . So as the confidence of the classification increases, ie. the probability of ground truth cases, samples are considered increasingly “easier” and contribute less and less to the loss. The result is that the hard examples, or where the probability of ground truth is lower, contribute the most to the loss, thus forcing the model to tend to them in order to minimize the loss function. This is conceptually similar to boosting algorithms, where previously incorrectly classified examples receive more weight but in a different context. . . Using FL as an Optimizer . For binary classification tasks, assume the prediction output of our model is pred. Then the corresponding probability is sigmoid(pred) or s(pred) for short. Substituting for $p$ into (2) above we get our evaluation function: . FL(pred,y)=−(αy+(1−α)(1−y))⋅((1−(y⋅s(pred)+(1−y)(1−s(pred))))γ)⋅(ylog(s(pred))+(1−y)log(1−s(pred)))(5) text{FL}(pred, y) = -( alpha y + (1- alpha)(1-y)) cdot ((1-(y cdot s(pred) + (1-y)(1-s(pred))))^ gamma) cdot (y text{log}(s(pred)) + (1-y) text{log}(1-s(pred))) tag{5}FL(pred,y)=−(αy+(1−α)(1−y))⋅((1−(y⋅s(pred)+(1−y)(1−s(pred))))γ)⋅(ylog(s(pred))+(1−y)log(1−s(pred)))(5) . All that is left to optimize according to FL is to calculate the first-order and second-order partial derivative of $ text{FL}(pred, y)$ with respect to pred and take them as the return value of objective loss function. . FL Gradient . Luckily for us, the authors of the paper provide the derivative of focal loss with respect to the model’s output $z$ in appendix B: . ∂FL∂z=αty(1−pt)γ(γptlog(pt)+pt−1)(6) frac{ partial FL}{ partial z} = alpha_ty(1-p_t)^ gamma ( gamma p_t text{log}(p_t) +p_t -1) tag{6}∂z∂FL​=αt​y(1−pt​)γ(γpt​log(pt​)+pt​−1)(6) . But lets try deriving it anyway. So we want to use the chain rule to solve: . ∂FL∂z=∂FL∂pt∂pt∂p∂p∂z(7) frac{ partial FL}{ partial z} = frac{ partial FL}{ partial p_t} frac{ partial p_t}{ partial p} frac{ partial p}{ partial z} tag{7}∂z∂FL​=∂pt​∂FL​∂p∂pt​​∂z∂p​(7) . So solving each component one at a time we get for the first term: . ∂FL∂pt=αtγ(1−pt)γ−1log(pt)−αt(1−pt)γpt(8) frac{ partial FL}{ partial p_{t}}= alpha_t gamma(1-p_t)^{ gamma-1} text{log}(p_{t)-} frac{ alpha_{t}(1-p_{t})^ gamma}{p_t} tag{8}∂pt​∂FL​=αt​γ(1−pt​)γ−1log(pt)−​pt​αt​(1−pt​)γ​(8) . The second component $ frac{ partial p_t}{ partial p}$ relies on the relationship: . pt=p(y+1)2+(1−p)(1−y)2p_{t}= frac{p(y+1)}{2} + frac{(1-p)(1-y)}{2}pt​=2p(y+1)​+2(1−p)(1−y)​ . So our derivative is . ∂pt∂p=y+12+y−12=y(9) frac{ partial p_t}{ partial p} = frac{y+1}{2} + frac{y-1}{2} = y tag{9}∂p∂pt​​=2y+1​+2y−1​=y(9) . Our third term is the derivative of a sigmoid function $p(z) = frac{1}{1+e^{-z}}$ which is just: . ∂p∂z=p(1−p)(10) frac{ partial p}{ partial z} = p(1-p) tag{10}∂z∂p​=p(1−p)(10) . Since $p(1-p)$ is equal to $p_t(1-p_t)$ some terms cancel out and we are left with equation (6). . FL Hessian . We can obtain the second order derivative by differentiating the gradient we have obtained (6) with respect to $z$: . ∂2FL∂z2=∂∂z(∂FL∂z) frac{ partial^2 FL}{ partial z^2} = frac{ partial }{ partial z}( frac{ partial FL}{ partial z})∂z2∂2FL​=∂z∂​(∂z∂FL​) . =∂∂pt(∂FL∂z)×∂pt∂p×∂p∂z(11)= frac{ partial }{ partial p_t}( frac{ partial FL}{ partial z}) times frac{ partial p_t}{ partial p} times frac{ partial p}{ partial z} . tag{11}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;=∂pt​∂​(∂z∂FL​)×∂p∂pt​​×∂z∂p​(11)&lt;/span&gt;&lt;/span&gt; . We have already computed the last two elements of the chain ($y$ and $p(1-p)$ respectfully), so we just have to compute the first element of the chain. We will use the following notation to make our computations a bit cleaner: $ frac{ partial FL}{ partial z} = u times v$ where $u = alpha_{t}y(1-p_t)^ gamma$ and $v = gamma p_t text{log}(p_{t)}+ p_{t}-1$ . Now we compute the derivatives of $u$ and $v$ with respect to $p_t$: . ∂u∂pt=−γαty(1−pt)γ−1(12) frac{ partial u}{ partial p_{t}}= - gamma alpha_ty(1-p_t)^{ gamma-1} tag{12}∂pt​∂u​=−γαt​y(1−pt​)γ−1(12) . ∂v∂pt=γlog(pt)+γ+1(13) frac{ partial v}{ partial p_{t}}= gamma text{log}(p_{t)}+ gamma + 1 tag{13}∂pt​∂v​=γlog(pt)​+γ+1(13) . So our second order derivative, or Hessian, is: . ∂2FL∂z2=(∂u∂pt×v+u×∂v∂pt)×∂pt∂p×∂p∂z(14) frac{ partial^2 FL}{ partial z^2} = ( frac{ partial u}{ partial p_t} times v + u times frac{ partial v}{ partial p_t}) times frac{ partial p_t}{ partial p} times frac{ partial p}{ partial z} tag{14}∂z2∂2FL​=(∂pt​∂u​×v+u×∂pt​∂v​)×∂p∂pt​​×∂z∂p​(14) . Coding FL . Focal loss was initially proposed to resolve the imbalance issues that occur when training object detection models. However, it can and has been used for many imbalanced learning problems. Focal loss is just a loss function, and may thus be used in conjunction with any model that uses gradients, including neural networks and gradient boosting. . To code any loss function in a ML/DL setting, we need the loss expressed mathematically as well as its gradient and hessian (the first and second order derivatives). Here, we will use LightGBM/Catboost, since one of their nice features is that you can provide it with a custom loss function. . As your optimization function: . def focal_loss_lgb(y_pred, dtrain, alpha, gamma): a,g = alpha, gamma y_true = dtrain.label def fl(x,t): p = 1/(1+np.exp(-x)) return -( a*t + (1-a)*(1-t) ) * (( 1 - ( t*p + (1-t)*(1-p)) )**g) * ( t*np.log(p)+(1-t)*np.log(1-p) ) partial_fl = lambda x: fl(x, y_true) grad = derivative(partial_fl, y_pred, n=1, dx=1e-6) hess = derivative(partial_fl, y_pred, n=2, dx=1e-6) return grad, hess . As your evaluation function (as opposed to F1 score): . def focal_loss_lgb_eval_error(y_pred, dtrain, alpha, gamma): a,g = alpha, gamma y_true = dtrain.label p = 1/(1+np.exp(-y_pred)) loss = -( a*y_true + (1-a)*(1-y_true) ) * (( 1 - ( y_true*p + (1-y_true)*(1-p)) )**g) * ( y_true*np.log(p)+(1-y_true)*np.log(1-p) ) # (eval_name, eval_result, is_higher_better) return &#39;focal_loss&#39;, np.mean(loss), False . A more modular/cleaner approach might look like this, courtesy of Max Halford2: . python import numpy as np from scipy import optimize from scipy import special class FocalLoss: def __init__(self, gamma, alpha=None): self.alpha = alpha self.gamma = gamma def at(self, y): if self.alpha is None: return np.ones_like(y) return np.where(y, self.alpha, 1 - self.alpha) def pt(self, y, p): p = np.clip(p, 1e-15, 1 - 1e-15) return np.where(y, p, 1 - p) def __call__(self, y_true, y_pred): at = self.at(y_true) pt = self.pt(y_true, y_pred) return -at * (1 - pt) ** self.gamma * np.log(pt) def grad(self, y_true, y_pred): y = 2 * y_true - 1 # {0, 1} -&gt; {-1, 1} at = self.at(y_true) pt = self.pt(y_true, y_pred) g = self.gamma return at * y * (1 - pt) ** g * (g * pt * np.log(pt) + pt - 1) def hess(self, y_true, y_pred): y = 2 * y_true - 1 # {0, 1} -&gt; {-1, 1} at = self.at(y_true) pt = self.pt(y_true, y_pred) g = self.gamma u = at * y * (1 - pt) ** g du = -at * y * g * (1 - pt) ** (g - 1) v = g * pt * np.log(pt) + pt - 1 dv = g * np.log(pt) + g + 1 return (du * v + u * dv) * y * (pt * (1 - pt)) def init_score(self, y_true): res = optimize.minimize_scalar( lambda p: self(y_true, p).sum(), bounds=(0, 1), method=&#39;bounded&#39; ) p = res.x log_odds = np.log(p / (1 - p)) return log_odds def lgb_obj(self, preds, train_data): y = train_data.get_label() p = special.expit(preds) return self.grad(y, p), self.hess(y, p) def lgb_eval(self, preds, train_data): y = train_data.get_label() p = special.expit(preds) is_higher_better = False return &#39;focal_loss&#39;, self(y, p).mean(), is_higher_better . `` . T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollar, “Focal loss ´ for dense object detection,” in Proceedings of the IEEE international conference on computer vision, 2017, pp. 2980–2988. &#8617; . | https://maxhalford.github.io/blog/lightgbm-focal-loss/ &#8617; . |",
            "url": "https://jackhmiller.github.io/My-DS-Blog/2019/03/11/Focal-loss.html",
            "relUrl": "/2019/03/11/Focal-loss.html",
            "date": " • Mar 11, 2019"
        }
        
    
  
    
        ,"post14": {
            "title": "Non-Max Suppression",
            "content": "Introducing NMS . Non Maximum Suppression (NMS) is a technique used in numerous computer vision tasks. It is a class of algorithms to select one entity (e.g., bounding boxes) out of many overlapping entities. We can choose the selection criteria to arrive at the desired results. The criteria are most commonly some form of probability number and some form of overlap measure (e.g. Intersection over Union). . Most object detection algorithms use NMS to whittle down many detected bounding boxes to only a few. At the most basic level, most object detectors do some form of windowing. Thousands of windows (anchors) of various sizes and shapes are generated. . These windows supposedly contain only one object, and a classifier is used to obtain a probability/score for each class. Once the detector outputs a large number of bounding boxes, it is necessary to filter out the best ones. NMS is the most commonly used algorithm for this task. . The Intersection over Union (IoU) metric, also referred to as the Jaccard index, is essentially a method used usually to quantify the percent overlap between the ground truth BBox (Bounding Box) and the prediction BBox. However, in NMS, we find IoU between two predictions BBoxes instead. . Implementation of IoU in Python . We are given two boxes, box1 and box2, both defined by two coordinate tuples: (x, y) and (a, b) The first tuple is the lower left coordinate, and the second tuple is the upper right coordinates. . So the area of each bounding box is as follows: . area = (a-x) * (b-y) . Now we need to find the intersection box. To do that, find the largest (x, y) coordinates for the start of the intersection bounding box and the smallest (x, y) coordinates for the end of the intersection bounding box. . # box1 (x1,y1) (a1,b1) # box2 (x2,y2) (a2,b2) area1 = (a1-x1)*(b1-y1) area2 = (a2-x2)*(b2-y2) xx = max(x1, x2) yy = max(y1, y2) aa = min(a1, a2) bb = min(b1, b2) . So the intersection BBox has the coordinates (xx,yy) (aa,bb). Now we compute the width and height of the intersection bounding box, and use it to calculate the intersection area: . w = max(0, aa - xx) h = max(0, bb - yy) intersection_area = w*h . Now we find the union area of box boxes and compute the ratio of overlap between the computed bounding box and the bounding box in the area list . union_area = area1 + area2 - intersection_area IoU = intersection_area / union_area . The NMS Algorithm . Input . We get a list P of prediction BBoxes of the form (x1,y1,x2,y2,c), where (x1,y1) and (x2,y2) are the ends of the BBox and c is the predicted confidence score of the model. We also get overlap threshold IoU thresh_iou. . Output . A list of filtered prediction BBoxes . Pseudo Code . Initialize empty list $K$ for return values | Select prediction $S$ with the highest confidence score from $P$, and append it to $K$ | Compare prediction $S$ with all predictions present in $P$ by calculating the IoU of $S$ with every prediction $P$. If the IoU is greater than the threshold thresh_iou for any prediction T present in P, remove prediction T from P. | If there are still predictions left in P, then go to Step 1 again, else return the list keep containing the filtered predictions. | Code implementation . Lets define our function, and write the first few lines which will extract the coordinates and confidence scores for every prediction box present: . def non_max_suppression(P: torch.tensor, iou_thresh: float): x1 = P[:, 0] y1 = P[:, 1] x2 = P[:, 2] y2 = P[:, 3] scores = P[:, 4] . Now we calculate the area of all BBoxes, and initialize our list $K$ for the return values: . areas = (x2-x1) * (y2-y1) order = scores.argsort() K = [] . Now we begin to iterate over our BBoxes, beginning with step 2 from above by extracting the index of the prediction with the highest score. . while len(order) &gt; 0: idx = order[-1] K.append(P[idx]) order = order[order: -1] . Now we get the coordinates according to the indicies by order using Torch.index_select . xx1 = torch.index_select(x1,dim = 0, index = order) xx2 = torch.index_select(x2,dim = 0, index = order) yy1 = torch.index_select(y1,dim = 0, index = order) yy2 = torch.index_select(y2,dim = 0, index = order) . Now we find the intersection of the BBox S and all the predictions left in P . xx1 = torch.max(xx1, x1[idx]) # essentially max(S.x1, T.x1) yy1 = torch.max(yy1, y1[idx]) xx2 = torch.min(xx2, x2[idx]) yy2 = torch.min(yy2, y2[idx]) w = xx2 - xx1 h = yy2 - yy1 # Clip lower bound at 0 to avoid negative values w = torch.clamp(w, min=0.0) h = torch.clamp(h, min=0.0) . Finally, we calculate the IoU between $S$ and all other BBoxes in $P$ . intersect = w*h rem_areas = torch.index_select(areas, dim = 0, index = order) union = (rem_areas - inter) + areas[idx] IoU = inter / union # keep the boxes with IoU less than iou_thresh mask = IoU &lt; iou_thresh order = order[mask] . Experiment . Given . P = torch.tensor([ [1, 1, 3, 3, 0.95], [1, 1, 3, 4, 0.93], [1, 0.9, 3.6, 3, 0.98], [1, 0.9, 3.5, 3, 0.97] ]) . we can plot the BBoxes to get this: Now we run our function with iou_thresh as 0.5: .",
            "url": "https://jackhmiller.github.io/My-DS-Blog/2019/02/17/NMS.html",
            "relUrl": "/2019/02/17/NMS.html",
            "date": " • Feb 17, 2019"
        }
        
    
  
    
        ,"post15": {
            "title": "The Hypothesis Space and Representer Theorem",
            "content": "Introduction . When confronted with a machine learning task, probably some of the first questions a data scientist will ponder is: 1) What is our instance space? What is our label space? How do we define success? Are there computational issues associated with the task/data? How can we achieve generalization without over-fitting? What kind of features are we going to use? After answering these questions, a data scientist or team will settle on a learning algorithm and an appropriate loss function/evaluation metric. Unfortunately, in my experience, what is lacking from the initial discussion regarding data science tasks is a a seemingly trivial discuss of the hypothesis space. Since it is more of a theoretical framework as opposed to a tangible manifestation of a learning algorithm or code base, it is not discussed or included in a project workplan. However, ultimately a thorough discussion of the hypothesis space and its constraints is a vital component in initially framing a modeling task, whether machine learning or deep learning, and driving a project towards successful and reproducible results. When learning a model $g(x)$, we must choose which find of function we expect $g(x)$ to be. For example, consider an unput with four binary features $(x = left [ x_1x_2x_3x_4 right ]; x in left { 0,1 right })$ and an unknown function $f(x)$ that returns y. For four features there are 16 possible instances. Thus in the binary classification task there are $2^{16}$ possible functions to describe our data. Therefore, we are confronted with two issues. First, without without restrictions on the set of functinos $g(x)$ learning is not feasible. Second, even if it were feasible, assuming there is a deterministic target hypothesis $g_0$ relating x to y, chosing a $g$ from a large enough space of functions, we certainly would achieve very good performance on training data. If fact, if we wanted to de could always make the training error exactly zero. A simple hypothesis would do just that: $$ g(x) = left { begin{matrix} +1 &amp; i in left { 1, 2, ...m right } x_i = x, y_i =1 -1 &amp; otherwise end{matrix} right. $$ Obviously however this hypothesis would not generalize to data outside of our test set. Rather, we just created a model that simply memorizes labels. So all we ended up doing is minimizing the empirical error at the expense of the true error, which is called overfitting. What a good model does is minimize the true error $ Re (g) = E_D left [ L(g(x), y) right ]$ where the expectation is taken over (x,y) pairs drawn from some distribution D. . The Formal Model . The above considerations give rise to the formal model of our learning prolem. Let $X$ be the input space, $y$ be the output space, D an unknown probability distribution on $Xxy$ and let $g$ our hypothesis space be a class of functions $g: X to y $. For many practical algorithms, it is often observed that the true error is not too far from the empirical error. Real algorithms are not as ill behaved as the label memorization algorithm above, which was an extreme example. The key observation is the dicrepancy between the $R_D$ and $R_{emp}$ is related to the size of $g$, that is, how flexible our model is in terms of the size and shape of the hypothesis space. What made the label memorization algorithm so bad was that the class of possible hypotheses was so huge, permitting us to fit anything we wanted. That is an invitation for disastrous overfitting. Learning algorithms used in practice usually have access to a much more limited set of hyoptheses (such as linear discriminators in the case of the perceptron, SVM etc..), so they have less opportunity to overfit. One way to do this is by explicitly restricting the hypothesis space $g$ to “simple” hypotheses, as in Structural Risk Minimization.Another way is to introduce a penalty functional $ Omega $ that somehow measures the complexity of each hypothesis f, and to minimize the sum $$ R_reg(f) = R_{emp}(f) + Omega (f)$$. We can now begin to construct our hypothesis class g. Naturally we want G to be a linear function space in te sense that for any $f in G$ and any real number $ lambda$, $ lambda f$ is in G. Also, for any $f_1, f_2 in g$ the sum of $f_1 + f_2$ is in G. We also want G to be related to te regularizer $ Omega$. We define a norm on g and set $ Omega left [ f right ] = left | f right |^2$. We further require that the norm be derived from an inner product. This leads us to the notion of a Hilbert Space, or a linear inner product space, which will allow us to make the connection between the abstract structure of our hypothesis space g and what the elements of g actually are (discriminant functions or regressors). Within a Hilbert Space, for any such $ f in g$ any x has a corresponding $f_x in g$ such that x(f) = $ left langle f_x, x right rangle$. Therefore, for any $x in X$ we have a special function $k_x$ in our Hilbert Space called the representer and satisfying $f(x) = left langle k_x, f right rangle forall f in g$. Ultimately, we can rewrite our entire regularized risk minimization problem as: $$ hat{f} = arg underset{f in g}{min} left [ frac{1}{m} sum_{i=1}^{m}L( left langle k_x,f right rangle, y_i) + left langle f, f right rangle right ] $$ The hypothesis f only features in this equation in the form of inner producs with other functions in hypothesis space g. Once we know the form of the inner product and what the $k_x$ are, we can do everything we want with simple math. Moreover, anything outside of the span of $ left { k_x right }x in X$ is uninteresting since it does not affect what $f in g$ evaluated at any point of the input space is, so we can leave it out of the hypothesis space altogether. The result of the whole construction is just driven by the inner product $k(x, x&#39;) = left langle k_x, k_{x&#39;} right rangle$. This is called the kernel. In fact, we can reverse the whole procedure and construct g starting from the kernel. Given a positive definite function k on the input space X, we define g to be the minimal complete space of functions that includes all $ left { k_x right }x in X$ and that has an inner product define in a fashion above. This defines g uniquely, and formaly g is called the Reproducing Kernel Hilbert Space associated with kernel k. Finally. we have reduced the learning problem to that of defining the loss function Representer L and the kernel k. One more interesting point. By looking at our new formulation of our regularized risk minimization problem above, it is clear that $ hat{f}$ is going to be in the space of representers of the training data $k_1, k_2, k_3, ..., k_m$. We can tell because the loss term only depends on the inner products of f with $k_{x_1}, k_{x_2}, k_{x_3}, ..., k_{x_m}$ while the regularization term penalizes f in all directions. If f has any component orthogonal to the subspace spanned by $k_{x_1}, k_{x_2}, k_{x_3}, ..., k_{x_m}$, the loss term is not going to be sensitive to that component, but the regularization term will still penalize it. Hence ,the optimal f will be entirely contained in the subspace spanned by the representers. This is the meaning of the representer teorem and it means that the optimal hypothesis $ hat{f}$ can be expressed as $$ hat{f} = b + sum_{i=1}^{m} alpha_ik_{x_i} $$ for some real coefficients $ alpha_1, alpha_2, alpha_3... alpha_m$ and bais b. Or as it is better known, $$ hat{f} = b + sum_{i=1}^{m} alpha_ik(x_i, x) $$ .",
            "url": "https://jackhmiller.github.io/My-DS-Blog/2019/01/30/RKHS.html",
            "relUrl": "/2019/01/30/RKHS.html",
            "date": " • Jan 30, 2019"
        }
        
    
  
    
        ,"post16": {
            "title": "Accelerating Seq2Seq LSTM Training - GPU vs CPU",
            "content": "Introduction . Graphics processing units (GPUs), originally developed for accelerating graphics processing, can dramatically speed up computational processes for deep learning. They are an essential part of a modern artificial intelligence infrastructure, and new GPUs have been developed and optimized specifically for deep learning. . So when recently working on a task that involved using an autoencoder to create embeddings from text data, I thought that using a GPU would be a no brainer. The reality was more complicated. . No performance improvement . The encoder I used for the autoencoder was composed of a Bidirectional LSTM layer with a ReLu activation. But for simplicity’s sake, let’s look at a bare-bones LSTM model. . model = Sequential() model.add(LSTM(64, activation=ReLU(alpha=0.05), batch_input_shape=(1, timesteps, n_features), stateful=False, return_sequences = True)) model.add(Dropout(0.2)) model.add(LSTM(32)) model.add(Dropout(0.2)) model.add(Dense(n_features)) model.compile(loss=&#39;mean_squared_error&#39;, optimizer=Adam(learning_rate = 0.001), metrics=&#39;acc&#39;) model.fit(generator, epochs=epochs, verbose=0, shuffle=False) . When running the code I received this log warning, but I was not concerned since I figured the code would still achieve a considerable speed up by utilizing the GPU. . WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn&#39;t meet the criteria. It will use a generic GPU kernel as fallback when running on GPU. . My assumption was reinforced by what Nvidia has on their page describing LSTM acceleration: . Accelerating Long Short-Term Memory using GPUs . The parallel processing capabilities of GPUs can accelerate the LSTM training and inference processes. GPUs are the de-facto standard for LSTM usage and deliver a 6x speedup during training and 140x higher throughput during inference when compared to CPU implementations. cuDNN is a GPU-accelerated deep neural network library that supports training of LSTM recurrent neural networks for sequence learning. TensorRT is a deep learning model optimizer and runtime that supports inference of LSTM recurrent neural networks on GPUs. Both cuDNN and TensorRT are part of the NVIDIA Deep Learning SDK. . Source: https://developer.nvidia.com/discover/lstm . Yet I did not notice any speed up during training. When I compared the training times on GPU vs CPU while varying the batch size, I got: . So CPU on small batch sizes perform better than GPUs, which only achieve superior performance as batch size increases dramatically. What could be the reason for this? . Explanation . The explanation is rather simple; while the Bidirectional LSTM architecture is great for working with text data like IMDb reviews, TensorFlow uses an inefficient implementation of the LSTM on the GPU. The reason is probably that recurrent calculations are not parallel calculations, and GPUs are great for parallel processing. . Further, CUDNN has functionality to specifically accelerate LSTM and GRU layers. These GRU/LSTM layers can only be accelerated if they meet a certain criteria. This is what Francois Chollet, creator of keras library, main contributor of tensorflow framework, said about RNN runtime performance in his book Deep Learning with Python 2nd edition: . When using a Keras LSTM or GRU layer on GPU with default keyword arguments, your layer will be leveraging a cuDNN kernel, a highly optimized, low-level, NVIDIA-provided implementation of the underlying algorithm. As usual, cuDNN kernels are a mixed blessing: they’re fast, but inflexible—if you try to do anything not supported by the default kernel, you will suffer a dramatic slow- down, which more or less forces you to stick to what NVIDIA happens to provide. For instance, recurrent dropout isn’t supported by the LSTM and GRU cuDNN kernels, so adding it to your layers forces the runtime to fall back to the regular TensorFlow implementation, which is generally two to five times slower on GPU (even though its computational cost is the same). . To reinforce the point, in his blog post on benchmarking TensorFlow on cloud CPUs, Max Woolf analyzes the performance of Bidirectional LSTMs versus CPUs in terms of training time. His results speak for themselves as to how using a GPU for this architecture won’t give you the performance acceleration you are expecting: . Source: https://minimaxir.com/2017/07/cpu-or-gpu/ .",
            "url": "https://jackhmiller.github.io/My-DS-Blog/2018/11/29/GPU-training.html",
            "relUrl": "/2018/11/29/GPU-training.html",
            "date": " • Nov 29, 2018"
        }
        
    
  
    
        ,"post17": {
            "title": "Cost-Sensitive Feature Selection",
            "content": "Motivation . Feature selection is one of the trending challenges in multi-label classification, as the proliferation of high-dimensional data has become a trend in the last few years. Datasets with a dimensionality over the tens of thousands are constantly appearing in applications such as medical image and text retrieval or genetic data. The high-dimensionality of data has an important impact in learning algorithms, since they degrade their performance when a number of irrelevant and redundant features are present. In fact, this phenomenon is known as the curse of dimensionality, because unnecessary features increase the size of the search space and make generalization more difficult. For overcoming this major obstacle in machine learning, researchers usually employ dimensionality reduction techniques. In this manner, the set of features required for describing the problem is reduced, most of the times along with an improvement in the performance of the models. . Feature selection is arguably the most famous dimensionality reduction technique. It consists of detecting the relevant features and discarding the irrelevant ones. Its goal is to obtain a subset of features that describe properly the given problem with a minimum degradation in performance, with the implicit benefits of improving data and model understanding and the reduction in the need for data storage. With this technique, the original features are maintained, contrary to what usually happens in other techniques such as feature extraction, where the generated dataset is represented by a newly generated set of features, different than the original. . Feature selection methods can be divided into wrappers, filters and embedded methods. While wrapper models involve optimizing a predictor as a part of the selection process, filter models rely on the general characteristics of the training data to select features with independence of any predictor. The embedded methods generally use machine learning models for classification, and then an optimal subset or ranking of features is built by the classifier algorithm. Wrappers and embedded methods tend to obtain better performances but at the expense of being very time consuming and having the risk of overfitting when the sample size is small. On the other hand, filters are faster and, therefore, more suitable for large datasets. They are also easier to implement and scale up better than wrapper and embedded methods. As a matter of fact, filters can be used as a pre-processing step before applying other more complex feature selection methods. . However the existing approaches assume that all the features have the same cost. This assumption may be inappropriate when the acquisition of the feature values is costly. For example in medical diagnosis each diagnostic value extracted by a clinical test is associated with its own cost. In such cases it may be better to choose a model with an acceptable classification performance but a much lower cost. . As a result, our goal of here is to obtain a trade-off between a filter metric and the cost associated to the selected features, in order to select relevant features with a low associated cost. We will try and introduce four different approaches that can be considered filter methods. In this manner, any filter metric can be modified to have into account the cost associated to the input features. . . Cost-sensitive Methods . Approach #1- Feature-cost-sensitive Random Forest (FCS-RF) . Random forest (RF) is an ensemble of decision trees. RF has a wide range of applications because of its good stability and generalization. The typical construction process of RF consists of the following procedures. First, bagging is adopted on the training dataset to produce many subsets (with differences). Then each subset is used to construct a decision tree. In the tree growth, the splitting on each node depends on the feature selected from a group of candidates that are randomly chosen from all features. Without pruning (i.e., all leaf nodes are pure), all trees grow fully and each of them functions as a base classifier. Finally, all these tree classifiers are integrated. There are two important random characteristics in growing a random forest. One is randomly sampling, and the other is randomly generating the node splitting candidates. The diversity between the trees caused by randomness is crucial to the performance of the forest. . We can extend the random forest to incorporate feature cost by incorporating a probability vector into the tree construction process. The probability vector is generated based on the cost vector and we require the probability of a feature being selected inversely proportional to its cost. It can be easily shown that the expected cost of the feature selected in probability is smaller than the expected cost of the feature selected randomly. But some randomness among the trees still remains, which is indispensable to preserve the accuracy. It should be noted that if all the cost of features are equal, the FCS-RF will degenerate to the ordinary RF. . By using the the feature cost into the construction of random forest, the high-cost features less likely to be selected and the low-cost features with larger chances of being selected. In this way, the importance score of a feature is explicitly influenced by feature cost and its distinguishing ability. As a result, the top ranked features in the rank produced through FCS-RF have larger chance to be both low-cost and informative. . . Approach #2- Non-convex Constrained Optimization . Given a loss function (negative log-likelihood function) $ mathcal{L}_k( theta_k)$, the most natural way to take cost into account would be to minimize $ mathcal{L}_k( theta_k)$ with the addition of constraint $$ sum_{j=1}^{p}c_j( left| theta_{k,j} right|&gt;0) leq t $$ . This is equivalent to finding the best model (in terms of maximization of log-likelihood function) subject to limited budget. Parameter t can be interpreted as an upper bound for the budget of the kth model. When c1 = ··· = cp, the above problem reduces to best-subset selection. In best-subset selection we maximize log-likelihood function subject to limited number of features. Note that in best-subset selection parameter t stands for the maximal possible number of features that can be used in the model, whereas in in the additional constraint $t$ denotes the maximal possible cost that can be incurred to build the model. In other words, the constraint ensures that we control the budget instead of controlling the number of features. . Minimization of the log-likelihood function subject to (4) is equivalent to finding the optimal values of parameters $ theta_k$ by solving: $$ hat{ theta}_k= underset{ theta_k epsilon R}{argmin} left { mathcal{L}_k( theta_k) + lambda sum_{j=1}^{p}c_j( left| theta_{k,j} right|&gt;0) right } $$ . More precisely, for a given t &gt; 0 there exists $ lambda$ &gt; 0 such that the two problems share the same solution, and vice versa. The second term is a penalty for the cost. Parameter $ lambda$ &gt; 0 controls the balance between goodness of fit of the model and the cost. The larger is the value of $ lambda$ the cheaper is the model. Although the above approach seems to be encouraging, the issue is that the above equation is nonconvex, which makes it difficult to understand theoretically, and especially, to solve computationally. The problem is due to employment of $ mathscr{l}_0$-type penalty and renders the approach computationally infeasible for large number of features. The popular solution is to use $ mathscr{l}_1$ (lasso) penalty instead of $ mathscr{l}_0$, i.e., to optimize: . $$ hat{ theta}_k= underset{ theta_k epsilon R}{argmin} left { mathcal{L}_k( theta_k) + lambda sum_{j=1}^{p}c_j left| theta_{k,j} right| right } $$ It is known that $ mathscr{l}_1$ regularization (lasso) encourages sparsity in parameter vector, and thus selects relevant features. We can take this approach a step further, whereby instead of using $ mathscr{l}_1$ penalty, we can consider the elastic-net penalty which is a linear combination of $ mathscr{l}_1$ and $ mathscr{l}_2$ norms. Including $ mathscr{l}_2$ (ridge) regularization stabilizes the solution. The standard elastic-net does not take into account feature costs. The key idea in our approach is to modify the elastic-net penalty in such a way that features with higher costs are assigned larger weights: $$ hat{ theta}_k= underset{ theta_k epsilon R}{argmin} left { mathcal{L}_k( theta_k) + lambda sum_{j=1}^{p}c_jP_w( theta_{k,j}) + lambda sum_{j=p+1}^{p+k-1}P_w( theta_{k,j}) right } $$ . where $P_w( theta_{k,j}) = (1-w) theta_{k,j}^2 + w left| theta_{k,j} right|$ is the elastic net penalty, and $ lambda$ is a regularization parameter. The second term depends on costs; the larger is the cost the larger is the penalty. In other words, it is more difficult to choose expensive features to the final model. The costs are normalized in such a way that they sum up to p. Such normalization is necessary, otherwise the scale can be absorbed into the value of $ lambda$. . The total cost depends on $ lambda$, large enough value of $ lambda$ sets all the coefficients exactly equal to zero which gives a total cost zero. Small value of $ lambda$ results in many non-zero coefficients which yields large total cost. A practical challenge is how to select $ lambda$ in order to get a certain number of features with non-zero coefficients or to get a certain total cost. Unfortunately, it is not possible to directly control the number of selected features, and as such the total cost. It is necessary to test different regularization parameters and observe the total cost associated with selected features, similar to tuning a hyperparameter. It is relatively easy to obtain the solutions for many different values of $ lambda$, which in turn allows to select the value of $ lambda$ best corresponding to the desired total cost. . . Approach #3- Cost-based Correlation-based Feature Selection (CCFS) . CFS (Correlation-based Feature Selection) is a multivariate subset filter algorithm. It uses a search algorithm combined with an evaluation function to estimate the merit of feature subsets. The evaluation function takes into account the usefulness of individual features for predicting the class label as well as the level of correlation among them. It is assumed that good feature subsets contain features highly correlated with the class and uncorrelated with each other. The evaluation function can be seen in the following equation: $$ M_s = frac{k overline{r_{ci}}}{ sqrt{k+k(k-1) overline{r_{ii}}}} $$ . where $M_s$ is the merit of a feature subset S that contains k features, overline{r{ci}} is the average correlation between the features of S and the class, and overline{r{ii}} is the average intercorrelation between the features of S.In fact, this function is Pearson&#39;s correlation with all variables standardized. The numerator estimates how predictive of the class S is and the denominator quantifies the redundancy among the features in S. We can modify CFS by adding a term to the evaluation function to take into account the cost of the features, as can be seen in the following equation: $$ MC_s = frac{k overline{r_{ci}}}{ sqrt{k+k(k-1) overline{r_{ii}}}} - lambda frac{ sum_{i}^{k}C_i}{k} $$ . where $MC_s$ is the merit of the subset S affected by the cost of the features, $C_i$ is the cost of the feature i, and $ lambda$ is a parameter introduced to weight the influence of the cost in the evaluation function. If $ lambda$ is 0, the cost is ignored and the method works as the regular CFS. If $ lambda$ is between 0 and 1, the influence of the cost is smaller than the other term. If $ lambda = 1$ both terms have the same influence and if $ lambda &gt; 1$, the influence of the cost is greater than the influence of the other term. . . Approach #4- Cost-based Minimal-Redundancy-Maximal-Relevance (mRMR) . mRMR (Minimal-Redundancy-Maximal-Relevance) is a multivariate ranker filter algorithm. As mRMR is a ranker, the search algorithm is simpler than CFS&#39;s.The evaluation function combines two constraints (as the name of the method indicates), maximal relevance and minimal redundancy. The former is denoted by the letter D, it corresponds to the mean value of all mutual information values between each feature $x_i$ and class c, and has the expression shown in the following equation: $$ D(S,c) = frac{1}{ left| S right|} sum_{}^{}I(x_i;c) $$ . where S is a set of features and $I(x_i;c)$ is the mutual information between feature x and class c. The constraint of minimal redundancy is denoted by R and is shown in the following equation: $$ R(S) = frac{1}{ left| S right|^2} sum_{}^{}I(x_i,x_j) $$ . The evaluation function to be maximized combines the two constraints, $D(S,c) - R(S)$. We can make a modification of mRMR by adding a term to the condition to be maximized so as to take into account the cost of the feature to be selected, as can be seen in the following equation: $$ underset{x_j epsilon X-S_{m-1}}{max} left[ left( I(x_j;c)- frac{1}{m-1} sum_{x_i}I(x_j;x_i) right) - lambda C_j right] $$ . where $C_j$ is the cost of the feature j, and $ lambda$ is a parameter introduced to weight the influence of the cost in the evaluation function, as explained in the previous subsection. . . In Practice . When using the above mentioned approaches in real machine learning projects, cost sensitive feature selection methods typically achieves higher accuracy when the budget is low. For higher budgets, the traditional methods tend to perform better. This is because for a larger budget, traditional methods can include all relevant features, which results in a large predictive power of the model. For a limited budget, cost sensitive methods select features that serve as cheaper substitutes of the relevant expensive features. The graph below shows a typical tradoff between cost sensitive and non-cost sensitive feature selection approachs. . &nbsp; . Source: Teisseyre, P., Klonecki, T. (2021). Controlling Costs in Feature Selection: Information Theoretic Approach. In: Paszynski, M., Kranzlmüller, D., Krzhizhanovskaya, V.V., Dongarra, J.J., Sloot, P.M.A. (eds) Computational Science – ICCS 2021. ICCS 2021. . We can see that until 60% of the total cost, cost sensitive method performs better. This is due to the fact that,in this case, traditional methods can only use a fraction of all original features (say 1 or 2 out of 4 original features) within the assumed budget, which deteriorates the predictive performance of the corresponding classification model. On the other hand, the cost sensitive method aims to replace the original features by their cheaper counterparts, which allows to achieve higher accuracy of the corresponding model. When the budget exceeds 60% of the total cost, the traditional feature selection method tends to perform better than the proposed method, which is associated with the fact that, in this case, traditional methods include all original features (i.e., those which constitute the minimal set allowing for accurate prediction of the target variable) which results in a large predictive power of the corresponding model. For a larger budget, cost sensitive methods include both noisy features as well as the original ones. The noisy features become redundant when considering together with the original ones. This results in slightly lower prediction accuracy of the corresponding model. As expected, the cost sensitive methods are worth considering when the assumed budget is limited. .",
            "url": "https://jackhmiller.github.io/My-DS-Blog/2018/09/07/CFR.html",
            "relUrl": "/2018/09/07/CFR.html",
            "date": " • Sep 7, 2018"
        }
        
    
  
    
        ,"post18": {
            "title": "Multi-Objective Optimization",
            "content": "Introduction . Machine learning is inherently a multi-objective task. In most real-world problems, a learner is required to find a model with strong performance across multiple objectives $L_1, …, L_p$. Committing to a single objective $L_k$ fails to capture the full complexity of the underling problem and causes models to overfit to that individual objective. . Why not use more than one loss function? . A practitioner might want to use a variety of common training loss functions, each having its own advantages and drawbacks. For example, the Zero-One loss function often results in robust models when the data contains outliers, but it underperforms when the data is concentrated near decision surface. The opposite usually applies to the hinge loss function. To solve a complex machine learning problem, the learner would wish to combine multiple loss functions, making use of their strengths and mitigating weaknesses. . Unfortunately, in multi-objective optimization, there generally exists no singular solution. Thus when considering an objective function $f$, there exists on single optimal model but rather a set of optimal models.1 A model $x epsilon chi$ dominates another model $x’ epsilon chi$ if $f_i(x) leq f_i(x’)$ for all $i$ and there exists some $i$ such that $f_i(x) leq f_i(x’)$. . The set of all non-dominated solutions (i.e. models) is denoted as the Pareto front $ mathcal{P}_f( chi)$ whose members are commonly referred to as the Pareto-optimal solutions: . Pf(χ)={x∈χ∣∃x′∈χ:x′≺fx} mathcal{P}_f( chi) = left { x in chi | hspace{.2cm} exists hspace{.2cm} x&amp;#x27; in chi: x&amp;#x27; prec _f x right }Pf​(χ)={x∈χ∣∃x′∈χ:x′≺f​x} . . The inherent difficulty in multi-objective optimization . Despite the multi-objective nature of machine learning, working with a vector of multiple objectives at the same time turns out to be computationally challenging. For that reason, researchers often combine the set of base objectives $L_1, …, L_p$ into a weighted ensemble of the form $L_k = sum_{k=1}^p lambda_l L_k$, with each objective $L_k$ weighted by $ lambda_k$. This allows one to make use of efficient scalar function optimization algorithms on large-scale problems, typically using various stochastic gradient descent techniques. However, working with an ensemble of base objectives raises a natural question: how should we set the mixture weights $ lambda_1,…, lambda_p$? . Despite the simplicity of the questions above, there is no clear answer to how to determine the mixture weights for multi-objective problems. Particularly, this is because there is no straightforward way to map the requirements of a particular problem that a learner is seeking to solve to a corresponding distribution of the mixture weight. Thus, the mixture weights are usually assigned to uniform by default. However, in many cases the uniform ensemble of objectives can do more harm than good. This is simply because fitting models with the uniform mixture weights can significantly hurt some of the individual objectives in the mixture. For many machine learning applications (e.g., vision or speech), a significant decrease in performance on one objective is intolerable even if the performance on the uniform average of objectives is improved. . One can argue that if the uniform combination of objectives is not the natural target, then we should just set mixture weights for every problem separately based on the relative importance of each of the base losses. However, this is still not satisfactory, because the learner’s preferences are often shifting over time, they may be unobservable in some cases or even unknown. It is also often the case in the machine learning industry that when several parties develop a model for a particular multi-objective problem, their preferences for the base objectives are conflicting with each other. . A solution to weight assignment . We assume that the true mixture mixture of coefficients $ lambda$, or our weights for the different loss functions, are unknown. Our goal is to maximize the weight of a respective loss function $h$ within the overall weighting schema for all of the loss functions. Thus we can define our multi-objective loss function where each loss function $h$ is parameterized by a $ lambda$. Thus, we want to minimize each loss function, while maximizing each’s $ lambda$ parameter with respect to the other $ lambda s$. . With our new loss function, we can formulize our new optimization problem as minh∈HLΛ(h) underset{h in H}{min}L_{ Lambda}(h)h∈Hmin​LΛ​(h) Given that the hypothesis is parameterized by a vector $w in W$, our final optimization problem can be formulated as follows: minw∈W maxλ∈ΛL(w,λ)=minw∈W maxλ∈Λ∑k=1pλk1m∑i=1mhk(xi,yi) underset{w in W}{min} underset{ lambda in Lambda }{max} L(w, lambda) = underset{w in W}{min} underset{ lambda in Lambda }{max} sum_{k=1}^p lambda_k frac{1}{m} sum_{i=1}^{m}h_k(x_i, y_i)w∈Wmin​ λ∈Λmax​L(w,λ)=w∈Wmin​ λ∈Λmax​∑k=1p​λk​m1​∑i=1m​hk​(xi​,yi​) where 1m∑i=1mhk(xi,yi)=Lk(hw) frac{1}{m} sum_{i=1}^{m}h_k(x_i, y_i) = L_k(h_w)m1​∑i=1m​hk​(xi​,yi​)=Lk​(hw​) So to simplify: minw∈W maxλ∈Λ∑k=1pλkLk(hw) underset{w in W}{min} underset{ lambda in Lambda }{max} sum_{k=1}^p lambda_k L_k(h_w)w∈Wmin​ λ∈Λmax​∑k=1p​λk​Lk​(hw​) Fortunately, the optimization problem above is convex, and can be solved using gradient-based algorithms. We can also introduce regularization parameters and norms in order to control for the complexity of the hypothesis space. . Implementation . The optimization of our problem above can be solved using projected gradient descent, where optimization is performed over $(w, lambda)$. To simplify our notation: L(w)=1m∑i=1mhk(xi,yi)L(w) = frac{1}{m} sum_{i=1}^{m}h_k(x_i, y_i)L(w)=m1​∑i=1m​hk​(xi​,yi​) L(w,λ)=∑k=1pλkLk(w)L(w, lambda) = sum_{k=1}^p lambda_k L_k(w)L(w,λ)=∑k=1p​λk​Lk​(w) Or simply written: minw∈W maxλ∈Λ L(w,λ) underset{w in W}{min} underset{ lambda in Lambda }{max} L(w, lambda)w∈Wmin​ λ∈Λmax​ L(w,λ) The inner maximization choses a value for $ lambda in Lambda$ to maximize the objective function, while the second minimization seeks to minimize loss by choosing $w in W$. Each gradient descent iteration will operate on the loss function with respect to $w$ as well as $ lambda$. So for each iteration $t in [1, T]$, the updates for $w$ and $ lambda$ are as follows: . wt←∏W[wt−1−γwδwL(wt−1,λt−1)]w_t leftarrow prod W [w_{t-1} - gamma _w delta _wL(w_{t-1}, lambda_{t-1})]wt​←∏W[wt−1​−γw​δw​L(wt−1​,λt−1​)] λt←∏Λ[λt−1−γλδλL(wt−1,λt−1)] lambda_t leftarrow prod Lambda [ lambda_{t-1} - gamma _ lambda delta _ lambda L(w_{t-1}, lambda_{t-1})]λt​←∏Λ[λt−1​−γλ​δλ​L(wt−1​,λt−1​)] . We can use these update rules to combined any number of loss functions into a signal objective function, that can in turn be minimized using gradient descent. . Experiments . Such a framework was implemented in 2 suggest that this framework for multi-objective optimization improves the model for the worst performing loss. In certain cases it can lead to an improvement in performance, but it is better suited to increase model robustness and guard against overfitting. . Michael TM Emmerich and André H Deutz. A Tutorial on Multiobjective Optimization: Fundamentals and Evolutionary Methods. Natural Computing, 17(3):585–609, 2018. &#8617; . | Cortes, C., Mohri, M., Gonzalvo, J., &amp; Storcheus, D. (2020). Agnostic Learning with Multiple Objectives. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, &amp; H. Lin (Eds.), Advances in Neural Information Processing Systems (Vol. 33, pp. 20485–20495). Retrieved from https://proceedings.neurips.cc/paper/2020/file/ebea2325dc670423afe9a1f4d9d1aef5-Paper.pdf &#8617; . |",
            "url": "https://jackhmiller.github.io/My-DS-Blog/2018/08/11/multi-objective-optimization.html",
            "relUrl": "/2018/08/11/multi-objective-optimization.html",
            "date": " • Aug 11, 2018"
        }
        
    
  
    
        ,"post19": {
            "title": "Bayesian Optimization",
            "content": "What are hyperparameters? . All machine learning models have a set of hyperparameters or arguments that must be specified by the practitioner. These are values that must be specified outside of the training procedure. Vanilla linear regression doesn’t have any hyperparameters. But variants of linear regression do. Ridge and Lasso Regression both add a regularization term to linear regression; the weight for the regularization term is called the regularization parameter. Decision trees have hyperparameters such as the desired depth and number of leaves in the tree. Support Vector Machines require setting a misclassification penalty term. Kernelized SVM require setting kernel parameters like the width for RBF kernels. . These types of hyperparameter control the capacity of the model, i.e., how flexible the model is, how many degrees of freedom it has in fitting the data. Proper control of model capacity can prevent overfitting, which happens when the model is too flexible, and the training process adapts too much to the training data, thereby losing predictive accuracy on new test data. So a proper setting of the hyperparameters is important. . Another type of hyperparameters comes from the training process itself. For instance, stochastic gradient descent optimization requires a learning rate or a learning schedule. Some optimization methods require a convergence threshold. Random forests and boosted decision trees require knowing the number of total trees. (Though this could also be classified as a type of regularization hyperparameter.) These also need to be set to reasonable values in order for the training process to find a good model. . Back-box optimization . Conceptually, hyperparameter tuning is an optimization task, just like model training. However, these two tasks are quite different in practice. When training a model, the quality of a proposed set of model parameters can be written as a loss function. Hyperparameter tuning is a meta-optimization task. Since the training process doesn’t set the hyperparameters, there needs to be a meta process that tunes the hyperparameters. It is a process commonly referred to as black-box optimization, meaning that we want to minimize a function $f( theta)$ but we only get to query values rather than directly computing gradients. This is why hyperparameter tuning is harder- the quality of those hyperparameters cannot be written down in a closed-form formula, because it depends on the outcome of a blackbox (the model training process). This is why hyperparameter tuning is much harder. . Naive approaches for hyperparameter optimization . Up until a few years ago, the only available methods were grid search and random search. Grid search evaluates each possible combination, and returns the best configuration based on a loss function. Unlike grid search, a key distinction with Random search is that we do not specify a set of possible values for every hyperparameter. Instead, we sample values from a statistical distribution for each hyperparameter. A sampling distribution is defined for every hyperparameter to do a random search. In fact, in a 2012 paper Bergstra and Bengio proved that in many instances random search performs as well as grid search.1 . The implementation of these methods is simple; for each proposed hyperparameter setting, the inner model training process comes up with a model for the dataset and outputs evaluation results on hold-out or cross validation datasets. After evaluating a number of hyperparameter settings, the hyperparameter tuner outputs the setting that yields the best performing model. Formally this can be represented as . x∗=argminx∈χ  f(x)x^* = underset{x in chi }{argmin} ; f(x)x∗=x∈χargmin​f(x) . Here we are minimizing the score of our evaluation functon evaluation over the validation set. $x^*$ is the set of hyperparameters that yields the lowest value of the score. . The last step is to train a new model on the entire dataset (training and validation) under the best hyperparameter setting. . Hyperparameter_tuning (training_data, validation_data, hp_list):   hp_perf = []   foreach hp_setting in hp_list:     m = train_model(training_data, hp_setting)     validation_results = eval_model(m, validation_data)     hp_perf.append(validation_results)   best_hp_setting = hp_list[max_index(hp_perf)]   best_m = train_model(training_data.append(validation_data), best_hp_setting)   return (best_hp_setting, best_m) . Grid and random search are naive yet comprehensive. However, their computational time increases exponentially with a growing parameter space. They also don’t utilize search results from previous iterations. . Introducing Bayesian hyperparameter optimization . Hyperparameter optimization has proven to be one of the most successful applications of Bayesian optimization (BO). While BO is an area of research decades old, it has seen a resurgence that coincides with the resurgence in neural networks granted new life by modern computation: the extreme cost of training these models demands efficient routines for hyperparameter tuning. . BO will allow us to answer the question; Given a set of observations of hyperparameter performance, how do we select where to observe the function next? . Big picture . At a high-level, BO methods are efficient because they choose the next hyperparameters in an informed manner. The basic idea is: spend a little more time selecting the next hyperparameters in order to make fewer calls to the objective function. In practice, the time spent selecting the next hyperparameters is inconsequential compared to the time spent in the objective function. By evaluating hyperparameters that appear more promising from past results, Bayesian methods can find better model settings than random search in fewer iterations. . Theory . BO consists of two main components: a surrogate model for modeling the objective function, and an acquisition function for deciding where to sample next. We use a surrogate for the objective function so that we can both make predictions and maintain a level of uncertainty over those predictions via a posterior probability distribution. . Proposing sampling points in the search space is done by acquisition functions. They trade off exploitation and exploration. Exploitation means sampling where the surrogate model predicts a high objective and exploration means sampling at locations where the prediction uncertainty is high. Both correspond to high acquisition function values and the goal is to maximize the acquisition function to determine the next sampling point. . [!faq] What is Bayesian about this? You may be wondering what’s “Bayesian” about Bayesian Optimization if we’re just optimizing an acquisition functions. Well, at every step we maintain a model describing our estimates and uncertainty at each point, which we update according to Bayes’ rule at each step, conditioning our model on a limited set of previous function evaluations . Our ultimate goal is to collapse the uncertainty surrounding the posterior mean of our model parameters in order to identify the best set of parameters. This is depicted in the figure2 below, where we reduce the blue-shaded area that represents our uncertainty regarding our parameterized model performance by repeatedly sampling and subsequently evaluating our surrogate models (dashed colored lines represented as Gaussian processes), and finally adjusting our surrogate models. (The mean of the Gaussian process represented as the solid blue line gives the approximate response) . Each time we observe our function at a new point, this posterior distribution is updated. . Sequential model-based global optimization (SMBO) . In an application where the true function $f: chi rightarrow mathbb{R}$ is costly to evaluate, model-based algorithms approximate $f$ with a surrogate that is cheaper. The point $x^*$ (in our case a set of hyperparameters) that maximizes the surrogate model becomes the proposal for where the true function should be evaluated. . [!info] SMBO iterates between fitting models and using them to make choices about which configurations to investigate. It offers the appealing prospects of interpolating performance between observed parameter settings and of extrapolating to previously unseen regions of parameter space. It can also be used to quantify importance of each parameter and parameter interactions. . The psuedo-code for how SMBO algorithms model $f$ via observational history $ mathcal{H}$ is as follows: . begin{algorithm} caption{SMBO} begin{algorithmic} PROCEDURE{SMBO}{$f, M, T, S$} STATE $ mathcal{H} leftarrow phi $ FOR{$t leftarrow 1 ; to ; T$} STATE $ x^* leftarrow underset{x}{argmin} ; S(x, M_{t-1}) $ STATE $ text{EVALUATE} ; f(x^*) $ STATE $ mathcal{H} leftarrow mathcal{H} cup (x^*, f(x^*))$ STATE $ text{ fit a new model} ; M_t ; text{to} ; mathcal{H}$ ENDFOR RETURN $ mathcal{H}$ ENDPROCEDURE end{algorithmic} end{algorithm} . A probabilistic surrogate model . SMBO algorithms differ in what criterion they optimize to obtain $x^*$ given a surrogate of $f$. The novel work of Bergstra et al. proposed creating a probabilistic surrogate model of $f$ by modeling a hierarchical Gaussian process, and use expected improvement (EI) as the acquisition function. In their case, they implement EI via a tree-structed Parzen estimator (TPE).3 The processes can be written as follows: . begin{algorithm} caption{BO} begin{algorithmic} PROCEDURE{BO}{$f, n, T$} STATE $ text{Place a Gaussian process prior on objective function} ; f$ STATE $ text{Observe} ; f ; text{at} ; n ; text{points according to an initial space-filling experimental design}$ FOR{$t leftarrow 1 ; to ; T$} STATE $ text{Update the posterior probability distribution of} ; f ; text{using all available data}$ STATE $ text{Calculate the maximizer of the acquisition function to find the next sampling point}$ STATE $x_t^* = underset{x}{argmax} ; Acquisition(x| mathcal{D}_{1:t-1})$ STATE $ text{Pass the parameters to the objective function to obtain a sample}$ STATE $y_t= f(x_t^*)$ STATE $ text{Add sample to previous samples}$ STATE $ mathcal{D}_{1:t} = mathcal{D}_{1:t-1}, (x_{t} ,y_t)$ ENDFOR RETURN $ mathcal{H} vdash ; underset{ mathcal{H}}{argmin} ; y$ ENDPROCEDURE end{algorithmic} end{algorithm} . The EI acquisition function that we will optimize to choose the next experiment can be represented as follows: . EIy(x)=∫−∞∞max(y∗−y,0)p(y∣x)dyEI_y(x) = int_{- infty}^{ infty } max(y^{*}-y, 0)p(y|x)dyEIy​(x)=∫−∞∞​max(y∗−y,0)p(y∣x)dy . Here $x$ is our set of hyperparameters, $y^{}$ is our target performance/value of the best sample so far, and $y$ is our loss. We want the $p(y &lt; y^{})$, which we will define using a quantile search result to achieve the following: . ∫−∞y∗p(y)dy int_{- infty}^{y^*}p(y)dy∫−∞y∗​p(y)dy . Most other surrogate models like Random Forest Regressions and Gaussian-processes represent $ p(y | x) $ like in the EI equation above, where $y$ is the value on the response surface, i.e. the validation loss, and $x$ is the hyper-parameter. However, TPE calculates $p(x | y)$ which is the probability of the hyperparameters given the score on the objective function. This is done by replacing the distribution of the configuration prior with non-parametric densities. The TPE defines $p(x | y)$ using the following two densities: | . p(x∣y)={l(x)if  y&lt;y∗g(x)if  y≥y∗p(x|y) = left { begin{matrix} l(x) &amp; if ; y &lt; y^* g(x) &amp; if ; y geq y^* end{matrix} right.p(x∣y)={l(x)g(x)​ify&lt;y∗ify≥y∗​ . The explanation of this equation is that we make two different distributions for the hyperparameters: one where the value of the objective function is less than a threshold $y^{}$, $l(x)$, and one where the value of the objective function is greater than the threshold $y^{}$, $g(x)$. In other words, we split the observations in two groups: the best performing one (e.g. the upper quartile) and the rest, defining $y^*$ as the splitting value for the two groups (often represented as a quantile). . After constructing two probability distributions for the number of estimators, we model the likelihood probability for being in each of these groups (Gaussian processes to model the posterior probability). Ultimately we want to draw values of x from l(x) and not from g(x) because this distribution is based only on values of x that yielded lower scores than the threshold. Interestingly, Bergstra et al. show that the expected improvement is proportional to $ frac{l(x)}{g(x)}$, so we should seek to maximize this ratio. . Putting the above together, our surrogate modeling process looks like the following: . Draw sample hyperparameters form $l(x)$ | Evaluate the hyperparameters in terms of $ frac{l(x)}{g(x)}$ | Return the set of hyperparameters that yields the highest value under $ frac{l(x)}{g(x)}$ | Evaluate these hyperparameters via the objective function | If the surrogate function is correct, then these hyperparameters should yield a better value when evaluated. . A bit more on TPEs . The two densities l and g are modeled using Parzen estimators (also known as kernel density estimators) which are a simple average of kernels centered on existing data points. In other words, we approximate our PDF by a mixture of continuous distributions. This is useful since we assume that there is some unknown but nonzero density around the near neighborhood of $x_i$ points and we use kernels $k$ to account for it. The more points is in some neighborhood, the more density is accumulated around this region and so, the higher the overall density of our function. For example, below4 we have a density displayed via the blue line which could represent $l$ or $g$, and three observations with Gaussian kernels centered on each. . . To see how likely a new point is under our mixed distribution, we compute the mixture probability density at a given point $x_i$ as follows: . [PDF1(xi)+PDF2(xi)+PDF3(xi)]/(number of kernels)[PDF_1(x_{i)}+ PDF_2(x_{i})+ PDF_3(x_i)]/( text{number of kernels})[PDF1​(xi)​+PDF2​(xi​)+PDF3​(xi​)]/(number of kernels). . Implementation . Lets use this function for our objective function: . f(x)=−sin(3x)−x2+0.7x+ϵf(x) = -sin(3x) - x^2 + 0.7x + epsilonf(x)=−sin(3x)−x2+0.7x+ϵ . The following plot shows the noise-free objective function, the amount of noise by plotting a large number of samples and the two initial samples. . . We are trying to find the global maximum at the left peak, via the fewest number of steps. Now we will implement the acquisition function defined as the expected improvement function above. . def ei_acquisition(X, X_sample, Y_sample, gauss, xi=0.01): &quot;&quot;&quot; Expected improvement at points X using Gaussian surrogate model &quot;&quot;&quot; mu, sigma = gauss.predict(X, return_std=True) mu_sample = gauss.predict(X_sample) sigma = sigma.reshape(-1, 1) mu_sample_opt = np.max(mu_sample) with np.errstate(divide=&#39;warn&#39;): imp = mu - mu_sample_opt - xi Z = imp / sigma ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z) ei[sigma == 0.0] = 0.0 return ei def min_obj(X): # Minimization objective is the negative acquisition function return -ei_acquisition(X.reshape(-1, dim), X_sample, Y_sample, gauss) . Now we are ready to run our experiment: . Bayesian optimization runs for 10 iterations. In each iteration, a row with two plots is produced. The left plot shows the noise-free objective function, the surrogate function which is the GP posterior predictive mean, the 95% confidence interval of the mean and the noisy samples obtained from the objective function so far. The right plot shows the acquisition function. The vertical dashed line in both plots shows the proposed sampling point for the next iteration which corresponds to the maximum of the acquisition function. . Note how the two initial samples initially drive search into the direction of the local maximum on the right side but exploration allows the algorithm to escape from that local optimum and find the global optimum on the left side. Also note how sampling point proposals often fall within regions of high uncertainty (exploration) and are not only driven by the highest surrogate function values (exploitation). . A convergence plot reveals how many iterations are needed the find a maximum and if the sampling point proposals stay around that maximum i.e. converge to small proposal differences between consecutive steps. . [!danger] Bayesian optimization is efficient in tuning few hyper-parameters but its efficiency degrades a lot when the search dimension increases too much, up to a point where it is on par with random search. . Bergstra, James &amp; Bengio, Y.. (2012). Random Search for Hyper-Parameter Optimization. The Journal of Machine Learning Research. 13. 281-305. &#8617; . | Rasmussen &amp; Williams, Gaussian Processes for Machine Learning, MIT Press 2006. &#8617; . | James Bergstra, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl. 2011. Algorithms for hyper-parameter optimization. In Proceedings of the 24th International Conference on Neural Information Processing Systems (NIPS’11): 2546–2554. &#8617; . | https://www.youtube.com/watch?v=bcy6A57jAwI&amp;t=620s &#8617; . |",
            "url": "https://jackhmiller.github.io/My-DS-Blog/2018/07/21/bayesian-opt.html",
            "relUrl": "/2018/07/21/bayesian-opt.html",
            "date": " • Jul 21, 2018"
        }
        
    
  
    
        ,"post20": {
            "title": "Coordinate Descent is Fast!",
            "content": "Coordinate descent (CD) algorithms solve optimization problems by successively performing approximate minimization along coordinate directions or coordinate hyperplanes. They have been used in applications for many years, and their popularity continues to grow because of their usefulness in data analysis, machine learning, and other areas of current interest. . CD methods are the archetype of an almost universal approach to algorithmic optimization: solving an optimization problem by solving a sequence of simpler optimization problems. The obviousness of the CD approach and its acceptable performance in many situations probably account for its longstanding appeal among practitioners. Paradoxically, the apparent lack of sophistication may also account for its unpopularity as a subject for investigation by optimization researchers, who have usually been quick to suggest alternative approaches in any given situation. . Various applications (including several in computational statistics and machine learning) have yielded problems for which CD approaches are competitive in performance with more reputable alternatives. The properties of these problems (for example, the low cost of calculating one component of the gradient, and the need for solutions of only modest accuracy) lend themselves well to efficient implementations of CD, and CD methods can be adapted well to handle such special features of these applications as nonsmooth regularization terms and a small number of equality constraints. At the same time, there have been improvements in the algorithms themselves and in our understanding of them. Besides their extension to handle the features just mentioned, new variants that make use of randomization and acceleration have been introduced. Parallel implementations that lend themselves well to modern computer architectures have been implemented and analyzed. Perhaps most surprisingly, these developments are relevant even to the most fundamental problem in numerical computation: solving the linear equations Aw = b. . So let&#39;s explore Coordinate Descent with the simplest of cases: plain old linear regression using some of the ever so used mtcars dataset. Recall that the sum of squared residuals is: . $$RSS = sum left(y_i - sum x_{ij} beta_j right)^2$$ . Quickly deriving our objective function: . $$ f( beta ) = argmin left | y_i-x_{ij} beta_j right |^2_2 $$ . $$ =(y-x beta)^T(y-x beta) $$ . $$ hat{ beta} = (x^Tx)^{-1}(x^Ty) $$aka OLS. . Now let&#39;s do our update steps: . $$ 0 = bigtriangledown_i f(x) = beta_j^T( beta_j x_{ij} + beta_{j}x_{i-1,j}-y_i) $$ . $$ x_{ij} = frac{ beta_j^T(y_i- beta_jx_{i-1, j})}{ beta_j^T beta_j} $$ $$ x_{ij} = frac{ beta_j^T(residual_{i-1,j})}{ left | beta_j right |^2} $$ . Loading the data and off we go. . cd_data = loadmat(&#39;mtcars.mat&#39;)[&#39;data&#39;] cd_df = pd.DataFrame(cd_data) . y = cd_df[0].values x = cd_df.loc[:, 1:].values . inter = np.ones(x.shape[0]) X = np.column_stack((inter, x)) X_Normalized = X / np.sqrt(np.sum(np.square(X), axis=0)) . Define our loss function . def loss(b): return sum((y - X_Normalized@b)**2) . Coding the descent from scratch using numpy . b = np.zeros(X_Normalized.shape[1]) losses = [loss(b)] iterations = 1 for iteration in range(100): r = y - X_Normalized.dot(b) for j in range(len(b)): r = r + X_Normalized[:, j] * b[j] b[j] = X_Normalized[:, j].dot(r) r = r - X_Normalized[:, j] * b[j] losses.append(loss(b)) iterations += 1 . array([193.44428449, -44.7866386 , -27.77448248, -13.3819371 ]) . plt.plot(losses) plt.title(&#39;CD Loss&#39;) plt.xlabel(&#39;Iteration&#39;) . Text(0.5, 0, &#39;Iteration&#39;) . Our loss function converges almost instantly! . print(&quot;MSE:&quot;, sum((y - X_Normalized@b)**2) ) . MSE: 261.3695510665015 . Just as a sanity check, lets make sure our from scratch implementation produced the same results as sklearn&#39;s implementation of linear regression . from sklearn import linear_model reg = linear_model.LinearRegression() . reg.fit (X_Normalized, y) reg.coef_ . array([ 0. , -44.66189706, -27.81172298, -13.40793129]) .",
            "url": "https://jackhmiller.github.io/My-DS-Blog/2018/05/04/CD.html",
            "relUrl": "/2018/05/04/CD.html",
            "date": " • May 4, 2018"
        }
        
    
  
    
        ,"post21": {
            "title": "Functional Linear Regression",
            "content": "Covariance estimation is a problem of great interest in many different disciplines, including machine learning, signal processing, economics and bioinformatics. In many applications the number of variables is very large, e.g., in the tens or hundreds of thousands, leading to a number of covariance parameters that greatly exceeds the number of observations. To address this problem constraints are frequently imposed on the covariance to reduce the number of parameters in the model. For example, the Glasso model of Yuan and Lin and Banerjee et al 1 imposes sparsity constraints on the covariance. The Kronecker product model of Dutilleul and Werner et al 2 assumes that the covariance can be represented as the Kronecker product of two lower dimensional covariance matrices. Here we will implement a combination of these two aproaches. . Here is our problem setting: . A combustion engine produces gas with polluting substances such as nitrogen oxides (NOx).Gas emission control regulations have been set up to protect the environment. The NOx Storage Catalyst (NSC) is an emission control system by which the exhaust gas is treated after the combustion process in two phases: adsorption and regeneration. During the regeneration phase, the engine control unit is programmed to maintain the combustion process in a rich air-to-fuel status. The average relative air/fuel ratio is the indicator of a correct regeneration phase. Our goal is to predict this value, using the information from eleven sensors. To do so, we are going to use group lasso regression. . List of on-board sensorsair aspirated per cylinder . engine rotational speed | total quantity of fuel injected | low presure EGR valve | inner torque | accelerator pedal position | aperture ratio of inlet valve | downstreem intercooler preasure | fuel in the 2nd pre-injection | vehicle velocity | . First we will write the problem that we want to solve in mathematical notation. . $$ underset{ beta_g in mathbb{R}}{armin} left | sum_{g in G} left [ X_g beta_g right ]-y right |_2^2 + lambda_1 left | beta right |_1 + lambda_2 sum_{g in G} sqrt[]{d_g} left | beta_g right |_2 $$ Where $$ $$ $ X_g in mathbb{R}^{n x d_g}$ is the data matrix for each sensor&#39;s covariates which compose group $g$, $ beta_g $ is the B spline coefficients for group $g$, $ y in mathbb{R}^{n}$ is the air/fuel ratio target, $ n$ is the number of measurements, $d_g$ is the dimensionality of group $g$, $ lambda_1 $ is the parameter-wise regularization penalty, $ lambda_2$ is the group-wise regularization penalty, $ G $ is the set of all groups for all sensors . Now on to the code. We will use group lasso to learn the B-spline coefficients. We will use B-splines with 8 knots to reduce the dimensionality of the problem. Ultimately, we want to determine which sensors are correlated with the air/fuel ratio? Also, we want to predict the air/fuel ratio for the observations in the test dataset. . from scipy import interpolate import group_lasso import sklearn.linear_model as lm from sklearn.model_selection import GridSearchCV, RandomizedSearchCV . x_train = loadmat(&#39;NSC.mat&#39;)[&#39;x&#39;] y_train = loadmat(&#39;NSC.mat&#39;)[&#39;y&#39;] x_test = loadmat(&#39;NSC.test.mat&#39;)[&#39;x_test&#39;] y_test = loadmat(&#39;NSC.test.mat&#39;)[&#39;y_test&#39;] . for i in range(len(x_train[0])): plt.figure(figsize=(15,8)) pd.DataFrame(x_train[0][i]).plot(legend=False, title=f&quot;Sensor {i}&quot;) . def transformation(data): coefficients = [] x = np.linspace(0, 203, 203) knots = np.linspace(0, 203, 10) [1:-1] for i,d in enumerate(data): t, c, k = interpolate.splrep(x, d, task=-1, t=knots, k=2) coefficients.append(np.trim_zeros(c, trim=&#39;b&#39;)[:-1]) return np.array(coefficients) def standardize(data): results = [] for i in data: temp = scaler.fit_transform(i) results.append(temp) return results . scaler = StandardScaler() Y_train = transformation(scaler.fit_transform(y_train)).ravel() Y_test = transformation(scaler.fit_transform(y_test)).ravel() X_train = np.hstack(np.array([transformation(i) for i in standardize(x_train[0])])) X_test = np.hstack(np.array([transformation(i) for i in standardize(x_test[0])])) . identity = np.identity(10) . Kronecker Products . final_train = np.kron(X_train, identity) final_test = np.kron(X_test, identity) . g = [[i]*100 for i in range(1,11)] groups = np.array([item for sublist in g for item in sublist]) . gl = group_lasso.GroupLasso( groups=groups, group_reg=0, l1_reg=0, fit_intercept=True, scale_reg=&quot;none&quot;, supress_warning=True, tol=1e-5 ) . lambdas, _, _ = lm.lasso_path(final_train, Y_train) . CV = RandomizedSearchCV(estimator=gl, param_distributions={&#39;group_reg&#39;: lambdas[::5]}, scoring=&#39;neg_mean_squared_error&#39;, n_iter=100, verbose=2) CV.fit(final_train, Y_train) . coef = gl.coef_.ravel().reshape(100, 10) coef_base = X_train@coef coef_df = pd.DataFrame(coef_base) . print(&quot;Best lambda:&quot;, CV.best_params_[&#39;group_reg&#39;]) . print(&quot;Coefficients Correlated to Target&quot;) coef_df.corrwith(pd.DataFrame(Y_train.reshape(150,10))) . It appears sensors 2 and 7 have the greatest correlation to the air fuel ration . _y = pd.DataFrame(Y_train.reshape(150,10)) for sensor in [2, 7]: plt.figure(figsize=(15,8)) plt.scatter(coef_df[sensor], _y[sensor]) plt.title(f&quot;Correlation of sensor {sensor} and air/fuel ratio&quot;) plt.xlabel(f&quot;Sensor {sensor}&quot;) plt.ylabel(&quot;Air/fuel ratio&quot;) . coef_df[2].plot(title=&#39;Coefficients for sensor 2&#39;) . coef_df[7].plot(title=&#39;Coefficients for sensor 7&#39;) . predicted = CV.predict(final_test) . print(&quot;Mean Square Prediction Error:&quot;, sum((Y_test - predicted)**2)) . . Yuan et al. &quot;Model Selection and Estimation in Regression With Grouped Variables,&quot; Journal of the Royal Statistical Society Series B. (2006): 49-67. . Tsiligkaridis et al. &quot;Convergence Properties of Kronecker Graphical Lasso Algorithms,&quot; IEEE (2013). .",
            "url": "https://jackhmiller.github.io/My-DS-Blog/2018/04/02/FLR.html",
            "relUrl": "/2018/04/02/FLR.html",
            "date": " • Apr 2, 2018"
        }
        
    
  
    
        ,"post22": {
            "title": "Sparse Linear Regression",
            "content": "There are two fundamental goals in statistical learning: ensuring high prediction accuracy and discovering relevant predictive variables. Variable selection is particularly important when the true underlying model has a sparse representation. It may be important to clarify that the expression &#39;sparse&#39; should not be confused with techniques for sparse data, containing many zero entries. Here, sparsity refers to the estimated parameter vector, which is forced to contain many zeros. A sparse representation can be manifested as a result of two common occurances. First, the number of predictors might exceed the number of observations. Such high-dimensional data settings are nowadays commonplace in operational research. Second, some data points might behave differently from the majority of the data. Such atypical data points are called outliers in statistics, and anomalies in machine learning. Traditional methods for linear regression analysis such as the ordinary Least Squares estimator (OLS) fail when these problems arise: the OLS cannot be computed or becomes unreliable due to the presence of outliers. . A regression vector is sparse if only some of its components are nonzero while the rest is set equal to zero, hereby inducing variable selection. . . Here we want to compare some different regression techniques that induce feature or input sparsity: Lasso Regression, Ridge Regression, Adaptive Lasso Regression, and Elastic Net Regression. We will calculate the optimal tuning parameters, and fit the model to aquire the coefficients obtained with the optimal parameters as well as the Mean Square Prediction Error for the test dataset. . In this demonstration our goal is to predict the concentration of carbon oxide (CO) in mg/m^3. For this purpose, we have the following information provided by air quality sensors: . Benzene (C6H6) concentration in μg/m3 | Non Metanic HydroCarbons (NMHC) concentration in μg/m3 | Nitrogen Oxides (NOx)concentration in ppb | Nitrogen Dioxide (NO2) concentration in μg/m3 | Ozone (O3) concentration in μg/m3 | Temperature (T) in Celsius degrees | Relative Humidity (RH) | Absolute Humidity (AH) | . from sklearn.model_selection import RandomizedSearchCV, GridSearchCV from scipy.stats import uniform from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression, lasso_path, LassoCV from sklearn.metrics import mean_squared_error from sklearn.preprocessing import StandardScaler from numpy import arange . scaler = StandardScaler() . train_data = pd.read_csv(&#39;train.air.csv&#39;) test_data = pd.read_csv(&#39;test.air.csv&#39;) . standardized_train = scaler.fit_transform(train_data) standardized_test = scaler.fit_transform(test_data) . train = pd.DataFrame(standardized_train, columns=train.columns) test = pd.DataFrame(standardized_test, columns=test.columns) . y_train = train[&#39;CO&#39;] x_train = train.drop(&#39;CO&#39;, axis=1) y_test = test[&#39;CO&#39;] x_test = test.drop(&#39;CO&#39;, axis=1) . Ridge . param_grid = {&#39;alpha&#39;: uniform()} model = Ridge() ridge_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=100) ridge_search.fit(x_train, y_train) print(&quot;Optimal lasso penality parameter:&quot;, round(ridge_search.best_estimator_.alpha, 3)) print(&quot;Best parameter score:&quot;, round(ridge_search.best_score_, 3)) . print(&quot;Coefficients:&quot;, ridge_search.best_estimator_.coef_) . ridge_pred = ridge_search.predict(x_test) . print(&quot;Ridge MSE for test data:&quot;, round(mean_squared_error(y_test, ridge_pred),2)) . Lasso . param_grid = {&#39;alpha&#39;: uniform()} model = Lasso() lasso_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=100) lasso_search.fit(x_train, y_train) print(&quot;Optimal lasso penality parameter:&quot;, round(lasso_search.best_estimator_.alpha, 3)) print(&quot;Best parameter score:&quot;, round(lasso_search.best_score_, 3)) . print(&quot;Coefficients:&quot;, lasso_search.best_estimator_.coef_) . lasso_pred = lasso_search.predict(x_test) . print(&quot;Lasso MSE for test data:&quot;, round(mean_squared_error(y_test, lasso_pred), 2)) . Adaptive Lasso . coefficients = LinearRegression(fit_intercept=False).fit(x_train, y_train).coef_ gamma = 2 weights = coefficients**-gamma X = x_train/weights lambdas, lasso_betas, _ = lasso_path(X, y_train) lassoCV = LassoCV(alphas=lambdas, fit_intercept=False, cv=10) lassoCV.fit(X, y_train) . print(&quot;Optimal adaptive lasso penality parameter:&quot;, lassoCV.alpha_) . print(&quot;Coefficients:&quot;, lassoCV.coef_) . adaptive_pred = lassoCV.predict(x_test/weights) . print(&quot;Adaptive Lasso MSE for test data:&quot;, round(mean_squared_error(y_test, adaptive_pred), 2)) . Elastic Net . param_grid = {&#39;alpha&#39;: uniform(), &#39;l1_ratio&#39;: arange(0, 1, 0.01)} model = ElasticNet() EN_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=100) EN_search.fit(x_train, y_train) print(&quot;Optimal parameters:&quot;, EN_search.best_params_) print(&quot;Best parameter score:&quot;, round(EN_search.best_score_, 3)) . print(&quot;Coefficients:&quot;, EN_search.best_estimator_.coef_) . EN_pred = EN_search.predict(x_test) . print(&quot;Elastic Net MSE for test data:&quot;, round(mean_squared_error(y_test, EN_pred), 2)) . Conclusion . Elastic net can be recommended without knowing the size of the dataset or the number of predictors, as it automatically handles data with various sparsity patterns as well as correlated groups of regressors. Lasso outperforms ridge for data with a small to moderate number of moderate-sized effects. In these cases, rdige will not provide a sparse model that is easy to interpret, which would lead one to use Lasso methods. On the other hand, Ridge regression performs the best with a large number of small effects.This is because the ridge penalty will prefer equal weighting of colinear variables while lasso penalty will not be able to choose. This is one reason ridge (or more generally, elastic net, which is a linear combination of lasso and ridge penalties) will work better with colinear predictors. If the data give little reason to choose between different linear combinations of colinear predictors, lasso will struggle to prioritize a predictor amongst colinears, while ridge tends to choose equal weighting. Given our dataset and number of predictors here, I would recommend Lasso. .",
            "url": "https://jackhmiller.github.io/My-DS-Blog/2018/03/17/Sparse.html",
            "relUrl": "/2018/03/17/Sparse.html",
            "date": " • Mar 17, 2018"
        }
        
    
  
    
        ,"post23": {
            "title": "A Linear Algebraic Perspective of Neural Networks",
            "content": "Introduction . With the increased popularity of neural networks, some of the mathematical intuition and foundations can be abstracted away thanks to frameworks like Tensorflow, which make neural networks readily available due ease-of-implementation. However, in order to be able to use such tools effectively or to simply appreciate their effectiveness, it is important to remind ourselfs of their humble foundations. . Linear Algebra – A Preface . Linear algebra provides use with a framework for working with linear transformations via matrix multiplication. In fact, neural networks provide us with another framework for doing the same thing (and more). For example, the following linear transformation that takes a vectory $(x,y)$ and produces $(2x, y)$: [2001][31]=[61] begin{bmatrix} 2 &amp; 0 0 &amp; 1 end{bmatrix} begin{bmatrix} 3 1 end{bmatrix} = begin{bmatrix} 6 1 end{bmatrix}[20​01​][31​]=[61​] . This is how the following equation could be viewd as a neural network with one input layer and one output layer: . . 3 . 1 . 1 . 6 . . . 21 . 00Input LayerOutput Layer In the neural network diagram above, each output unit produces the linear combination of the inputs and the connection weights, which is the same thing we do with matrix multiplication. . We’d like handle affine transformations, which are basically linear transformations with translations allowed. With linear algebra, we usually handle affine transformations using vector addition: . $$ begin{bmatrix} 2 &amp; 0 0 &amp; 1 end{bmatrix} begin{bmatrix} 3 1 end{bmatrix} + . begin{bmatrix} 0 4 end{bmatrix} = begin{bmatrix} 6 5 end{bmatrix} $$ In our neural network, this affine transformation would take the form of bias inputs: . . 3 . 1 . 5 . 6 . . . 21 . 00Input LayerOutput Layer . 1 . 04 Like before, each output unit performs a linear combination of the incoming weights and inputs. This time though, the units have a constant bias input, which each output unit can weight independently to achieve the effect of a translation vector. In this case we use the weight 0 for the first output unit to zero out the bias. We use the weight 4 for the second output unit to scale the bias accordingly. . Neural networks as general nonlinear transformers . Let’s take a simple yet classic classification problem, something which is not linearly separable, to demonstrate the superior capabilities of neural networks over linear models. We can synthetically generate a spiral dataset as shown below with 2 classes and a 1000 points. . A linear model is not going to be able to separate these classes, since linear models will have a straight line as a decision boundary. However, it is very simply to train a Neural Network on this data to classify them with high accuracy. Using just two hidden layers with 128 and 2 units, respectively, gives us an accuracy of 95%. As we can see below, the neural network created a non-linear decision boundary. . . We know that neural networks are a bunch of linear transformations with non-linearities spread in between, with the final layer usually being a linear transformation. So, if we consider all the layers before the last layer, all they are doing is apply matrix multiplication and activation functions so that the data becomes linearly separable for the final layer. From the geometric interpretation of linear algebra, this is just a linear transformation of the vector space. . Let us again visualize the points in original space as well as the output of the final hidden layer of the network network. . . We can see that the hidden layers have learnt is a transformation from the original input space to another space in which the points are linearly separable! This “learning” is essentially the warping and folding of space by the neural network so that the input points become linearly separable. This is made possible by the activation function (in this case ReLU), which introduces non-linearity. If there was no activation function, the total transformation would still be linear and would not be able to resolve the non-linearly distributed points to linearly separable ones. .",
            "url": "https://jackhmiller.github.io/My-DS-Blog/2018/02/12/linear-alg-NN.html",
            "relUrl": "/2018/02/12/linear-alg-NN.html",
            "date": " • Feb 12, 2018"
        }
        
    
  
    
        ,"post24": {
            "title": "Group Lasso",
            "content": "It is frequently the case when dealing with high dimensional datasets that there are more variables than observations but we only expect a small fragment of the variables to be truly meaningful. To compensate for such occurances and enhance our ability to generalize our resulting model, it is common to employ regularization techniques which are also used to reduce overfitting in other settings. The most common examples are L1 (Lasso) and L2 (Ridge) regularization. Lasso, in particular, causes sparsity for weights. It provides sparse solutions, because it will send to zero some of the β coefficients (the least related with the response variable). The effect of this penalization can be controlled using the λ parameter. A large λ value provides solutions where the penalization has a greater importance, and thus there are more zeros among the β coefficients. . However, there are many regression problems dealing with high dimensional data in which the covariates have a natural group structure, and it is desirable to have all coefficients within a group become nonzero (or zero) simultaneously. For example, in biostatistics it is common to deal with genetic datasets in which predictors are grouped into genetical pathways. In stock market analysis one can group companies from the same business segment. In climate data one can group different regions… And lasso provides individual sparse solutions, not group sparse. A more general leading example is when we have qualitative factors among our predictors. We typically code their levels using a set of dummy variables or contrasts, and would want to include or exclude this group of variables together. For such a scenario, we have a technique called the Group Lasso, which accounts for a natural grouped structure of predictors while causing sparsity for weights. . In the image above, we can see a visual depiction of the contours of the L1, group lasso, and L2 penalties respectively (Source: Ming Yuan, &quot;Model selection and estimation in regression with grouped variables,&quot; Statistical Methodology 68:1 (Feb 2006). Specifically, we can see how the group lasso (middle image) incorporates elements of both the L1 and L2 penalties. . Lets get into the math of the Group Lasso. Consider a linear regression model involving J groups of covariates, where j = 1,..., J, and the vector $ mathbb{Z}_j in mathbb{R}^{pj}$ represents the covariates of group j. Our goal is to predict a real valued response $Y in mathbb{R}$ based on the collection of covariates ($ mathbb{Z}_1, ..., mathbb{Z}_J $. A linear model for the regression function $ mathbb{E}(Y| mathbb{Z})$ $ sum_{j=1}^{J} mathbb{Z}_{j}^{T} theta_j$ where $ theta_j in mathbb{R}^{pj}$ represents a group of $p_j$ regression coefficients. . Given a collection of N samples $ left { (y_i, z_{i1}, z_{i2},..., z_{iJ}) right }_{i=1}^{N}$ the group lasso solves the convex problem: $$ underset{ theta_j in mathbb{R}^{pj}}{min} frac{1}{2} left | y- sum_{j=1}^{J} mathbb{Z}_{j}^{T} theta_j right |_{2}^{2} + lambda sum_{j=1}^{J} left | theta_j right |_2 $$ where $ left | theta_j right |_2$ is the euclidean norm of the vector $ theta_j$. . This is a group generalization of the lasso, with the properties: . depending on $ lambda geq 0$, either the entire vector $ theta_j$ will be zero, or all its elements will be nonzero | when $p_j=1$ (continuous variables), then we have $ left | theta_j right |_2 = left | theta_j right |$, so if all of the groups are singletons, the optimization problem reduces to ordinary lasso. | . We can solve the group lasso problem using block coordinate descent. Here is a proof showing that we can solve it iteratively, for $j = 1 · · · , J$: . We start with our optimization problem: $$ underset{ theta_j in mathbb{R}^{pj}}{min} frac{1}{2} left | r_j- mathbb{Z}_j^T theta_j right |_2^2 + lambda left | theta_j right |_2 $$ Where $r_j = y - sum_{k neq j}^{}Z_k^T theta_k$ . With a bit of manipulation we get $$ -Z_j^T(y- sum_{j=1}^{J}Z_j theta_j)+ lambda(s( frac{ theta_j}{ left | theta_j right |_{2}})) $$ . Using the definition of $r_j$ we can solve for $ theta$: $$ theta_j = (Z_j^TZ_j + frac{ lambda}{ left | theta_j right |_2})^{-1}Z_j^Tr_j $$ . Or $$ theta_j = S_{ frac{ lambda}{ left | theta_j right |^2}}( frac{Z_{j}^{T}r_j}{Z_{j}^{T}Z_j}) $$ . . We will now implement the block coordinate proximal gradient descent to solve the group lasso problem presented above. We will be using the kaggle classic boston housing dataset, where our independent variable is price, and our dependent variables are a combination of categorical (number of bed/bath) and continous (price/sq feet) features. Our first step is to create dummy variables corresponding to the categorical variables. To avoid multicollinearity issues we use 0 bedrooms, 1 bathroom, and short sale as baselines, respectively. To improve results we standardize our data, and will use a $ lambda$ value of 0.012. . from sklearn.preprocessing import MinMaxScaler import itertools . bh = pd.read_csv(&quot;boston_housing.csv&quot;) . Create dummy variables . rooms = pd.get_dummies(bh.Bedrooms, prefix=&#39;Bedrooms&#39;) baths = pd.get_dummies(bh.Bathrooms, prefix=&#39;Bath&#39;) status = pd.get_dummies(bh.Status, prefix=&#39;Status&#39;) . df = pd.concat([bh, rooms, baths, status], axis=1) . df.drop([&#39;Bathrooms&#39;, &#39;Bedrooms&#39;, &#39;Status&#39;], axis=1, inplace=True) . Normalize data . scaler = MinMaxScaler() df[&#39;Price&#39;] = scaler.fit_transform(df[&#39;Price&#39;].values.reshape(-1,1)) df[&#39;PriceSF&#39;] = scaler.fit_transform(df[&#39;PriceSF&#39;].values.reshape(-1,1)) . df.head() . Price PriceSF Bedrooms_0 Bedrooms_1 Bedrooms_2 Bedrooms_3 Bedrooms_4 Bath_1 Bath_2 Bath_3 Bath_4 Status_1 Status_2 Status_3 . 0 0.389410 | 0.280785 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | . 1 0.188751 | 0.108646 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | . 2 0.262731 | 0.142556 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | . 3 0.447175 | 0.211009 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | . 4 0.042260 | 0.061014 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | . y = df[&#39;Price&#39;].values . features = df.drop(&#39;Price&#39;, axis=1) . groups = [&#39;PriceSF&#39;, &#39;Bed&#39;, &#39;Bath&#39;, &#39;Status&#39;] . Defining our soft thresholding function for PGM, and our loss function. . def soft_threshold(x, gamma=lamda): for i, val in enumerate(x): if val &gt; gamma: x[i] = 1-(lamda/abs(val-gamma)) elif val &lt; gamma: x[i] = 1-(lamda/abs(val+gamma)) elif (val &lt;= gamma) and (val&gt;= -gamma): x[i] = 0 return x def loss(b, l=0.012): temp_coeffs = [[beta]*i for beta, i in zip(b, group_lengths)] coeff_vector = np.array(list(itertools.chain(*temp_coeffs))) f_x = np.sum(0.5*(y - np.dot(features.values,coeff_vector))**2) penalty = l*sum([i**2 for i in b]) return f_x + penalty def create_b_not_vector(b, i): not_group_lengths = [j for k, j in enumerate(group_lengths) if k != i] temp_coeffs = [[beta]*i for beta, i in zip(b, not_group_lengths)] return np.array(list(itertools.chain(*temp_coeffs))) . slices = [(0,1), (1, 6), (6, 10), (10, 13)] b = np.random.rand(len(features.columns)) lamda = 0.012 losses = [loss(b)] iterations = 1 for iteration in range(200): for sliced in slices: Z = features[features.columns[sliced[0]:sliced[1]]] Z_cols = Z.columns Z_not = features.loc[:, [feat for feat in features.columns if feat not in Z_cols]] b_not = [i for j, i in enumerate(b) if j not in range(sliced[0], sliced[1])] r = y - np.dot(Z_not, b_not) a = b[sliced[0]:sliced[1]] - np.sum((-Z.values.T*(r-np.dot(Z.values, b[sliced[0]:sliced[1]]))), axis=1) b[sliced[0]:sliced[1]] = soft_threshold(a) f_x = np.sum(0.5*(y - np.dot(features.values,b))**2) penalty = lamda*sum([i**2 for i in b]) losses.append(f_x + penalty) iterations += 1 . And there you have it .",
            "url": "https://jackhmiller.github.io/My-DS-Blog/2018/01/23/GL.html",
            "relUrl": "/2018/01/23/GL.html",
            "date": " • Jan 23, 2018"
        }
        
    
  
    
        ,"post25": {
            "title": "Zero Inflated Poisson (ZIP) Regression",
            "content": "Motivation . The Poisson distribution can be derived as a limiting form of the binomial distribution if you consider the distribution of the number of successes in a very large number of Bernoulli trials with a small probability of success in each trial. Thus, the Poisson distribution provides an approximation to the binomial for the analysis of rare events. . The classic text on probability theory by Feller (1957) includes a number of examples of observations fitting the Poisson distribution, including data on the number of flying-bomb hits in the south of London during World War II. The city was divided into 576 small areas of one-quarter square kilometers each, and the number of areas hit exactly k times was counted. There were a total of 537 hits, so the average number of hits per area was 0.9323. . In the flying-bomb example, we can think of each day as one of a large number of trials where each specific area has only a small probability of being hit. Assuming independence across days would lead to a binomial distribution which is well approximated by the Poisson. Alternatively, if we consider the Poisson distribution in terms of a stochastic process, the porbability of at least one occurence of the event in a given time interval is propotional to the length of the interval. . Unlike this traditional Poisson distribution, many real world phenomena produce counts that are almost always zero. For example, the number of sales per item in a store. Such data are hard to deal with using traditional models for counts data such as the Poisson, the Binomial or the Negative Binomial regression models. This is because such data sets contain more number of zero valued counts than what one would expect to observe using the traditional model’s probability distribution. . To demonstrate, recall the PMF for a Poisson distribution $$ P(y=k) = frac{e^{- lambda}* lambda^k}{k!} $$ Where $ lambda_i = e^{x_i beta}$- here $ beta$ is the vector of regression coefficients . If we assume that our events obey a Poisson(5) process, you would expect to see zero counts no more than 0.67% of the time: . s = np.random.poisson(5, 10000) sns.histplot(s) . &lt;AxesSubplot:ylabel=&#39;Count&#39;&gt; . As a result, if you use a standard Poisson or Binomial or NB regression model on such data sets, it can fit badly and will generate poor quality predictions, no matter how much you tweak its parameters . . The ZIP Model . Fortunately, the ZIP model provides us with a modification of the standard counts model such as Poisson or Negative Binomial to account for the presence of the extra zeroes. Another technique, called the Hurdle Model, has also been designed to deal with the excess zeroes, but here we will focus on ZIP. . The intuition behind the ZIP model is that there is a second underlying process that is determining whether a count is zero or non-zero. Once a count is determined to be non-zero, the regular Poisson process takes over to determine its actual non-zero value based on the Poisson process’s PMF. The PMF of the ZIP model is as follows: $$ P(y_i) left { begin{matrix} y_i = 1,2,3... &amp; phi_i + (1- phi_i)*e^{- lambda} y_i = 0 &amp; (1- phi_i) frac{e^{- lambda}* lambda^k}{k!} end{matrix} right. $$ . The main difference between the ZIP model and the above mentioned Poisson process if the addition of the $ phi$ parameter, which is a measure of the proportion of excess zeroes corresponding to the ith row in the dataset. Described differently, it can be considered a weight or penalty that increased the tendancy of the model to add structural zeroes to its posterior modeling of the distribution. Thus, a $ phi$ value of zero results in the ZIP model reducing to a standard Poisson process and PMF. . A crude way to calculate $ phi$ would be to set it as the number of zero valued samples divided by the total number of samples. A better way to estimate $ phi$ would be to estimate it as a function of our regressor variables/features. This is done by transforming the y variable (a continuous count) into a binary 0 or 1 variable. We then fit a logistic regression model on the binary y to produce fitted probabilities of an outcome whith or without an event. This vector of probabilities of a non-zero event is set as our $ phi$ vector. We can then use Maximum Likelihood Estimation (MLE) to train the ZIP model. . Demo . Thankfully, there are many python packages that automate the procedure of estimating $ phi$ and using the estimated $ phi$ to train the ZIP model using the MLE technique on your data set. Here we will use statsmodels, but there are other python implementations such as pymc3. Along with the statsmodels implementation, we will use daily retail sales data as our features. . import pickle import statsmodels.api as sm import pandas as pd from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score, f1_score, mean_absolute_error, r2_score import xgboost as xgb import warnings warnings.filterwarnings(&#39;ignore&#39;) . with open(weekly_sales, &#39;rb&#39;) as f: frames = pickle.load(f) . w1_df = pd.concat(frames) df = w1_df.sample(n=100000) . Below we can see that most of our entries for y are zero, but we also have considerable sales (count) data. . sns.distplot(y, rug=True) . &lt;AxesSubplot:ylabel=&#39;Density&#39;&gt; . X = df.drop([&#39;part&#39;, &#39;sales_quantity&#39;], axis=1) y = df[&#39;sales_quantity&#39;].values X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42) . There are two critical parameters the class constructor can take, which are &quot;inflation&quot; and &quot;exog_infl&quot;: . inflation: The ZeroInflatedPoisson model class will internally use a LogisticRegression model to estimate the parameter ϕ. Hence we set the model parameter inflation to ’logit’. We can also experiment with setting it to other Binomial link functions such as ‘probit’. | exog_infl: We also want to ask the ZIP model to estimate ϕ as a function of the same set of regression variables as the parent model, namely: LIVE_BAIT, CAMPER, PERSONS and CHILDREN. Hence we set the parameter exog_infl to X_train. If you want to use only a subset of X_train, you can do so, or you can set exog_infl to an entirely different set of regression variables. | . zip_training_results = sm.ZeroInflatedPoisson(endog=y_train, exog=X_train, exog_infl=X_train, inflation=&#39;logit&#39;).fit() . Warning: Desired error not necessarily achieved due to precision loss. Current function value: 7.028915 Iterations: 0 Function evaluations: 15 Gradient evaluations: 3 . zip_predictions = zip_training_results.predict(X_test ,exog_infl=X_test) . print(&quot;Mean Absolute Error (MAE) for ZIP predictions: {0}&quot;.format(mean_absolute_error(y_test, zip_predictions))) . Mean Absolute Error (MAE) for ZIP predictions: 0.29544 . xgb_reg = xgb.XGBRegressor() xgb_reg.fit(X_train, y_train) xgb_predictions = xgb_reg.predict(X_test) . print(&quot;Mean Absolute Error (MAE) for XGBoost predictions: {0}&quot;.format(mean_absolute_error(y_test, xgb_predictions))) . Mean Absolute Error (MAE) for XGBoost predictions: 0.35163331031799316 . Thus the ZIP model performs significantly better than a gradient boosting algorithm for our case. .",
            "url": "https://jackhmiller.github.io/My-DS-Blog/2017/12/21/ZIP.html",
            "relUrl": "/2017/12/21/ZIP.html",
            "date": " • Dec 21, 2017"
        }
        
    
  
    
        ,"post26": {
            "title": "A Detailed Guide to 5 Loss Functions for Machine Learning Algorithms with Python Code",
            "content": "In supervised machine learning algorithms, we want to minimize the error for each training example during the learning process. This is done using some optimization strategies like gradient descent. And this error comes from the loss function. Similat to a loss function, we also use a cost function. However, although cost function and loss function are synonymous and used interchangeably, they are different. A loss function is for a single training example. It is also sometimes called an error function. A cost function, on the other hand, is the average loss over the entire training dataset. The optimization strategies aim at minimizing the cost function. . . Regression Loss Functions . You must be quite familiar with linear regression at this point. It deals with modeling a linear relationship between a dependent variable, Y, and several independent variables, X_i’s. Thus, we essentially fit a line in space on these variables $Y = a0 + a1 * X1 + a2 * X2 + ....+ an * Xn$. We will use the given data points to find the coefficients a0, a1, …, an. . We will use the famous Boston Housing Dataset for understanding this concept. And to keep things simple, we will use only one feature – the Average number of rooms per dwelling (X) – to predict the dependent variable – Median Value (Y) of houses in USD 1000′ s. We will use Gradient Descent as an optimization strategy to find the regression line. Just as a refresher, the gradient descent update is as follows: $$ text{Repeat until convergence} left { theta_j gets theta_j - alpha frac{ partial }{ partial theta_j}J( theta) right } $$ . Here, $ theta_j$ is the weight to be updated, alpha is the learning rate and J is the cost function. The cost function is parameterized by theta. Our aim is to find the value of theta which yields minimum overall cost . For each loss function we will follow the following steps: . Write the expression for our predictor function, f(X) and identify the parameters that we need to find | Identify the loss to use for each training example | Find the expression for the Cost Function – the average loss on all examples | Find the gradient of the Cost Function with respect to each unknown parameter | Decide on the learning rate and run the weight update rule for a fixed number of iterations | import numpy as np import pandas as pd import matplotlib.pyplot as plt df = pd.read_csv(&#39;housing.csv&#39;) X = df[&#39;RM&#39;].values y = df[&#39;MEDV&#39;].values . . 1. Squared Error Loss . Squared Error loss for each training example, also known as L2 Loss, is the square of the difference between the actual and the predicted values: $ L = (y-f(x))^2$. The corresponding cost function is the Mean of these Squared Errors (MSE). It is a positive quadratic function (of the form ax^2 + bx + c where a &gt; 0). A quadratic function only has a global minimum. Since there are no local minima, we will never get stuck in one. Hence, it is always guaranteed that Gradient Descent will converge. . def squared_loss_update(m, b, X, y, learning_rate): # intitialize at zero m_deriv = 0 b_deriv = 0 N = len(X) for i in range(N): # calculate partial derivatives for loss function (y-mx-b)^2 # -2x(y-mx-b) m_deriv = -2*X[i] * (y[i]-(m*X[i])-b) # -2(y-mx-b) b_deriv = -2 * (y[i] - m*X[i]-b) m -= (m_deriv/float(N)) * learning_rate b -= (b_deriv/float(N)) * learning_rate return m, b . MSE = [] m = 0.001 b = 0 N = len(X) lr = 0.0001 for _ in range(0, 500): loss = 0 for i in range(0, N): loss += (y[i] - m*X[i] - b)**2 loss /= float(N) MSE.append(loss) m, b = squared_loss_update(m, b, X, y, lr) . index = range(500) plt.plot(index, MSE, label = &#39;alpha = 0.0001&#39;) . [&lt;matplotlib.lines.Line2D at 0x2cc24c9f730&gt;] . . 2. Mean Absolute Error Loss . Absolute Error for each training example is the distance between the predicted and the actual values, irrespective of the sign. Absolute Error is also known as the L1 loss: $L = left| y-f(x) right|$. The cost is the Mean of these Absolute Errors (MAE), and it is more robust to outliers than MSE. Lets code the weights update for MAE. Recall, for the purposes of calculating the partial derivatives that $ frac{ partial }{ partial x_j}(x to left | x_1 right |) = frac{x_j}{ left| x_j right|}$. . def MAE_loss_update(m, b, X, y, learning_rate): m_deriv = 0 b_deriv = 0 N = len(X) for i in range(N): # calculate partial derivatives # -x(y-mx-b) / |mx +b| m_deriv += (-X[i] * (y[i] - m*X[i] -b)) / (abs(y[i] - m*X[i]-b)) # -(y-mx-b) / |mx +b| b_deriv += -(y[i] - m*X[i] -b) / (abs(y[i] - m*X[i]-b)) m -= (m_deriv / float(N)) * learning_rate b -= (b_deriv / float(N)) * learning_rate return m, b . MAE = [] m = 0.0001 b = 0 lr = 0.01 for _ in range(0, 500): loss = 0 for i in range(0, N): loss += abs(y[i] -m*X[i] -b) loss /= float(N) MAE.append(loss) m, b = MAE_loss_update(m, b, X, y, lr) . . 2. Mean Absolute Error Loss . The Huber loss combines the best properties of MSE and MAE. It is quadratic for smaller errors and is linear otherwise (and similarly for its gradient). It is identified by its delta parameter: $$ L_ delta = left { begin{matrix} frac{1}{2}(y-f(x))^2, &amp; if left | y-f(x) right | leq delta delta left | y-f(x) right | - frac{1}{2} delta^2 &amp; otherwise end{matrix} right. $$ . def Huber_update(m, b, X, Y, delta, learning_rate): m_deriv = 0 b_deriv = 0 N = len(X) for i in range(N): # derivative of quadratic for small values and of linear for large values if abs(Y[i] - m*X[i] - b) &lt;= delta: m_deriv += -X[i] * (Y[i] - (m*X[i] + b)) b_deriv += - (Y[i] - (m*X[i] + b)) else: m_deriv += delta * X[i] * ((m*X[i] + b) - Y[i]) / abs((m*X[i] + b) - Y[i]) b_deriv += delta * ((m*X[i] + b) - Y[i]) / abs((m*X[i] + b) - Y[i]) # We subtract because the derivatives point in direction of steepest ascent m -= (m_deriv / float(N)) * learning_rate b -= (b_deriv / float(N)) * learning_rate return m, b . Huberloss = [] m = 0.001 b = 0 learning_rate = 0.0001 delta = 20 for iter in range(0, 500): loss = 0 for i in range(0, N): if abs(y[i] - m*X[i] - b) &lt;= delta: loss += ((y[i] - m * X[i] - b) ** 2) / 2 else: loss += delta * abs(y[i] - m * X[i] - b) - (delta ** 2) / N loss /= float(N) Huberloss.append(loss) m, b = Huber_update(m, b, X, y, delta, learning_rate) . Binary Classification Loss Functions . Binary Classification refers to assigning an object into one of two classes. This classification is based on a rule applied to the input feature vector. For example, classifying an email as spam or not spam based on, say its subject line, is binary classification. Lets illustrate these binary classification loss functions on the Breast Cancer dataset. A greater value of entropy for a probability distribution indicates a greater uncertainty in the distribution. Likewise, a smaller value indicates a more certain distribution. This makes binary cross-entropy suitable as a loss function – you want to minimize its value. We use binary cross-entropy loss for classification models which output a probability p. Then, the cross-entropy loss for output label y (can take values 0 and 1) and predicted probability p is defined as: $$ L = -y*log(p) - (1-y)*log(1-p) = left { begin{matrix} -log(1-p) &amp; y=0 -log(p) &amp; y=1 end{matrix} right. $$ . This is also called Log-Loss. To calculate the probability p, we can use the sigmoid function. Here, z is a function of our input features: $S(z) = frac{1}{1+e^{-z}}$ . import math from sklearn.datasets import load_breast_cancer from sklearn.preprocessing import MinMaxScaler cancer_dataset = load_breast_cancer() data1 = pd.DataFrame(cancer_dataset.data, columns=cancer_dataset.feature_names) data1[&#39;Class&#39;] = cancer_dataset.target X1 = data1[&#39;worst area&#39;].values.reshape(-1, 1) X2 = data1[&#39;mean symmetry&#39;].values.reshape(-1, 1) Y = data1[&#39;Class&#39;].values N1 = len(X1) scaler = MinMaxScaler() X1 = scaler.fit_transform(X1) X2 = scaler.fit_transform(X2) . 1. Binary Cross Entropy Loss . def BCE_update(m1, m2, x1, x2, y, learning_rate): m1_deriv = 0 m2_deriv = 0 b_deriv = 0 N = len(X1) for i in range(0, N): m1_deriv += -X1[i] * (s - Y[i]) m2_deriv += -X2[i] * (s - Y[i]) b_deriv += -(s - Y[i]) m1 -= (m1_deriv/float(n)) * learning_rate m2 -= (m2_deriv/float(n)) * learning_rate b -= (b_deriv/float(n)) * learning_rate return m1, m2, b . BCE = [] m1 = 0 m2 = 0 b = 0 lr = 0.0001 for _ in range(500): loss = 0 for i in range(0, N1): p = 1/(1 + math.exp(-m1*X1[i] - m2*X2[i] - b)) if Y[i] == 0: loss += -math.log(1-p) else: loss += -math.log(p) loss /= float(N1) BCE.append(loss) m1, m2, b = BCE_update(m1, m2, b, X1, X2, Y, lr) . 2. Hinge Loss . Hinge loss is primarily used with Support Vector Machine (SVM) Classifiers with class labels -1 and 1. So make sure you change the label of the ‘Malignant’ class in the dataset from 0 to -1. Hinge Loss not only penalizes the wrong predictions but also the right predictions that are not confident. Hinge loss for an input-output pair (x, y) is given as: $$ L = max(0, 1-y*f(x)) $$ . def Hinge_update(m1, m2, b, X1, X2, y, learning_rate): m1_deriv = 0 m2_deriv = 0 b_deriv = 0 N = len(X1) for i in range(0, N): if Y[i]*(m1*X1[i] + m2*X2[i] + b) &lt;= 1: m1_deriv += -X1[i] * Y[i] m2_deriv += -X2[i] * Y[i] b_deriv += -Y[i] m1 -= (m1_deriv / float(N)) * learning_rate m2 -= (m2_deriv / float(N)) * learning_rate b -= (b_deriv / float(N)) * learning_rate return m1, m2, b . Hinge = [] m1 = 0 m2 = 0 b = 0 learning_rate = 0.001 for _ in range(0, 1000): loss = 0 for i in range(0, N1): if Y[i]*(m1*X1[i] + m2*X[i] + b) &lt;= 1: loss += 1 - Y[i]*(m1*X1[i] + m2*X[i] + b) loss /= float(N1) Hinge.append(loss) m1, m2, b = Hinge_update(m1, m2, b, X1, X2, y, learning_rate) .",
            "url": "https://jackhmiller.github.io/My-DS-Blog/2017/11/13/Losses.html",
            "relUrl": "/2017/11/13/Losses.html",
            "date": " • Nov 13, 2017"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "My CV",
          "content": "Work Experience . Senior Data Scientist at CB4, A Gap Company . Herzliya, October 2021 - Present . Research, design, and implementation in production of deep learning models for predicting inventory inefficiencies and lost sales opportunities across all Gap retail and online stores | Models include zero-inflated deep hurdle model for out-of-stock prediction, demand forecasting with the Temporal Fusion Transformer (PyTorch), data/concept drift model for monitoring | Image feature extraction and text embeddings for sales forecasting for new products | Built and deployed parallelized and containerized model deployment at scale in a Kafka event-driven microservice architecture in Kubernetes using Terraform-managed Google Big Query infrastructure | . Data Scientist at Sparks AB . Hod Hasharon, October 2018 – October 2021 . Built and deployed end-to-end ML pipelines for financial instrument trading and forecasting | Research and implementation of different trading strategies and data preprocessing techniques such as the application of computer vision and wavelet transformations to signals | Oversaw data orchestration, model training/optimization, and deployment architecture | Development and deployment forecasting models that include temporal CNNs, LSTMs, deep autoregressive models (GluonTS DeepAR), and seasonal hybrid anomaly detection models | ML Ops using AWS S3/Lambda/SageMaker, Jenkins, custom API to automate training and deployment | . Junior Data Scientist at EY (Ernst &amp; Young) . Tel Aviv, November 2016 – December 2018 . Digital innovation team designed develop and deploy machine learning-based products both internally and to clients to increase efficiency of tax &amp; audit processes | Image segmentation and recognition for automatic form competition | Classification model for tax-sensitive transactions including anomaly/fraud detection | Autoregressive forecasting model for regulatory penalty scheduling | . Quantitative Analyst at McKinsey &amp; Co. . Washington D.C., January 2013-October 2015 . Data analytics and predictive modeling for defense acquisition processes between US Department of Defense and defense manufactures | Development of unsupervised clustering models for network analysis and defense platform optimization | . Education . University of Pennsylvania, BA Statistics | 2010-2014 . Thesis: Spatial Clustering of Innovations in Military Theaters | GPA: 3.9/4.0 Magna Cum Laude | . Georgia Tech, Msc Computer Science | 2015-2019 . Computational Perception and Robotics | GPA: 3.8/4.0 | .",
          "url": "https://jackhmiller.github.io/My-DS-Blog/CV/",
          "relUrl": "/CV/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "About Me",
          "content": "I am a data scientist working and based in Israel, with an undergraduate degree in statistics from the University of Pennsylvania, and a masters degree in computer science Georgia Tech. . 050-709-2944 | Jack.harris.miller@gmail.com . .",
          "url": "https://jackhmiller.github.io/My-DS-Blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  

  
  

  
      ,"page13": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jackhmiller.github.io/My-DS-Blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}