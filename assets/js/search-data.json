{
  
    
        "post0": {
            "title": "Title",
            "content": "In this blog post we will solve the following optimization problem using the scaled form of alternating direction method of multipliers (ADMM). . $$ min frac{1}{2} left | Ax-b right |_{2}^{2} + lambda_1 left | x right |_1 + frac{ lambda_2}{2} left | x right |_{2}^{2}min frac{1}{2} left | Ax-b right |_{2}^{2} + lambda_1 left | x right |_1 + frac{ lambda_2}{2} left | x right |_{2}^{2}$$ . Background on ADMM . The alternating direction method of multipliers (ADMM) is an algorithm that solves convex optimization problems by breaking them into smaller pieces, each of which are then easier to handle. Namely, it is intended to blend the decomposability of dual ascent with the superior convergence properties of the method of multipliers. The original paper can be found here: https://stanford.edu/~boyd/papers/pdf/admm_distr_stats.pdf. . ADMM received lots of attention a few years ago due to the tremendous proliferation and subsequentdemand from large-scale and data-distributed machine learning applications. ADMM a fairly simple computational method for optimization proposed in 1970s. It stemmed from the augmented Lagrangian method (also known as the method of multipliers) dating back to late 1960s. The theoretical aspects of ADMM have been studied since the 1980s, and its global convergence was established in the literature (Gabay, 1983; Glowinski &amp; Tallec, 1989;Eckstein &amp; Bertsekas, 1992). As reviewed in the comprehensive paper (Boyd et al., 2010), with the ability of dealing with objective functions separately and synchronously , ADMM turned out to be a natural fit in the field of large-scale data-distributed machine learning and big-data related optimization, and therefore received significant amount of attention beginning in 2015. Considerable work was conducted thereafter. . On the theoretical side, ADMM was shown to have an O(1/N) rate of convergence for convex problems. . The algorithm solves the problem in the form: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; minimize $f(x) + g(z)$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; subject to $Ax+ Bz = c$ . with variables $x in R^n$ and $z in R^m$, where $A in R^{pxn}$, $B in R^{pxm}$ and $C in R^{p}$. . The only difference from the general linear equality-constrained problem is that the variable x has been split into two parts, called x and z, with the objective function separable across this splitting. The optimal value of the problem is now denoted by: $$ p^* = inf left { f(x) + g(x) | Ax + Bz = c right } $$ Which forms the augmented Lagrangian: $$ L_p(x, z, y) = f(x) + g(z) + y^T(Ax+Bz-c) + frac{ rho}{2} left | Ax + Bz -c right |_{2}^{2} $$ . Finally, we have our ADMM which consists of the following iterations: $$ x^{k+1} = underset{x}{argmin} L_ rho(x, z^k, y^k)$$ $$ z^{k+1} = underset{x}{argmin} L_ rho(x^{k+1}, z, y^k) $$ $$ y^{k+1} = y^k + rho(Ax^{k+1} +Bz^{k+1}-c) $$ $$s.t. rho&gt;0$$ . The algorithm is very similar to dual ascent and the method of multipliers: it consists of an x-minimization step, a z-minimization step, and a dual variable update. As in the method of multipliers, the dual variable update uses a step size equal to the augmented Lagrangian parameter. However, while with the method of multipliers the augmented Lagrangian is minimized jointly with respect to the two primal variables, in ADMM, on the other hand, x and z are updated in an alternating or sequential fashion, which accounts for the term alternating direction. . Simple examples show that ADMM can be very slow to converge to high accuracy. However, it is often the case that ADMM converges to modest accuracy—sufficient for many applications—within a few tens of iterations. This behavior makes ADMM similar to algorithms likethe conjugate gradient method, for example, in that a few tens of iterations will often produce acceptable results of practical use. However, the slow convergence of ADMM also distinguishes it from algorithms such as Newton’s method (or, for constrained problems, interior-point methods), where high accuracy can be attained in a reasonable amount of time. While in some cases it is possible to combine ADMM with a method for producing a high accuracy solution from a low accuracy solution, in the general case ADMM will be practically useful mostly in cases when modest accuracy is sufficient. Fortunately, this is usually the case for large-scale industrial applications. Also, in the case of machine learning problems, solving a parameter estimation problem to very high accuracy often yields little to no improvement in actual prediction performance, the real metric of interest in applications. . Our Optimization Problem . First we will write the augmented Lagrangian function (the scaled form) and drive the ADMM updates. . Scaled form of the augmented Lagrangian . $$ L(x, z, u: rho) = min frac{1}{2} left | Ax-b right |_{2}^{2} + lambda_{1} left | z right |_{1} + frac{ lambda_{2}}{2} left | z right |_{2}^{2} + frac{ rho}{2} left | x-z+ w right |_{2}^{2} + frac{ rho}{2} left | w right |_{2}^{2} $$ . $$ x_{k} = underset{x}{argmin} frac{1}{2} left | Ax-b right |_{2}^{2} + frac{ rho}{2} left | x-z_{k-1}+ w_{k-1} right |_{2}^{2} $$ . $$ x_k = ((A^TA+ rho I))^{-1}( rho(z_{k-1}-w_{k-1})+A^Tb) $$ . $$ z_{k} = underset{z}{argmin} ( lambda_{1} left | z right |_{1} + frac{ lambda_{2}}{2} left | z right |_{2}^{2}) + frac{ rho}{2} left | x_{k}-z+ w_{k-1} right |_{2}^{2} $$ . if $z&gt;0$ $$ z_{k} = frac{ rho(w_{k-1}+x_{k})}{ lambda_2- rho}- lambda_1$$ . if $z&lt;0$ $$ z_{k} = frac{ rho(w_{k-1}+x_{k})}{ lambda_2- rho}+ lambda_1$$ . $$ w_{k} = w_{k-1} + x_k -z_k $$ . Practical Application . Now, we will implement a regression algorithm using our augmented lagrangian. The dataset is the performance decay over time of a ship&#39;s Gas Turbine (GT) compressor. We split our test and train data 20:80. The range of decay of compressor has been sampled with a uniform grid of precision 0.001 so to have a good granularity of representation. For the compressor decay state discretization the kMc coefficient has been investigated in the domain [0.95,1]. Ship speed has been investigated sampling the range of feasible speed from 3 knots to 27 knots with a granularity of representation equal to tree knots. A series of measures (13 features) which indirectly represents of the state of the system subject to performance decay has been acquired and stored in the dataset over the parameter’s space. . The A 13-feature vector containing the GT measures at steady state of the physical asset: . Lever position (lp) | Ship speed (v) | Gas Turbine (GT) shaft torque (GTT) | GT rate of revolutions (GTn) | Gas Generator rate of revolutions (GGn) | Port Propeller Torque (Tp) | Hight Pressure (HP) Turbine exit temperature (T48) | GT Compressor outlet air temperature (T2) | HP Turbine exit pressure (P48) | GT Compressor outlet air pressure (P2) | GT exhaust gas pressure (Pexh) | Turbine Injection Control (TIC) | Fuel flow (mf) | GT Compressor decay state coefficient | . import pandas as pd import numpy as np import matplotlib.pyplot as plt %matplotlib inline from scipy.io import loadmat . ship_test = pd.read_csv(&quot;Shiptest-2.csv&quot;, header=None) ship_train = pd.read_csv(&quot;Shiptrain-2.csv&quot;, header=None) . X_train = ship_train.iloc[:,:-1] y_train = ship_train.iloc[:,-1:] X_test = ship_test.iloc[:,:-1] y_test = ship_test.iloc[:,-1:] . lambda_1 = 0.1 lambda_2 = 0.9 . iterations = 100 rho = 0.1 w = 0 z = np.random.rand(13).reshape(-1,1) A = X_train.values b = y_train.values loss = [] for i in range(iterations): x = (np.linalg.inv(np.dot(A.T,A) + (rho*np.eye(13)))).dot(rho*(z-w) + np.dot(A.T, b)) for i in range(len(z)): if np.sign(z[i])&gt;0: z = rho*(w+x)/(lambda_2-rho) - lambda_1 else: z = rho*(w+x)/(lambda_2-rho) + lambda_1 w = w + rho*(x-z) loss.append(np.sum(0.5*(np.dot(A,x)-b)**2)) . plt.figure(figsize=(10,5)) plt.plot(loss) plt.title(&#39;Obj function vs iterations&#39;) plt.xlabel(&#39;Number of iterations&#39;) plt.ylabel(&#39;Obj&#39;) . Text(0, 0.5, &#39;Obj&#39;) . print(&quot;Coefficients&quot;) x . Coefficients . array([[-5.05840315e-02], [ 1.03456745e-02], [ 1.21482112e-05], [-2.86479882e-04], [-1.49933306e-05], [-1.52302544e-03], [-9.13559793e-04], [ 1.36474482e-03], [ 1.85080531e-01], [ 5.00636758e-02], [ 6.72727845e-01], [-1.62063399e-04], [-1.76737055e-01]]) . Sum absolute errors . abs_ms_errors = [] abs_errors = [] for A, b in zip(X_test.values, y_test.values): abs_ms_errors.append(abs(np.sum(0.5*(np.dot(A,x)-b)**2))) abs_errors.append(abs(np.sum(np.dot(A,x)-b))) print(&quot;Sum absolute mean-squared errors:&quot;, round(sum(abs_ms_errors), 2)) print(&quot;Sum absolute errors:&quot;, round(sum(abs_errors), 2)) . Sum absolute mean-squared errors: 0.14 Sum absolute errors: 21.1 .",
            "url": "https://jackhmiller.github.io/My-DS-Blog/2021/08/30/GasTurbine_ADMM.html",
            "relUrl": "/2021/08/30/GasTurbine_ADMM.html",
            "date": " • Aug 30, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://jackhmiller.github.io/My-DS-Blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://jackhmiller.github.io/My-DS-Blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "My CV",
          "content": "Work Experience . Data Scientist at Sparks AB . Hod Hasharon, October 2019 – Current . ML &amp; Deep Learning for Algorithmic Trading | End-to-end data science pipelines for financial market predictions, including research, feature creation/selection, model training/optimization, and deployment architecture | Deep learning (RNN, CNN, transformers) &amp; machine learning models (gradient boosting) | Engineering using AWS Cloud suite, Jenkins, custom APIs to automate training and deployment | . Junior Data Scientist/ML Engineer at WeWork Technology . Tel Aviv, November January 2019 – October 2019 . Data Science &amp; Engineering | Automated end-to-end data pipeline engineering using AWS, Snowflake &amp; Apache Airflow | Reproducible DS environment engineering using Docker and Jupyter | Unstructured sentiment modeling and automation for global billing and payment services | Anomaly detection model for payment API monitoring &amp; security | . Senior Data Analyst at EY (Ernst &amp; Young) . Tel Aviv, November 2016 – December 2018 . Innovation and R&amp;D | Machine &amp; deep learning PoCs for Israeli and international consulting clients as solutions to predictive and analytics challenges | Sample of projects: financial anomaly detection for global bank, unsupervised learning model for drug discovery for pharma co., appraisal modeling for commercial real estate | . (Aliyah and army service October 2015 – November 2016) . Quant Analyst at McKinsey &amp; Co. . Washington D.C., January 2013-October 2015 . Quantitative Analytics and Modeling | Modeling for warfare simulations and predictions regarding autonomous robotics | Part of DoD-led study to determine the effectiveness of commercial &amp; defense machine learning technologies to solve defense needs | . Research Associate at Defense Advanced Research Projects Agency (DARPA) . Philadelphia, PA, September 2011- May 2014 . Competitive contingency estimation program | Heuristic modeling for event forecasting and classification | Sentiment analysis research for modeling of exogenous events &amp; systematic shocks | . Education . University of Pennsylvania, BA Statistics | 2010-2014 . Military Applications of Statistical Modeling | GPA: 3.9/4.0 Magna Cum Laude | . Georgia Tech, Msc Computer Science | 2020-Present . Computational Perception and Robotics | GPA: 3.8/4.0 | .",
          "url": "https://jackhmiller.github.io/My-DS-Blog/CV/",
          "relUrl": "/CV/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "About Me",
          "content": "I am a data scientist working and based in Israel, with an undergraduate degree in statistics from the University of Pennsylvania, and a masters degree in computer science Georgia Tech. . 050-709-2944 | Jack.harris.miller@gmail.com . .",
          "url": "https://jackhmiller.github.io/My-DS-Blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jackhmiller.github.io/My-DS-Blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}