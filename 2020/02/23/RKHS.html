<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>The Hypothesis Space and Representer Theorem | Not Another Data Science Blog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="The Hypothesis Space and Representer Theorem" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A theoretical discussion of learning theory, specifically the construction of a hypothesis space in ML/DL, and how it can be formulated as a Reproducing Kernel Hilbert Space." />
<meta property="og:description" content="A theoretical discussion of learning theory, specifically the construction of a hypothesis space in ML/DL, and how it can be formulated as a Reproducing Kernel Hilbert Space." />
<link rel="canonical" href="https://jackhmiller.github.io/My-DS-Blog/2020/02/23/RKHS.html" />
<meta property="og:url" content="https://jackhmiller.github.io/My-DS-Blog/2020/02/23/RKHS.html" />
<meta property="og:site_name" content="Not Another Data Science Blog" />
<meta property="og:image" content="https://jackhmiller.github.io/My-DS-Blog/images/RKHS.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-02-23T00:00:00-06:00" />
<script type="application/ld+json">
{"url":"https://jackhmiller.github.io/My-DS-Blog/2020/02/23/RKHS.html","@type":"BlogPosting","headline":"The Hypothesis Space and Representer Theorem","dateModified":"2020-02-23T00:00:00-06:00","datePublished":"2020-02-23T00:00:00-06:00","image":"https://jackhmiller.github.io/My-DS-Blog/images/RKHS.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://jackhmiller.github.io/My-DS-Blog/2020/02/23/RKHS.html"},"description":"A theoretical discussion of learning theory, specifically the construction of a hypothesis space in ML/DL, and how it can be formulated as a Reproducing Kernel Hilbert Space.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/My-DS-Blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://jackhmiller.github.io/My-DS-Blog/feed.xml" title="Not Another Data Science Blog" /><link rel="shortcut icon" type="image/x-icon" href="/My-DS-Blog/images/nn_JFc_icon.ico">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/My-DS-Blog/">Not Another Data Science Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/My-DS-Blog/CV/">My CV</a><a class="page-link" href="/My-DS-Blog/about/">About Me</a><a class="page-link" href="/My-DS-Blog/search/">Search</a><a class="page-link" href="/My-DS-Blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-02-23-RKHS.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Introduction">Introduction<a class="anchor-link" href="#Introduction"> </a></h2><p><br />
When confronted with a machine learning task, probably some of the first questions a data scientist will ponder is: 1) What is our instance space? What is our label space? How do we define success? Are there computational issues associated with the task/data? How can we achieve generalization without over-fitting? What kind of features are we going to use? After answering these questions, a data scientist or team will settle on a learning algorithm and an appropriate loss function/evaluation metric. Unfortunately, in my experience, what is lacking from the initial discussion regarding data science tasks is a a seemingly trivial discuss of the hypothesis space. Since it is more of a theoretical framework as opposed to a tangible manifestation of a learning algorithm or code base, it is not discussed or included in a project workplan. However, ultimately a thorough discussion of the hypothesis space and its constraints is a vital component in initially framing a modeling task, whether machine learning or deep learning, and driving a project towards successful and reproducible results. 
<br />
When learning a model $g(x)$, we must choose which find of function we expect $g(x)$ to be. For example, consider an unput with four binary features $(x = \left [ x_1x_2x_3x_4 \right ]; x \in \left \{ 0,1 \right \})$ and an unknown function $f(x)$ that returns y. For four features there are 16 possible instances. Thus in the binary classification task there are $2^{16}$ possible functions to describe our data. Therefore, we are confronted with two issues. First, without without restrictions on the set of functinos $g(x)$ learning is not feasible. Second, even if it were feasible, assuming there is a deterministic target hypothesis $g_0$ relating x to y, chosing a $g$ from a large enough space of functions, we certainly would achieve very good performance on training data. If fact, if we wanted to de could always make the training error exactly zero. A simple hypothesis would do just that: 
<br />
$$ g(x) = \left\{\begin{matrix}
+1 &amp; i \in\left \{ 1, 2, ...m \right \} x_i = x, y_i =1\\ 
-1 &amp; otherwise
\end{matrix}\right. $$
<br />
Obviously however this hypothesis would not generalize to data outside of our test set. Rather, we just created a model that simply memorizes labels. So all we ended up doing is minimizing the empirical error at the expense of the true error,
which is called overfitting. What a good model does is minimize the true error $\Re (g) = E_D\left [ L(g(x), y) \right ]$ where the expectation is taken over (x,y) pairs drawn from some distribution D.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-Formal-Model">The Formal Model<a class="anchor-link" href="#The-Formal-Model"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The above considerations give rise to the formal model of our learning prolem. Let $X$ be the input space, $y$ be the output space, D an unknown probability distribution on $Xxy$ and let $g$ our hypothesis space be a class of functions $g: X \to y $. For many practical algorithms, it is often observed that the true error is not too far from the empirical error. Real algorithms are not as ill behaved as the label memorization algorithm above, which was an extreme example. The key observation is the dicrepancy between the $R_D$ and $R_{emp}$ is related to the size of $g$, that is, how flexible our model is in terms of the size and shape of the hypothesis space. 
<br />
What made the label memorization algorithm so bad was that the class of possible hypotheses was so huge, permitting us to fit anything we wanted. That is an invitation for disastrous overfitting. Learning algorithms used in practice usually have access to a much more limited set of hyoptheses (such as linear discriminators in the case of the perceptron, SVM etc..), so they have less opportunity to overfit. One way to do this is by explicitly restricting the hypothesis space $g$ to “simple” hypotheses, as in Structural Risk Minimization.Another way is to introduce a penalty functional $\Omega $ that somehow measures the complexity of each hypothesis f, and to minimize the sum $$ R_reg(f) = R_{emp}(f) +  \Omega (f)$$.
<br />
We can now begin to construct our hypothesis class g. Naturally we want G to be a linear function space in te sense that for any $f \in G$ and any real number $\lambda$, $\lambda f$ is in G. Also, for any $f_1, f_2 \in g$ the sum of $f_1 + f_2$ is in G. We also want G to be related to te regularizer $\Omega$. We define a norm on g and set $\Omega\left [ f \right ] =  \left \| f \right \|^2$. We further require that the norm be derived from an inner product. 
<br />
This leads us to the notion of a Hilbert Space, or a linear inner product space, which will allow us to make the connection between the abstract structure of our hypothesis space g and what the elements of g actually are (discriminant functions or regressors). Within a Hilbert Space, for any such $ f \in g$ any x has a corresponding $f_x \in g$ such that x(f) = $\left \langle f_x, x \right \rangle$. Therefore, for any $x \in X$ we have a special function $k_x$ in our Hilbert Space called the representer and satisfying $f(x) = \left \langle k_x, f \right \rangle \forall f \in g$. Ultimately, we can rewrite our entire regularized risk minimization problem as: 
<br />

$$ \hat{f} = arg\underset{f \in g}{min}\left [ \frac{1}{m}\sum_{i=1}^{m}L(\left \langle k_x,f \right \rangle, y_i) + \left \langle f, f \right \rangle\right ] $$

<br />
The hypothesis f only features in this equation in the form of inner producs with other functions in hypothesis space g. Once we know the form of the inner product and what the $k_x$ are, we can do everything we want with simple math. Moreover, anything outside of the span of $\left \{ k_x \right \}x \in X$ is uninteresting since it does not affect what $f \in g$ evaluated at any point of the input space is, so we can leave it out of the hypothesis space altogether. The result of the whole construction is just driven by the inner product $k(x, x') = \left \langle k_x, k_{x'} \right \rangle$. This is called the kernel. 
<br />
In fact, we can reverse the whole procedure and construct g starting from the kernel. Given a positive definite function k on the input space X, we define g to be the minimal complete space of functions that includes all $\left \{ k_x \right \}x \in X$ and that has an inner product define in a fashion above. This defines g uniquely, and formaly g is called the Reproducing Kernel Hilbert Space associated with kernel k. Finally. we have reduced the learning problem to that of defining the loss function Representer L and the kernel k.
<br />
One more interesting point. By looking at our new formulation of our regularized risk minimization problem above, it is clear that $\hat{f}$ is going to be in the space of representers of the training data $k_1, k_2, k_3, ..., k_m$. We can tell because the loss term only depends on the inner products of f with $k_{x_1}, k_{x_2}, k_{x_3}, ..., k_{x_m}$ while the regularization term penalizes f in all directions. If f has any component orthogonal to the subspace spanned by $k_{x_1}, k_{x_2}, k_{x_3}, ..., k_{x_m}$, the loss term is not going to be sensitive to that component, but the regularization term will still penalize it. Hence ,the optimal f will be entirely contained in the subspace spanned by the representers. This is  the meaning of the <em>representer teorem</em> and it means that the optimal hypothesis $\hat{f}$ can be expressed as 

$$ \hat{f} = b + \sum_{i=1}^{m}\alpha_ik_{x_i} $$

for some real coefficients $\alpha_1, \alpha_2, \alpha_3...\alpha_m$ and bais b. Or as it is better known, 

$$ \hat{f} = b + \sum_{i=1}^{m}\alpha_ik(x_i, x) $$
</p>

</div>
</div>
</div>
</div>



      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/My-DS-Blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="https://jackhmiller.github.io/My-DS-Blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/My-DS-Blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>050-709-2944 | Jack.harris.miller@gmail.com</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li>
  <a rel="me" href="https://github.com/jackhmiller" target="_blank" title="github">
    <svg class="svg-icon grey">
      <use xlink:href="/My-DS-Blog/assets/minima-social-icons.svg#github"></use>
    </svg>
  </a>
</li>
</ul>
</div>

  </div>

</footer>
</body>

</html>
