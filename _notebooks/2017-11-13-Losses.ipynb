{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5103c729-af92-41a7-82c6-004665a0fcb9",
   "metadata": {
    "id": "d1a167fe-3ce7-40ea-9e72-1f4ef39ffd09",
    "tags": []
   },
   "source": [
    "# A Detailed Guide to 5 Loss Functions for Machine Learning Algorithms with Python Code\n",
    "> \"This post covers multiple loss functions, where they work, and how you can code them in Python\"\n",
    "- toc: false\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- image: images/lf.png\n",
    "- hide: false\n",
    "- search_exclude: false"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4db65c-e840-43ba-8e53-adfeef28e5d4",
   "metadata": {},
   "source": [
    "In supervised machine learning algorithms, we want to minimize the error for each training example during the learning process. This is done using some optimization strategies like gradient descent. And this error comes from the loss function. Similat to a loss function, we also use a cost function. However, although cost function and loss function are synonymous and used interchangeably, they are different. A loss function is for a single training example. It is also sometimes called an error function. A cost function, on the other hand, is the average loss over the entire training dataset. The optimization strategies aim at minimizing the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e951f9a6-3c65-4da5-838a-7a44f61149aa",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94de1c4d-b795-4454-8567-5e6e8979415c",
   "metadata": {},
   "source": [
    "## Regression Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c81e6f5-52d7-432b-a665-f04b3b66f62f",
   "metadata": {},
   "source": [
    "You must be quite familiar with linear regression at this point. It deals with modeling a linear relationship between a dependent variable, Y, and several independent variables, X_i’s. Thus, we essentially fit a line in space on these variables $Y = a0 + a1 * X1 + a2 * X2 + ....+ an * Xn$. We will use the given data points to find the coefficients a0, a1, …, an."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1c0a5e-6726-4017-a386-edb5c88a6491",
   "metadata": {},
   "source": [
    "We will use the famous Boston Housing Dataset for understanding this concept. And to keep things simple, we will use only one feature – the Average number of rooms per dwelling (X) – to predict the dependent variable – Median Value (Y) of houses in USD 1000′ s. We will use Gradient Descent as an optimization strategy to find the regression line. Just as a refresher, the gradient descent update is as follows:\n",
    "$$\n",
    "\\text{Repeat until convergence}\\left\\{ \\theta_j \\gets \\theta_j - \\alpha \\frac{\\partial }{\\partial \\theta_j}J(\\theta) \\right\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78975f94-5da8-48a9-869b-929c36bf3d53",
   "metadata": {},
   "source": [
    "Here, $\\theta_j$ is the weight to be updated, alpha is the learning rate and J is the cost function. The cost function is parameterized by theta. Our aim is to find the value of theta which yields minimum overall cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1eb611-8289-4014-b701-89ed4a1adb80",
   "metadata": {},
   "source": [
    "For each loss function we will follow the following steps:\n",
    "1. Write the expression for our predictor function, f(X) and identify the parameters that we need to find\n",
    "2. Identify the loss to use for each training example\n",
    "3. Find the expression for the Cost Function – the average loss on all examples\n",
    "4. Find the gradient of the Cost Function with respect to each unknown parameter\n",
    "5. Decide on the learning rate and run the weight update rule for a fixed number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe15f420-85ec-4407-84bf-b193c2689e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv('housing.csv')\n",
    "X = df['RM'].values\n",
    "y = df['MEDV'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832d03f6-3239-447b-ae7c-5570dd34655f",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8d183a-1610-46f2-86e7-049c56b47fed",
   "metadata": {},
   "source": [
    "#### 1. Squared Error Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7431350-18a1-41ec-abf3-def087bdbcfe",
   "metadata": {},
   "source": [
    "Squared Error loss for each training example, also known as L2 Loss, is the square of the difference between the actual and the predicted values: $ L = (y-f(x))^2$. The corresponding cost function is the Mean of these Squared Errors (MSE).  It is a positive quadratic function (of the form ax^2 + bx + c where a > 0). A quadratic function only has a global minimum. Since there are no local minima, we will never get stuck in one. Hence, it is always guaranteed that Gradient Descent will converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "efeb2126-924b-4171-93c9-aaf7d27465ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss_update(m, b, X, y, learning_rate):\n",
    "    # intitialize at zero\n",
    "    m_deriv = 0\n",
    "    b_deriv = 0\n",
    "    N = len(X)\n",
    "    \n",
    "    for i in range(N):\n",
    "        # calculate partial derivatives for loss function (y-mx-b)^2\n",
    "        # -2x(y-mx-b)\n",
    "        m_deriv = -2*X[i] * (y[i]-(m*X[i])-b)\n",
    "        \n",
    "        # -2(y-mx-b)\n",
    "        b_deriv = -2 * (y[i] - m*X[i]-b)\n",
    "        \n",
    "        m -= (m_deriv/float(N)) * learning_rate\n",
    "        b -= (b_deriv/float(N)) * learning_rate\n",
    "        \n",
    "    return m, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a96da39b-ccc2-4527-b794-e53cbe449467",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE = []\n",
    "m = 0.001\n",
    "b = 0\n",
    "N = len(X)\n",
    "lr = 0.0001\n",
    "for _ in range(0, 500):\n",
    "    loss = 0\n",
    "    for i in range(0, N):\n",
    "        loss += (y[i] - m*X[i] - b)**2\n",
    "    loss /= float(N)\n",
    "    MSE.append(loss)\n",
    "    m, b = squared_loss_update(m, b, X, y, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59c4bb8d-56db-472f-ae90-217f6704b0ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2cc24c9f730>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = range(500)\n",
    "plt.plot(index, MSE, label = 'alpha = 0.0001')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b14f401-fc10-4a39-8f8a-7548984fbab3",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48f92e9-585d-4162-a0be-029de099e700",
   "metadata": {},
   "source": [
    "#### 2. Mean Absolute Error Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f27b7c-2114-4642-b1cb-843093c1a9f0",
   "metadata": {},
   "source": [
    "Absolute Error for each training example is the distance between the predicted and the actual values, irrespective of the sign. Absolute Error is also known as the L1 loss: $L = \\left| y-f(x) \\right|$. The cost is the Mean of these Absolute Errors (MAE), and it is more robust to outliers than MSE. Lets code the weights update for MAE. Recall, for the purposes of calculating the partial derivatives that $\\frac{\\partial }{\\partial x_j}(x\\to \\left\\| x_1 \\right\\|) = \\frac{x_j}{\\left| x_j \\right|}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "05c8e80b-ec01-4093-80d7-a581a8d59635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAE_loss_update(m, b, X, y, learning_rate):\n",
    "    m_deriv = 0\n",
    "    b_deriv = 0\n",
    "    N = len(X)\n",
    "    \n",
    "    for i in range(N):\n",
    "        # calculate partial derivatives\n",
    "        # -x(y-mx-b) / |mx +b|\n",
    "        m_deriv += (-X[i] * (y[i] - m*X[i] -b)) / (abs(y[i] - m*X[i]-b))\n",
    "        \n",
    "        # -(y-mx-b) / |mx +b|\n",
    "        b_deriv += -(y[i] - m*X[i] -b) / (abs(y[i] - m*X[i]-b))\n",
    "    \n",
    "    m -= (m_deriv / float(N)) * learning_rate\n",
    "    b -= (b_deriv / float(N)) * learning_rate\n",
    "\n",
    "    return m, b   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d523a029-082a-4c28-9c49-02284099730b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE = []\n",
    "m = 0.0001\n",
    "b = 0\n",
    "lr = 0.01\n",
    "for _ in range(0, 500):\n",
    "    loss = 0\n",
    "    for i in range(0, N):\n",
    "        loss += abs(y[i] -m*X[i] -b)\n",
    "    loss /= float(N)\n",
    "    MAE.append(loss)\n",
    "    m, b = MAE_loss_update(m, b, X, y, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18aa6fb-4983-491f-97ca-1170042ad359",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0309371-4b75-4923-8786-c8af8c5f4b32",
   "metadata": {},
   "source": [
    "#### 2. Mean Absolute Error Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cb32a9-75de-4fc1-ba98-c6de8bf26bae",
   "metadata": {},
   "source": [
    "The Huber loss combines the best properties of MSE and MAE. It is quadratic for smaller errors and is linear otherwise (and similarly for its gradient). It is identified by its delta parameter:\n",
    "$$\n",
    "L_\\delta = \\left\\{\\begin{matrix}\n",
    "frac{1}{2}(y-f(x))^2,  & if \\left | y-f(x) \\right | \\leq \\delta\\\\ \n",
    "\\delta\\left | y-f(x) \\right | -\\frac{1}{2}\\delta^2 & otherwise \n",
    "\\end{matrix}\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ba2df132-143c-434d-8419-4aacda531f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Huber_update(m, b, X, Y, delta, learning_rate):\n",
    "    m_deriv = 0\n",
    "    b_deriv = 0\n",
    "    N = len(X)\n",
    "    for i in range(N):\n",
    "        # derivative of quadratic for small values and of linear for large values\n",
    "        if abs(Y[i] - m*X[i] - b) <= delta:\n",
    "            m_deriv += -X[i] * (Y[i] - (m*X[i] + b))\n",
    "            b_deriv += - (Y[i] - (m*X[i] + b))\n",
    "        else:\n",
    "            m_deriv += delta * X[i] * ((m*X[i] + b) - Y[i]) / abs((m*X[i] + b) - Y[i])\n",
    "            b_deriv += delta * ((m*X[i] + b) - Y[i]) / abs((m*X[i] + b) - Y[i])\n",
    "    \n",
    "    # We subtract because the derivatives point in direction of steepest ascent\n",
    "    m -= (m_deriv / float(N)) * learning_rate\n",
    "    b -= (b_deriv / float(N)) * learning_rate\n",
    "\n",
    "    return m, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "74e1c1aa-e9e4-4efe-b00b-ba7eab0eb6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Huberloss = []\n",
    "m = 0.001\n",
    "b = 0\n",
    "learning_rate = 0.0001\n",
    "delta = 20\n",
    "for iter in range(0, 500):\n",
    "    loss = 0\n",
    "    for i in range(0, N):\n",
    "        if abs(y[i] - m*X[i] - b) <= delta:\n",
    "            loss += ((y[i] - m * X[i] - b) ** 2) / 2\n",
    "        else:\n",
    "            loss += delta * abs(y[i] - m * X[i] - b) - (delta ** 2) / N\n",
    "        \n",
    "    loss /= float(N)\n",
    "    Huberloss.append(loss)\n",
    "    m, b = Huber_update(m, b, X, y, delta, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35366a8-82d6-4886-889f-da7f4c2c15c0",
   "metadata": {},
   "source": [
    "## Binary Classification Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfdd43e-ed2c-4f72-b21d-937a6c967d68",
   "metadata": {},
   "source": [
    "Binary Classification refers to assigning an object into one of two classes. This classification is based on a rule applied to the input feature vector. For example, classifying an email as spam or not spam based on, say its subject line, is binary classification. Lets illustrate these binary classification loss functions on the Breast Cancer dataset. A greater value of entropy for a probability distribution indicates a greater uncertainty in the distribution. Likewise, a smaller value indicates a more certain distribution. This makes binary cross-entropy suitable as a loss function – you want to minimize its value. We use binary cross-entropy loss for classification models which output a probability p. Then, the cross-entropy loss for output label y (can take values 0 and 1) and predicted probability p is defined as:\n",
    "$$\n",
    "L = -y*log(p) - (1-y)*log(1-p) = \\left\\{\\begin{matrix}\n",
    "-log(1-p) & y=0\\\\ \n",
    "-log(p) &  y=1\n",
    "\\end{matrix}\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d9c2e3-5d79-4616-96a9-add0829ae7cc",
   "metadata": {},
   "source": [
    "This is also called Log-Loss. To calculate the probability p, we can use the sigmoid function. Here, z is a function of our input features: $S(z) = \\frac{1}{1+e^{-z}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b7ee40de-6c17-4487-8dbc-9f3d7204ac41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "cancer_dataset = load_breast_cancer()\n",
    "data1 = pd.DataFrame(cancer_dataset.data, columns=cancer_dataset.feature_names)\n",
    "data1['Class'] = cancer_dataset.target\n",
    "X1 = data1['worst area'].values.reshape(-1, 1)\n",
    "X2 = data1['mean symmetry'].values.reshape(-1, 1)\n",
    "Y = data1['Class'].values\n",
    "N1 = len(X1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X1 = scaler.fit_transform(X1)\n",
    "X2 = scaler.fit_transform(X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85de5530-9b70-4364-ada5-1a1c57a2bb78",
   "metadata": {},
   "source": [
    "#### 1. Binary Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "92e7f8da-113e-4b80-8f20-cc401549ac96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BCE_update(m1, m2, x1, x2, y, learning_rate):\n",
    "    m1_deriv = 0\n",
    "    m2_deriv = 0\n",
    "    b_deriv = 0\n",
    "    \n",
    "    N = len(X1)\n",
    "    for i in range(0, N):\n",
    "        m1_deriv += -X1[i] * (s - Y[i])\n",
    "        m2_deriv += -X2[i] * (s - Y[i])\n",
    "        b_deriv += -(s - Y[i])\n",
    "    \n",
    "    m1 -= (m1_deriv/float(n)) * learning_rate\n",
    "    m2 -= (m2_deriv/float(n)) * learning_rate\n",
    "    b -= (b_deriv/float(n)) * learning_rate\n",
    "    \n",
    "    return m1, m2, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e83589-1b53-4619-b589-1e7cef03c0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BCE = []\n",
    "m1 = 0\n",
    "m2 = 0\n",
    "b = 0\n",
    "lr = 0.0001\n",
    "for _ in range(500):\n",
    "    loss = 0\n",
    "    for i in range(0, N1):\n",
    "        p = 1/(1 + math.exp(-m1*X1[i] - m2*X2[i] - b))\n",
    "        if Y[i] == 0:\n",
    "            loss += -math.log(1-p)\n",
    "        else:\n",
    "            loss += -math.log(p)\n",
    "    loss /= float(N1)\n",
    "    BCE.append(loss)\n",
    "    m1, m2, b = BCE_update(m1, m2, b, X1, X2, Y, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a319654-0153-4873-8c82-75a029c8430c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2. Hinge Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9511377-4090-4deb-8e43-07e94246551c",
   "metadata": {},
   "source": [
    "Hinge loss is primarily used with Support Vector Machine (SVM) Classifiers with class labels -1 and 1. So make sure you change the label of the ‘Malignant’ class in the dataset from 0 to -1. Hinge Loss not only penalizes the wrong predictions but also the right predictions that are not confident. Hinge loss for an input-output pair (x, y) is given as:\n",
    "$$\n",
    "L = max(0, 1-y*f(x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ff9d5cde-72cd-451e-9ade-fe9284dadd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hinge_update(m1, m2, b, X1, X2, y, learning_rate):\n",
    "    m1_deriv = 0\n",
    "    m2_deriv = 0\n",
    "    b_deriv = 0\n",
    "    N = len(X1)\n",
    "    \n",
    "    for i in range(0, N):\n",
    "        if Y[i]*(m1*X1[i] + m2*X2[i] + b) <= 1:\n",
    "            m1_deriv += -X1[i] * Y[i]\n",
    "            m2_deriv += -X2[i] * Y[i]\n",
    "            b_deriv += -Y[i]\n",
    "    \n",
    "    m1 -= (m1_deriv / float(N)) * learning_rate\n",
    "    m2 -= (m2_deriv / float(N)) * learning_rate\n",
    "    b -= (b_deriv / float(N)) * learning_rate\n",
    "\n",
    "    return m1, m2, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b8dff9-1922-4b25-89c1-5e1505042fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hinge = []\n",
    "m1 = 0\n",
    "m2 = 0\n",
    "b = 0\n",
    "learning_rate = 0.001\n",
    "for _ in range(0, 1000):\n",
    "    loss = 0\n",
    "    for i in range(0, N1):\n",
    "        if Y[i]*(m1*X1[i] + m2*X[i] + b) <= 1:\n",
    "            loss += 1 - Y[i]*(m1*X1[i] + m2*X[i] + b)\n",
    "    loss /= float(N1)\n",
    "    Hinge.append(loss)\n",
    "    m1, m2, b = Hinge_update(m1, m2, b, X1, X2, y, learning_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
