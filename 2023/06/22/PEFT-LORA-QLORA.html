<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>PEFT with LoRA and QLoRA | Concept Drift</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="PEFT with LoRA and QLoRA" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Large Language Model (LLM) fine-tuning with low-rank matrix approximations and data-type quantization." />
<meta property="og:description" content="Large Language Model (LLM) fine-tuning with low-rank matrix approximations and data-type quantization." />
<link rel="canonical" href="https://jackhmiller.github.io/My-DS-Blog/2023/06/22/PEFT-LORA-QLORA.html" />
<meta property="og:url" content="https://jackhmiller.github.io/My-DS-Blog/2023/06/22/PEFT-LORA-QLORA.html" />
<meta property="og:site_name" content="Concept Drift" />
<meta property="og:image" content="https://jackhmiller.github.io/My-DS-Blog/images/LoRA.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-06-22T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"https://jackhmiller.github.io/My-DS-Blog/2023/06/22/PEFT-LORA-QLORA.html","@type":"BlogPosting","headline":"PEFT with LoRA and QLoRA","dateModified":"2023-06-22T00:00:00-05:00","datePublished":"2023-06-22T00:00:00-05:00","image":"https://jackhmiller.github.io/My-DS-Blog/images/LoRA.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://jackhmiller.github.io/My-DS-Blog/2023/06/22/PEFT-LORA-QLORA.html"},"description":"Large Language Model (LLM) fine-tuning with low-rank matrix approximations and data-type quantization.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/My-DS-Blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://jackhmiller.github.io/My-DS-Blog/feed.xml" title="Concept Drift" /><link rel="shortcut icon" type="image/x-icon" href="/My-DS-Blog/images/nn_JFc_icon.ico">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/My-DS-Blog/">Concept Drift</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/My-DS-Blog/CV/">My CV</a><a class="page-link" href="/My-DS-Blog/about/">About Me</a><a class="page-link" href="/My-DS-Blog/search/">Search</a><a class="page-link" href="/My-DS-Blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">PEFT with LoRA and QLoRA</h1><p class="page-description">Large Language Model (LLM) fine-tuning with low-rank matrix approximations and data-type quantization.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2023-06-22T00:00:00-05:00" itemprop="datePublished">
        Jun 22, 2023
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      8 min read
    
</span></p>

    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h1 id="introduction">Introduction</h1>
<p>Since the inception of transfer learning, dozens of works have sought to make model adaptation more parameter- and compute-efficient. Yet it is still the case that one of main drawbacks for LLMs is that have to be fine-tuned for <em>each</em> downstream task, learning a different set of parameters.</p>

<blockquote>
  <p>[!Question]
Why can we use relatively vanilla gradient descent algorithms (e.g., without strong regularization) to tune a model with hundreds of millions of parameters on datasets with only hundreds or thousands of labeled examples?</p>
</blockquote>

<h1 id="peft-parameter-efficient-fine-tuning">PEFT: Parameter Efficient Fine Tuning</h1>

<h2 id="lora">LoRA</h2>
<p><strong>Low-Rank Adaptation, or LoRA</strong><sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>- which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. The most significant benefit comes from the reduction in memory and storage usage.</p>

<h3 id="refresher-on-matrix-decomposition">Refresher on Matrix Decomposition</h3>
<p>SVD representation of a matrix is the singular value decomposition of any matrix A so that $A=USV^T$.</p>
<ul>
  <li>$U$ is the left singular vectors obtained by finding an orthonormal set of eigenvectors $A^TA$.</li>
  <li>$S$ is a diagonal matrix of singular values, which are the square roots of the eigenvalues of $AA^T$</li>
  <li>V is the right singular vectors obtained by finding an orthonormal set of eigenvectors t$A^TA$</li>
</ul>

<p>You can truncate the SVD of a higher-rank matrix $A$ to get the a low-rank approximation. This is done by setting all but the first $k$ largest singular values to zero, and using the first $k$ rows and columns of $U$ and $V$. The rank is the $k$ chosen. This works because singular values decrease exponentially with rank, with earlier singular values being much larger than later ones.</p>

<h3 id="low-rank-parameterized-update-matrices">Low-Rank Parameterized Update Matrices</h3>
<p>A neural network contains many dense layers which perform matrix multiplication. The weight matrices in these layers typically have full-rank. When adapting to a specific task, Aghajanyan et al. (2020)<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup> shows that the pre-trained language models have a low “instrisic dimension” and can still learn efficiently despite a random projection to a smaller subspace. In other words, there exists a low dimension reparameterization that is as effective for fine-tuning as the full parameter space.</p>

<blockquote>
  <p>[!note]
An objective function’s intrinsic dimensionality describes the minimum dimension needed to solve the optimization problem it defines to some precision level. In the context of pretrained language models, measuring intrinsic dimensional will tell us how many free parameters are required to closely approximate the optimization problem that is solved while fine-tuning for each end task.</p>
</blockquote>

<p>Inspired by this, the LoRA authors hypothesize the updates to the weights also have a low “intrinsic rank” during adaptation.</p>

<p>For a pretrained weight matrix $W_{0}\in \mathbb{R}^{d \times k}$, the update is constrained by representing its with a low-rank decomposition $W_{0}+ \bigtriangleup W = W_{0}+ BA$ where $B \in \mathbb{R}^{d \times r}$, $A \in \mathbb{R}^{r \times k}$, and the rank $r « min(d, k)$. During training, $W_0$ is frozen and does not receive gradient updates, while $A$ and $B$ are trainable parameters. So the forward pass yields:</p>

<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mo>=</mo><msub><mi>W</mi><mrow><mn>0</mn><mi>x</mi></mrow></msub><mo>+</mo><mo>▽</mo><mi>W</mi><mi>x</mi><mo>=</mo><msub><mi>W</mi><mrow><mn>0</mn><mi>x</mi></mrow></msub><mo>+</mo><mi>B</mi><mi>A</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">h = W_{0x}+ \bigtriangledown Wx = W_{0x}+ BAx</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">h</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mord mathdefault mtight">x</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord">▽</span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mord mathdefault mtight">x</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mord mathdefault">A</span><span class="mord mathdefault">x</span></span></span></span></span>

<p>In other words, gradients during stochastic gradient descent are passed through the fixed pretrained model weights to the adapter, which is updated to optimize the loss function. LoRA augments a linear projection through an additional factorized projection.</p>

<p>Using the technique shown above, <strong><em>r(n + k)</em></strong> parameters have to be tuned during model adaption. Since <strong><em>r « min(n, k)</em></strong>, this is much lesser than the number of parameters that would have to be tuned otherwise (<strong><em>nk</em></strong>). This reduces the time and space required to finetune the model by a large margin. Some numbers from the paper and our experiments are discussed in the sections below.</p>

<p>To take an extreme example, supposed the $W_O$ is of size 512x512. That is 512$^2$ parameters. On the other hand, using two matrices via LoRA to replace $W_O$ where $L_{1}\in \mathbb{R}^{512 \times 1}$ and $L_{2}\in \mathbb{R}^{1 \times 512}$, that is only 1024 total parameters.</p>

<h3 id="simple-lora-implementation">Simple LoRA Implementation</h3>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">os</span>
<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">"CUDA_VISIBLE_DEVICES"</span><span class="p">]</span><span class="o">=</span><span class="s">"0"</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">bitsandbytes</span> <span class="k">as</span> <span class="n">bnb</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s">"bigscience/bloom-3b"</span><span class="p">,</span> 
    <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float16</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="s">'auto'</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s">"bigscience/tokenizer"</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">():</span>
  <span class="n">param</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">False</span>  <span class="c1"># freeze the model - train adapters later
</span>  <span class="k">if</span> <span class="n">param</span><span class="p">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="c1"># cast the small parameters (e.g. layernorm) to fp32 for stability
</span>    <span class="n">param</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">param</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">model</span><span class="p">.</span><span class="n">gradient_checkpointing_enable</span><span class="p">()</span>  <span class="c1"># reduce number of stored activations
</span><span class="n">model</span><span class="p">.</span><span class="n">enable_input_require_grads</span><span class="p">()</span>

<span class="k">class</span> <span class="nc">CastOutputToFloat</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="nb">super</span><span class="p">().</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">CastOutputToFloat</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">lm_head</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">print_trainable_parameters</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="s">"""
    Prints the number of trainable parameters in the model.
    """</span>
    <span class="n">trainable_params</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">all_param</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="n">named_parameters</span><span class="p">():</span>
        <span class="n">all_param</span> <span class="o">+=</span> <span class="n">param</span><span class="p">.</span><span class="n">numel</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">param</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="n">trainable_params</span> <span class="o">+=</span> <span class="n">param</span><span class="p">.</span><span class="n">numel</span><span class="p">()</span>
    <span class="k">print</span><span class="p">(</span>
        <span class="s">f"trainable params: </span><span class="si">{</span><span class="n">trainable_params</span><span class="si">}</span><span class="s"> || all params: </span><span class="si">{</span><span class="n">all_param</span><span class="si">}</span><span class="s"> || trainable%: </span><span class="si">{</span><span class="mi">100</span> <span class="o">*</span> <span class="n">trainable_params</span> <span class="o">/</span> <span class="n">all_param</span><span class="si">}</span><span class="s">"</span>
    <span class="p">)</span>

<span class="c1"># Obtain LoRA Model
</span><span class="kn">from</span> <span class="nn">peft</span> <span class="kn">import</span> <span class="n">LoraConfig</span><span class="p">,</span> <span class="n">get_peft_model</span> 

<span class="n">config</span> <span class="o">=</span> <span class="n">LoraConfig</span><span class="p">(</span>
    <span class="n">r</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">lora_alpha</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">target_modules</span><span class="o">=</span><span class="p">[</span><span class="s">"query_key_value"</span><span class="p">],</span>
    <span class="n">lora_dropout</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
    <span class="n">bias</span><span class="o">=</span><span class="s">"none"</span><span class="p">,</span>
    <span class="n">task_type</span><span class="o">=</span><span class="s">"CAUSAL_LM"</span>
<span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">get_peft_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>
<span class="n">print_trainable_parameters</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="c1"># Load sample data
</span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="n">qa_dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s">"squad_v2"</span><span class="p">)</span>

<span class="c1"># Train LoRA
</span><span class="kn">import</span> <span class="nn">transformers</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">transformers</span><span class="p">.</span><span class="n">Trainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> 
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">mapped_qa_dataset</span><span class="p">[</span><span class="s">"train"</span><span class="p">],</span>
    <span class="n">args</span><span class="o">=</span><span class="n">transformers</span><span class="p">.</span><span class="n">TrainingArguments</span><span class="p">(</span>
        <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> 
        <span class="n">gradient_accumulation_steps</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
        <span class="n">warmup_steps</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
        <span class="n">max_steps</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> 
        <span class="n">fp16</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="n">logging_steps</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
        <span class="n">output_dir</span><span class="o">=</span><span class="s">'outputs'</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">data_collator</span><span class="o">=</span><span class="n">transformers</span><span class="p">.</span><span class="n">DataCollatorForLanguageModeling</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">mlm</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">use_cache</span> <span class="o">=</span> <span class="bp">False</span>  <span class="c1"># silence the warnings. Please re-enable for inference!
</span><span class="n">trainer</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
</code></pre></div></div>
<p><sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">3</a></sup></p>

<h3 id="where-lora-falls-short">Where LoRA Falls Short</h3>
<p>While LoRA was designed as a PEFT method, most of the memory footprint for LLM finetuning comes from activation gradients and not from the learned LoRA parameters. For example, for a 7B LLaMA model trained on FLAN v2 with a batch size of 1, with LoRA weights equivalent to commonly used 0.2% of the original model weights, the LoRA input gradients have a memory footprint of 567 MB while the LoRA parameters take up only 26 MB.</p>

<h2 id="qlora--efficient-finetuning-of-quantized-llms">QLoRA- Efficient Finetuning of Quantized LLMs</h2>
<p>QLoRA reduces the average memory needs of finetuning a 65B parameter model from &gt;780GB of GPU RAM to 48GB without sacrificing runtime or predictive performance.<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">4</a></sup> This is done via an algorithm developed by researchers at the University of Washington that quantizes a pretrained model using to a 4-bit resolution before adding a sparse set of learnable Low-rank Adapter weights modified by backpropagating gradients through the quantized consequences. As a result, QLoRA  has made the largest publicly available models to date fine-tunable on a single GPU.</p>

<h4 id="what-is-quantization">What is Quantization?</h4>
<p>Quantization is the process of discretizing an input from a representation that holds more information to a representation with less information. It often means taking a data type with more bits and converting it to fewer bits, for example from 32-bit floats to 8-bit Integers. To ensure that the entire range of the low-bit data type is used, the input data type is commonly rescaled into the target data type range through normalization by the absolute maximum of the input elements, which are usually structured as a tensor.</p>

<blockquote>
  <p>This is important since to calculate the model size in bytes, one multiplies the number of parameters by the size of the chosen precision in bytes.</p>
</blockquote>

<p><strong>Example 1</strong>
Let say we want to go from <code class="language-plaintext highlighter-rouge">float32</code> to <code class="language-plaintext highlighter-rouge">int8</code>.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array = [-1024, 5, 2048, 256] # as type float32
</code></pre></div></div>

<p>To do the conversion, we will multiply each element in the array by a quantization factor <code class="language-plaintext highlighter-rouge">c</code>. To calculate <code class="language-plaintext highlighter-rouge">c</code> , we first find the max bit number for the desired conversion type (so in this case for <code class="language-plaintext highlighter-rouge">int8</code> it is 127 since the range of <code class="language-plaintext highlighter-rouge">int8</code> is <code class="language-plaintext highlighter-rouge">[-127, 127]</code>), and then divide it by the max number of the array.</p>

<p>Then, for each item in our array, we divide the item by the quantization factor and round, thus achieving quantization albeit with an expected loss of precision.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt;&gt;&gt; c = 127/max(array)
&gt;&gt;&gt; c
0.62

&gt;&gt;&gt; quantized = [round(i/c) for i in array]
&gt;&gt;&gt; quantized
[-64, 0, 127, 16]
</code></pre></div></div>

<p><strong>Example 2</strong> - why is this problematic
Using the same conversion instead for <code class="language-plaintext highlighter-rouge">[0, 100, 100000]</code>, you will get 0 for the first two elements of the array, leading to a lot of information loss. The problem with this approach is that if a large magnitude value (i.e., an outlier) occurs in the input tensor, then the quantization bins—certain bit combinations—are not utilized well with few or no numbers quantized in some bins</p>

<h3 id="qloras-solution">QLoRA’s Solution</h3>
<p>QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimizers to manage memory spikes.
 
4-bit NormalFloat quantization improves upon quantile quantization by ensuring an equal number of values in each quantization bin. This avoids computational issues and errors for outlier values. During finetuning, QLoRA backpropagates gradients through the frozen 4-bit quantized pretrained language model into the Low-Rank Adapters. QLoRA dequantizes weights from the 4-bit NormalFloat (what the authors call storage data type) to the 16-bit BrainFloat (computation data type) to perform the forward and backward passes, but only computes weight gradients for the LoRA parameters which use 16-bit BrainFloat. The weights are decompressed only when they are needed, therefore the memory usage stays low during training and inference.</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, &amp; Weizhu Chen (2021). LoRA: Low-Rank Adaptation of Large Language Models_. CoRR, <em>abs/2106.09685</em>. <a href="https://arxiv.org/pdf/2106.09685.pdf">https://arxiv.org/pdf/2106.09685.pdf</a> <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>Armen Aghajanyan, Luke Zettlemoyer, &amp; Sonal Gupta (2020). Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning_. CoRR, <em>abs/2012.13255</em>, <a href="https://arxiv.org/abs/2012.13255.">https://arxiv.org/abs/2012.13255.</a> <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>https://colab.research.google.com/drive/1iERDk94Jp0UErsPf7vXyPKeiM4ZJUQ-a?usp=sharing#scrollTo=kfAO01v-qEPS <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, &amp; Luke Zettlemoyer. (2023). QLoRA: Efficient Finetuning of Quantized LLMs. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

  </div><a class="u-url" href="/My-DS-Blog/2023/06/22/PEFT-LORA-QLORA.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/My-DS-Blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="https://jackhmiller.github.io/My-DS-Blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/My-DS-Blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>050-709-2944 | Jack.harris.miller@gmail.com</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li>
  <a rel="me" href="https://github.com/jackhmiller" target="_blank" title="github">
    <svg class="svg-icon grey">
      <use xlink:href="/My-DS-Blog/assets/minima-social-icons.svg#github"></use>
    </svg>
  </a>
</li>
</ul>
</div>

  </div>

</footer>
</body>

</html>
