{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "2020-08-05-PGD.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "797a45b6-fca9-49b3-bcd6-f3b20ea0f367"
      },
      "source": [
        "# Proving Proximal Gradient Method's Convergence Rate and a Code Demonstration\n",
        "> \"A rigorous proof of the convergence rate of PGM, and code implementation from scratch using MNIST data\"\n",
        "- toc: false\n",
        "- branch: master\n",
        "- badges: true\n",
        "- comments: true\n",
        "- image: images/PGD.png\n",
        "- hide: false\n",
        "- search_exclude: false"
      ],
      "id": "797a45b6-fca9-49b3-bcd6-f3b20ea0f367"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "632dc8e3-0a7b-4055-843f-f907e21980a9"
      },
      "source": [
        "#### Proximal Gradient Descent"
      ],
      "id": "632dc8e3-0a7b-4055-843f-f907e21980a9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ddcca9c-66ab-4097-bace-e0f323de6139"
      },
      "source": [
        "A proximal algorithm is an algorithm for solving a convex optimization problem that uses the proximal operators of the objective terms. It is called such since it consists of a gradient step followed by a proximal mapping.  There are three main benefits to the application of proximal algorithms:\n",
        "- 1. They work under extremely general conditions, including cases where the functions are nonsmooth and extended real-valued\n",
        "- 2. They can be fast, since there can be simple proximal operators for functions that are otherwise challenging to handle in an optimization problem\n",
        "- 3. They are amenable to distributed optimization, so they can be used to solve very large scale problems"
      ],
      "id": "7ddcca9c-66ab-4097-bace-e0f323de6139"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e3fbbd0-e7f8-4d52-83c4-194ae784e249"
      },
      "source": [
        "The proximal operator is defined as $$ prox_f(x) = argmin\\left \\{ f(u) + \\frac{1}{2}\\left \\| u-x \\right \\|^2: u \\in \\mathbb{R}^n \\right \\}, \\forall x \\in \\mathbb{R}^n $$\n",
        "with the goal being to $$minimize\\left \\{ f(u) + h(u): u \\in \\mathbb{R}^n \\right \\}$$\n",
        "where h is a proper lower semi-continuous function and f is a smooth convex function on dom(h). \n",
        "<br>"
      ],
      "id": "0e3fbbd0-e7f8-4d52-83c4-194ae784e249"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "014a02cc-2ae0-4460-b262-5fc800cdb357"
      },
      "source": [
        "__Some important assumptions before we begin:__"
      ],
      "id": "014a02cc-2ae0-4460-b262-5fc800cdb357"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "766b1dcf-1928-4c89-9bc6-9e14890275cf"
      },
      "source": [
        "We assume that f has L-Lipschitz continuous gradient, i.e., $$\\left \\| \\bigtriangledown f(x) - \\bigtriangledown f(y) \\right \\| \\leq L\\left \\| x-y \\right \\|, \\forall x, y \\in dom(h)$$\n",
        "and hence for every $x, y \\in dom(h)$, $$ f(x) \\leq l_f(x; y) + \\frac{L}{2}\\left \\| x-y \\right \\|^2$$\n",
        "where $l_f(x; y) := f(y) + \\left \\langle \\bigtriangledown f(y), x-y \\right \\rangle$."
      ],
      "id": "766b1dcf-1928-4c89-9bc6-9e14890275cf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62f42921-7269-4314-a585-014c497d478c"
      },
      "source": [
        "Recal that PGM with a constant prox stepsize is recursive in nature and iterates according to : $$x_{k+1}=prox_{\\lambda h}(x_k-\\lambda\\nabla f(x_k)).$$"
      ],
      "id": "62f42921-7269-4314-a585-014c497d478c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "073e167d-8aa1-438f-b5d3-5c6494048e76"
      },
      "source": [
        "#### Let's get started!"
      ],
      "id": "073e167d-8aa1-438f-b5d3-5c6494048e76"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "276bbe32-c81a-4403-9c32-fe2a4122a52b"
      },
      "source": [
        "First, we will derive a single iteration of PGM and prove that it is strong convex."
      ],
      "id": "276bbe32-c81a-4403-9c32-fe2a4122a52b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7dbb27c-b73b-4f5a-93df-ebc4076c4fe3"
      },
      "source": [
        "$$ x_{k+1} = argmin\\left \\{ h(u) +\\frac{1}{2}\\left \\| u-(x_k-\\bigtriangledown f(x_k)) \\right \\|^2 \\right \\}$$"
      ],
      "id": "d7dbb27c-b73b-4f5a-93df-ebc4076c4fe3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14ff5fc9-0aa2-456c-b056-0a49f7de7aad"
      },
      "source": [
        "$$ x_{k+1} = argmin\\left \\{ f(x_k) + \\left \\langle \\bigtriangledown f(x_k), u-x_k \\right \\rangle +h(u) + \\frac{1}{2}\\left \\| x-x_k \\right \\|^2 \\right \\}$$"
      ],
      "id": "14ff5fc9-0aa2-456c-b056-0a49f7de7aad"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bafad07c-d696-47b8-8146-96555cd4d8e5"
      },
      "source": [
        "$$x_{k+1}= argmin \\left\\{\\ell_f(u;x_k)+h(u) + \\frac{1}{2\\lambda}||u-x_k||^2 \\right\\}, $$"
      ],
      "id": "bafad07c-d696-47b8-8146-96555cd4d8e5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e26a33f3-473d-40bd-afd8-69f12bb9fae6"
      },
      "source": [
        "And proving strong convexity $\\left \\langle \\bigtriangledown h(u) - \\bigtriangledown h(x), u - x \\right \\rangle \\geq \\lambda \\left \\| u-x \\right \\|^{2}$:"
      ],
      "id": "e26a33f3-473d-40bd-afd8-69f12bb9fae6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bd53c6c-56d1-4798-ba4f-9ed5af0e0645"
      },
      "source": [
        "$$ \\left \\langle prox_{\\lambda h}(u) - prox_{\\lambda h}(x), (u-\\frac{1}{\\lambda}\\bigtriangledown h(u)) -(x-\\frac{1}{\\lambda}\\bigtriangledown h(x))  \\right \\rangle \\geq \\left \\| prox_{\\lambda h}(u)-prox_{\\lambda h}(x) \\right \\|^{2} $$"
      ],
      "id": "9bd53c6c-56d1-4798-ba4f-9ed5af0e0645"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b63b924-9d05-4607-8882-3cdc594f4b1a"
      },
      "source": [
        "$$ \\left \\langle (u - \\frac{1}{\\lambda}\\bigtriangledown h(u)) - (x - \\frac{1}{\\lambda}\\bigtriangledown h(x)), (u-\\frac{1}{\\lambda}\\bigtriangledown h(u)) -(x-\\frac{1}{\\lambda}\\bigtriangledown h(x))  \\right \\rangle \\geq  \\left \\| (u - \\frac{1}{\\lambda}\\bigtriangledown h(u)) - (x - \\frac{1}{\\lambda}\\bigtriangledown h(x)) \\right \\|^{2} $$"
      ],
      "id": "9b63b924-9d05-4607-8882-3cdc594f4b1a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acb65112-8ebe-4ed1-9156-05d514b5d465"
      },
      "source": [
        "Using the definition of $x_{k+1}$ and the strong convexity, we obtain upon rearranging terms that:"
      ],
      "id": "acb65112-8ebe-4ed1-9156-05d514b5d465"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfab72fc-3f68-404c-8f59-048b1ac1f1f5"
      },
      "source": [
        "$$ h(x_{k+1}) \\leq h(x) + \\left \\langle -\\bigtriangledown f(u), x^{k+1}-x \\right \\rangle + \\frac{1}{2}\\left \\| u-x \\right \\|^2 - \\frac{1}{2}\\left \\| u-x^{k+1} \\right \\|^2 - \\frac{1}{2}\\left \\| x^{k+1}-x \\right \\|^2 $$"
      ],
      "id": "cfab72fc-3f68-404c-8f59-048b1ac1f1f5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d66888d6-311c-4a41-9475-0527db8fc21b"
      },
      "source": [
        "Due to the Lipschitz continuity:\n",
        "$$ f(x_{k+1}) \\leq f(u) + \\left \\langle -\\bigtriangledown f(u), u-x^{k+1} \\right \\rangle + \\frac{1}{2}\\left \\| u-x_{k+1} \\right \\|^2 $$"
      ],
      "id": "d66888d6-311c-4a41-9475-0527db8fc21b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "116cc4b9-d126-46bb-bd15-3baaa7308800"
      },
      "source": [
        "Adding the two: $$ f(x_{k+1}) + h(x_{k+1}) \\leq f(u) + h(x) + \\left \\langle \\bigtriangledown f(u), u-x \\right \\rangle - \\frac{1}{2}\\left \\| x_{k+1}-x \\right \\|^2 + \\frac{1}{2}\\left \\| u-x \\right \\|^2 $$"
      ],
      "id": "116cc4b9-d126-46bb-bd15-3baaa7308800"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2302fd3a-bba6-4241-8c1a-f10381609f13"
      },
      "source": [
        "Using definition for $\\ell_f(u;x_k)$ $$\\ell_f(u;x_k)+h(u) + \\frac{1}{2}||u-x_k||^2 \\geq \\ell_f(x_{k+1};x_k)+h(x_{k+1})+\\frac{1}{2}||x_{k+1}-x_k||^2 + \\frac{1}{2}||u-x_{k+1}||^2$$"
      ],
      "id": "2302fd3a-bba6-4241-8c1a-f10381609f13"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24a600b8-008d-4a1a-b005-e436900f8335"
      },
      "source": [
        "Similarly for any x in int(dom(f)):\n",
        "$$ f(x_*) \\leq f(x) + \\left \\langle \\bigtriangledown f(x), x_*-x \\right \\rangle + \\frac{1}{2\\lambda}\\left \\| x_*-x \\right \\|^2 $$\n",
        "It holds that $$ (f+h)(x)-(f+h)(x_*) \\geq \\frac{1}{2\\lambda}\\left \\| x-x_* \\right \\|^2 $$"
      ],
      "id": "24a600b8-008d-4a1a-b005-e436900f8335"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41e01e79-6cbb-4f61-9df6-949babd05280"
      },
      "source": [
        "Consider $$ g(u) = f(x_{k+1})+\\left \\langle \\bigtriangledown f(x_{k+1}), u-x_{k+1} \\right \\rangle + g(u)+\\frac{1}{2\\lambda}\\left \\| u-x_{k+1} \\right \\|^2$$"
      ],
      "id": "41e01e79-6cbb-4f61-9df6-949babd05280"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e211e92-4cc8-4015-9937-af3a110db694"
      },
      "source": [
        "$ x_* = argmin_g(u) $\n",
        "$$ g(x)-g(x_*) \\geq \\frac{1}{2\\lambda}\\left \\| x-x_* \\right \\|^2 $$"
      ],
      "id": "8e211e92-4cc8-4015-9937-af3a110db694"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "781a3bd4-af3a-4727-8a59-60b5293e1510"
      },
      "source": [
        "Since $$ g(x_*) = f(x_{k+1}) + \\left \\langle \\bigtriangledown f(x_{k+1}),x_*-x_{k+1} \\right \\rangle + \\frac{1}{2\\lambda}\\left \\| x_*-x_{k+1} \\right \\|^2 + h(x_*) $$\n",
        "$$ \\geq f(x_*)+h(x_*) = (f+h)(x_*) $$"
      ],
      "id": "781a3bd4-af3a-4727-8a59-60b5293e1510"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a80aa15d-64a9-45aa-9d00-5ddeea064a85"
      },
      "source": [
        "This implies that $$ h(x_{k+1})-(f+h)(x_*) \\geq \\frac{1}{2\\lambda}\\left \\|  x_{k+1}-x_*\\right \\|^2 $$"
      ],
      "id": "a80aa15d-64a9-45aa-9d00-5ddeea064a85"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad5dedcf-d250-4469-86c3-7643261551d8"
      },
      "source": [
        "Plugging for g(u) into above inequality"
      ],
      "id": "ad5dedcf-d250-4469-86c3-7643261551d8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0341f4ba-8870-44f7-a3f4-7ee423a6df3c"
      },
      "source": [
        "$$ f(x_{k+1}) + \\left \\langle \\bigtriangledown f(x_{k+1}), x-x_{k+1} \\right \\rangle + h(x)+\\frac{1}{2\\lambda}\\left \\| x-x_{k+1} \\right \\|^2 -(f+h)(x_*) \\geq \\frac{1}{2\\lambda}\\left \\| x-x_* \\right \\|^2$$"
      ],
      "id": "0341f4ba-8870-44f7-a3f4-7ee423a6df3c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e7f0187-bed1-4d85-9554-67ca092febee"
      },
      "source": [
        "Which is equal to "
      ],
      "id": "5e7f0187-bed1-4d85-9554-67ca092febee"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39a3c1db-6596-430a-a9e0-9aa7e67a8374"
      },
      "source": [
        "$$ (f+h)(x_{k+1})-(f+h)(x_*) \\geq \\frac{1}{2\\lambda}\\left \\| x_{k+1}-x_* \\right \\|^2 -\\frac{1}{2\\lambda}\\left \\| x-x_{k+1} \\right \\|^2 +f(x_{k+1}) + \\ell_f(x;x_{k+1}) $$"
      ],
      "id": "39a3c1db-6596-430a-a9e0-9aa7e67a8374"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "211c987c-c7d1-451f-ac95-429e9bbd5faf"
      },
      "source": [
        "$$(f+h)(x_*)+\\frac{1}{2\\lambda}||x_k-x_*||^2 \\geq (f+h)(x_{k+1})+h(x_{k+1})+ \\frac{1}{2\\lambda}||x_{k+1}-x_*||^2$$"
      ],
      "id": "211c987c-c7d1-451f-ac95-429e9bbd5faf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fbb94de-a28d-4c2d-93a8-663e2d339aed"
      },
      "source": [
        "Using $$ \\frac{1}{2\\lambda}((f+h)(x_*)-(f+h)(x_{k+1})) \\geq \\left \\| x_*-x_{k+1} \\right \\|^2 -\\left \\| x_*-x_k \\right \\|^2 + \\frac{1}{2\\lambda}\\ell_f(x_*,x_k)$$"
      ],
      "id": "4fbb94de-a28d-4c2d-93a8-663e2d339aed"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c4ca187-104f-41e3-888e-438ac4cbf4e5"
      },
      "source": [
        "$$ \\frac{1}{2\\lambda}((f+h)(x_*)-(f+h)(x_{k+1})) \\geq \\left \\| x_*-x_{k+1} \\right \\|^2 -\\left \\| x_*-x_k \\right \\|^2 $$"
      ],
      "id": "5c4ca187-104f-41e3-888e-438ac4cbf4e5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e3b7af4-d91f-4f66-9aec-00a85029701a"
      },
      "source": [
        "Sum over all n from 0 to k to obtain:\n",
        "$$\n",
        "\\frac{1}{2\\lambda}\\sum_{}^{k}(f+h)(x_*)-(f+h)(x_{k+1}) \\geq \\left \\| x_*-x_k \\right \\|^2-\\left \\| x_*-x_0 \\right \\|^2\n",
        "$$"
      ],
      "id": "7e3b7af4-d91f-4f66-9aec-00a85029701a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78b72791-9036-4108-97b5-897f616e4b53"
      },
      "source": [
        "Thus\n",
        "$$\n",
        "\\sum_{}^{k}((f+h)(x_{k+1})-(f+h)(x_*)) \\leq \\frac{1}{2\\lambda}\\left \\| x_*-x_0 \\right \\|^2-\\frac{1}{2\\lambda}\\left \\| x_*-x_k \\right \\|^2 \\leq \\frac{1}{2\\lambda}\\left \\| x_*-x_0 \\right \\|^2\n",
        "$$"
      ],
      "id": "78b72791-9036-4108-97b5-897f616e4b53"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cc66cdb-65db-4573-a413-f6f2b91360ae"
      },
      "source": [
        "Given the monotonicity of $(f+h)(x_n)$ for $n \\geq 0$\n",
        "$$ \n",
        "k((f+h)(x_k)-(f+h)(x_*)) \\leq \\sum_{}^{k}((f+h)(x_{k+1})-(f+h)(x_*)) \\leq \\frac{1}{2\\lambda}\\left \\| x_*-x_0 \\right \\|^2\n",
        "$$"
      ],
      "id": "0cc66cdb-65db-4573-a413-f6f2b91360ae"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "792eb5b5-127a-419d-a5fd-ca899ba32a47"
      },
      "source": [
        "Thus $$\\sum_{i=1}^k (f+h)(x_i)-k(f+h)(x_*) \\leq \\frac{||x_0-x_*||^2}{2\\lambda} $$\n"
      ],
      "id": "792eb5b5-127a-419d-a5fd-ca899ba32a47"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6fc25c9-e701-4637-95db-3c110d6ccdf9"
      },
      "source": [
        "Proving PGM has the descent property:"
      ],
      "id": "c6fc25c9-e701-4637-95db-3c110d6ccdf9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "badee0ff-5717-4481-8845-b1573b14714c"
      },
      "source": [
        "$$(f+h)(x_k) \\geq (f+h)(x_{k+1}), \\forall k \\geq 0 $$"
      ],
      "id": "badee0ff-5717-4481-8845-b1573b14714c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f2db67c-3ea8-450f-b7c9-0df81a070c5f"
      },
      "source": [
        "$$ \\frac{1}{2\\lambda}((f+h)(x_*)-(f+h)(x_{k+1})) \\geq \\left \\| x_*-x_{k+1} \\right \\|^2 -\\left \\| x_*-x_k \\right \\|^2 + \\frac{1}{2\\lambda}\\ell_f(x_*,x_k)$$"
      ],
      "id": "6f2db67c-3ea8-450f-b7c9-0df81a070c5f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2c46548-946a-45c5-8bc2-8eab51b3e9ea"
      },
      "source": [
        "Along with the relationship: $$ \\left \\| x_{k+1} -x_*\\right \\| \\leq \\left \\| x_k-x_* \\right \\|$$"
      ],
      "id": "c2c46548-946a-45c5-8bc2-8eab51b3e9ea"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eb9912d-dbc6-4276-9e59-6605c4a03821"
      },
      "source": [
        "It follows that: $$ (f+h)(x_*)-(f+h)(x_{k+1}) \\leq (f+h)(x_*)-(f+h)(x_{k}) \\leq 0$$"
      ],
      "id": "8eb9912d-dbc6-4276-9e59-6605c4a03821"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98adda48-0838-49d7-b51e-3cdb822f68ab"
      },
      "source": [
        "Thus for all k $\\geq 0$ $$(f+h)(x_{k+1}) \\leq (f+h)(x_{k}) $$"
      ],
      "id": "98adda48-0838-49d7-b51e-3cdb822f68ab"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9707a214-b66b-4ea9-b226-ded5586e69dc"
      },
      "source": [
        "Finally, given the above:\n",
        "$$ \n",
        "k((f+h)(x_k)-(f+h)(x_*)) \\leq \\sum_{}^{k}((f+h)(x_{k+1})-(f+h)(x_*)) \\leq \\frac{1}{2\\lambda}\\left \\| x_0-x_* \\right \\|^2\n",
        "$$\n",
        "Consequently\n",
        "$$ (f+h)(x_i)-(f+h)(x_*) \\leq  \\frac{1}{k2\\lambda}||x_0-x_*||^2 $$"
      ],
      "id": "9707a214-b66b-4ea9-b226-ded5586e69dc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "666f32f5-5c41-432a-82c1-1afc0115c5f3"
      },
      "source": [
        "Hence we obtain the $O(\\frac{1}{k})$ convergence rate"
      ],
      "id": "666f32f5-5c41-432a-82c1-1afc0115c5f3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "477d0b2b-bd4c-4d7d-a5d7-ec31c8947da9"
      },
      "source": [
        "__________________________________"
      ],
      "id": "477d0b2b-bd4c-4d7d-a5d7-ec31c8947da9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "313b0570-b151-4f47-b87a-8b535ee64fa9"
      },
      "source": [
        "### Code Example"
      ],
      "id": "313b0570-b151-4f47-b87a-8b535ee64fa9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0f3872d-5627-46f2-a45d-02b403b64934"
      },
      "source": [
        "Here we will employ proximal gradient descent with stochastic schemes. In general, when the loss function we are trying to minimize can be wwritten in the form $\\sum_{i=1}^{m}g_i(\\theta )$ where each $g_i(\\theta)$ is the loss sample at i, and the training time is long, then stochastic schemes should be considered. We will optimize \n",
        "$$f(\\theta) = \\underset{\\theta \\in \\mathbb{R}^d}{min}\\frac{1}{m}\\sum_{i=1}^{m}\\left [ log(1+exp(x_i\\theta)) -y_ix_i\\theta \\right ] + \\lambda\\left \\| \\theta \\right \\|_1$$\n",
        "We decompose $f(\\theta)$ into a convex and differentiable function g and a convex but not differentiable function h:\n",
        "$$ g(\\theta) = \\frac{1}{m}\\sum_{i=1}^{m}log(1+exp(x_i\\theta)) $$\n",
        "$$ h(\\theta) = \\frac{1}{m}\\sum_{i=1}^{m} -y_ix_i\\theta + \\lambda\\left \\| \\theta \\right \\|_1$$"
      ],
      "id": "e0f3872d-5627-46f2-a45d-02b403b64934"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00ab09d6-a064-4591-af17-0a4146fbc788"
      },
      "source": [
        "The data we are using is from the classic MNIST machine learning dataset. There are two classes, 0 and 1, and we have a total of 14,780 images; a training set of 12,665 and a test set of 2,115. Each image is 28x28. Each image is vectorized and stacked to form a training and test matrix, with the label appended to the last column of each matrix. Thus, our classifier will learn $\\theta$ on the train set to predict the labels for the test set. "
      ],
      "id": "00ab09d6-a064-4591-af17-0a4146fbc788"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5454956-7545-4c6c-9962-b3149e3c1ed2"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "id": "e5454956-7545-4c6c-9962-b3149e3c1ed2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e097ae6c-0bc8-4c4d-90b2-2baf5f04056d"
      },
      "source": [
        "x_train = train[:, :-1]\n",
        "y_train = train[:, -1 :]\n",
        "x_test = test[:, :-1]\n",
        "y_test = test[:, -1 :]\n",
        "\n",
        "x = x_train\n",
        "y = y_train"
      ],
      "id": "e097ae6c-0bc8-4c4d-90b2-2baf5f04056d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0b74e42-ffee-4734-aee6-573c068a6667"
      },
      "source": [
        "def predict_labels(X, weights):\n",
        "    return 1/(1+np.exp(-X.dot(weights)))\n",
        "\n",
        "def soft_threshold(x,t):\n",
        "    pos = np.maximum(x - t, 0)\n",
        "    neg = np.minimum(x + t, 0)\n",
        "    return pos+neg\n",
        "\n",
        "def log_loss(X, theta):\n",
        "    return np.sum(np.log(1 + np.exp(X.dot(theta)))) / X.shape[0]\n",
        "\n",
        "def h(X, y, lam=10, lr=0.01):\n",
        "    return (1/len(X))*(-y.T.dot(X)) + lam*lr\n",
        "\n",
        "def evaluate_gradient(X, theta, y=None):\n",
        "    return np.sum((X*np.exp(X.dot(theta))) / (1 + np.exp(X.dot(theta))), axis=0)/m"
      ],
      "id": "f0b74e42-ffee-4734-aee6-573c068a6667",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64828a46-6112-4425-b1f7-da66be8c5d3c"
      },
      "source": [
        "n = 100 \n",
        "lam = 10\n",
        "lr= 0.01\n",
        "max_iters=1000\n",
        "tol= 1e-3\n",
        "N, D = x.shape\n",
        "theta_current = np.zeros(shape=(D, 1))\n",
        "losses = [log_loss(x, theta_current)]\n",
        "thetas = [theta_current]\n",
        "\n",
        "iterations = 1\n",
        "while (loss > tol) or (iterations > max_iters):\n",
        "    theta_current = thetas[-1]\n",
        "\n",
        "    # Stochastic\n",
        "    number_of_rows = x.shape[0]\n",
        "    random_indices = np.random.choice(number_of_rows, size=n, replace=False)\n",
        "    x_temp, y_temp = x[random_indices, :], y[random_indices, :] \n",
        "\n",
        "    for it in range(n):\n",
        "    # Proximal GD\n",
        "        grad = evaluate_gradient(x_temp, theta_current).reshape(-1,1)\n",
        "        theta_new_grad =  theta_current - (lr * grad)\n",
        "        theta_new = soft_threshold(theta_new_grad, h(x_temp, y_temp))\n",
        "        theta_current = theta_new\n",
        "        \n",
        "    loss = log_loss(x, theta_current)\n",
        "    losses.append(loss)\n",
        "    thetas.append(theta_current)  \n",
        "    iterations += 1"
      ],
      "id": "64828a46-6112-4425-b1f7-da66be8c5d3c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4d3f55b-e024-4244-bba3-6d23fde05a37"
      },
      "source": [
        "# Non-stochastic approach\n",
        "\n",
        "n = 100 \n",
        "lam = 10\n",
        "lr= 0.01\n",
        "max_iters=1000\n",
        "tol= 1e-5\n",
        "N, D = x.shape\n",
        "theta_current = np.zeros(shape=(D, 1))\n",
        "loss_1 = log_loss(x, theta_current)\n",
        "losses = [loss_1]\n",
        "thetas = [theta_current]\n",
        "\n",
        "iterations = 1\n",
        "#while losses[-1] > tol:\n",
        "for i in range(200):\n",
        "    theta_current = thetas[-1]\n",
        "    grad = evaluate_gradient(x, theta_current).reshape(-1,1)\n",
        "    theta_new_grad =  theta_current - (lr * grad)\n",
        "    theta_new = soft_threshold(theta_new_grad, h(x, y).T)\n",
        "    theta_current = theta_new\n",
        "        \n",
        "    loss = log_loss(x, theta_current)\n",
        "    losses.append(loss)\n",
        "    thetas.append(theta_current)  \n",
        "    #iterations += 1"
      ],
      "id": "d4d3f55b-e024-4244-bba3-6d23fde05a37",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6131719d-2da1-465e-a706-8d0a5f86fbb6"
      },
      "source": [
        "predict_labels(x, thetas[-1])\n",
        "accuracy_score(y_test, predict_labels(x_test, thetas[-1]))"
      ],
      "id": "6131719d-2da1-465e-a706-8d0a5f86fbb6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6fa4563-bd4d-4364-b874-854fad11dca0"
      },
      "source": [
        "Overall, this stochastic implementation achieves an accuracy of 93.76 on the training set.  "
      ],
      "id": "c6fa4563-bd4d-4364-b874-854fad11dca0"
    }
  ]
}
