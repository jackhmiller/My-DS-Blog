<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://jackhmiller.github.io/My-DS-Blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://jackhmiller.github.io/My-DS-Blog/" rel="alternate" type="text/html" /><updated>2023-06-19T14:11:36-05:00</updated><id>https://jackhmiller.github.io/My-DS-Blog/feed.xml</id><title type="html">Concept Drift</title><subtitle>050-709-2944 | Jack.harris.miller@gmail.com</subtitle><entry><title type="html">Trying to Make Sense of Neural Network Generalization</title><link href="https://jackhmiller.github.io/My-DS-Blog/2021/12/11/Interpolation.html" rel="alternate" type="text/html" title="Trying to Make Sense of Neural Network Generalization" /><published>2021-12-11T00:00:00-06:00</published><updated>2021-12-11T00:00:00-06:00</updated><id>https://jackhmiller.github.io/My-DS-Blog/2021/12/11/Interpolation</id><author><name></name></author><category term="theory" /><category term="interpolation" /><summary type="html">I am sure that by now you have heard the jokes and seen the memes that AI is essentially just a bunch of if else statements. Take this tweet below:</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://jackhmiller.github.io/My-DS-Blog/images/interp.png" /><media:content medium="image" url="https://jackhmiller.github.io/My-DS-Blog/images/interp.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Neural Tangent Kernel</title><link href="https://jackhmiller.github.io/My-DS-Blog/2021/10/02/NTK.html" rel="alternate" type="text/html" title="Neural Tangent Kernel" /><published>2021-10-02T00:00:00-05:00</published><updated>2021-10-02T00:00:00-05:00</updated><id>https://jackhmiller.github.io/My-DS-Blog/2021/10/02/NTK</id><author><name></name></author><category term="DL_theory/Neural_tangent_kernel" /><summary type="html">Introduction Much of the research on deep learning theory over the past few years addresses the common theme of analyzing neural networks in the infinite-width limit. At first, this limit may seem impractical and even pointless to study. However, it turns out that neural networks in this regime simplify to linear models with a kernel called the neural tangent kernel. These results are significant as they give a way of understanding why neural networks converge to a optimal solution. Gradient descent is therefore very simple to study, leads to a proof of convergence of gradient descent to 0 training loss. Neural networks are know to be highly non-convex objects and so understanding their convergence under training is highly non-trivial.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://jackhmiller.github.io/My-DS-Blog/images/ntk.png" /><media:content medium="image" url="https://jackhmiller.github.io/My-DS-Blog/images/ntk.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">The Log-Sum-Exp Trick</title><link href="https://jackhmiller.github.io/My-DS-Blog/2021/03/01/Log-Sum-Exp-Trick.html" rel="alternate" type="text/html" title="The Log-Sum-Exp Trick" /><published>2021-03-01T00:00:00-06:00</published><updated>2021-03-01T00:00:00-06:00</updated><id>https://jackhmiller.github.io/My-DS-Blog/2021/03/01/Log-Sum-Exp-Trick</id><author><name></name></author><category term="" /><summary type="html">The Log-Sum-Exp Trick</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://jackhmiller.github.io/My-DS-Blog/images/floatingpoint.png" /><media:content medium="image" url="https://jackhmiller.github.io/My-DS-Blog/images/floatingpoint.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Tweedie Distribution</title><link href="https://jackhmiller.github.io/My-DS-Blog/2021/01/31/tweedie.html" rel="alternate" type="text/html" title="Tweedie Distribution" /><published>2021-01-31T00:00:00-06:00</published><updated>2021-01-31T00:00:00-06:00</updated><id>https://jackhmiller.github.io/My-DS-Blog/2021/01/31/tweedie</id><author><name></name></author><category term="" /><summary type="html">Is that a typo? The Tweedie distribution is a three-parameter family of distributions that is a special case of exponential dispersion models, but is a generalization of several familiar probability distributions, including the normal, gamma, inverse Gaussian and Poisson distributions. The distribution along with exponential dispersion models were introduced by Jørgensen in 1987.1 According to its Wikipedia page, the unusually named distribution was named as such by Jørgensen after Maurice Tweedie, a statistician and medical physicist at the University of Liverpool, UK, who presented the first thorough study of these distributions in 1984.2 Jørgensen, B. (1987), Exponential Dispersion Models. Journal of the Royal Statistical Society: Series B (Methodological), 49: 127-145. https://doi.org/10.1111/j.2517-6161.1987.tb01685.x &amp;#8617; Tweedie, M.C.K. (1984). “An index which distinguishes between some important exponential families”. In Ghosh, J.K.; Roy, J (eds.). Statistics: Applications and New Directions. Proceedings of the Indian Statistical Institute Golden Jubilee International Conference. Calcutta: Indian Statistical Institute. pp. 579–604. MR 0786162 &amp;#8617;</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://jackhmiller.github.io/My-DS-Blog/images/tweedie.png" /><media:content medium="image" url="https://jackhmiller.github.io/My-DS-Blog/images/tweedie.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Alternating Method of Multipliers- Theory and Industry Example Application</title><link href="https://jackhmiller.github.io/My-DS-Blog/2020/12/17/ADMM.html" rel="alternate" type="text/html" title="Alternating Method of Multipliers- Theory and Industry Example Application" /><published>2020-12-17T00:00:00-06:00</published><updated>2020-12-17T00:00:00-06:00</updated><id>https://jackhmiller.github.io/My-DS-Blog/2020/12/17/ADMM</id><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://jackhmiller.github.io/My-DS-Blog/images/convex_opt.png" /><media:content medium="image" url="https://jackhmiller.github.io/My-DS-Blog/images/convex_opt.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>