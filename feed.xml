<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://jackhmiller.github.io/My-DS-Blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://jackhmiller.github.io/My-DS-Blog/" rel="alternate" type="text/html" /><updated>2023-06-04T07:15:04-05:00</updated><id>https://jackhmiller.github.io/My-DS-Blog/feed.xml</id><title type="html">Concept Drift</title><subtitle>050-709-2944 | Jack.harris.miller@gmail.com</subtitle><entry><title type="html">Trying to Make Sense of Neural Network Generalization</title><link href="https://jackhmiller.github.io/My-DS-Blog/2021/12/11/Interpolation.html" rel="alternate" type="text/html" title="Trying to Make Sense of Neural Network Generalization" /><published>2021-12-11T00:00:00-06:00</published><updated>2021-12-11T00:00:00-06:00</updated><id>https://jackhmiller.github.io/My-DS-Blog/2021/12/11/Interpolation</id><author><name></name></author><category term="theory" /><category term="interpolation" /><summary type="html">I am sure that by now you have heard the jokes and seen the memes that AI is essentially just a bunch of if else statements. Take this tweet below:</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://jackhmiller.github.io/My-DS-Blog/images/interp.png" /><media:content medium="image" url="https://jackhmiller.github.io/My-DS-Blog/images/interp.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Neural Tangent Kernel</title><link href="https://jackhmiller.github.io/My-DS-Blog/2021/10/02/NTK.html" rel="alternate" type="text/html" title="Neural Tangent Kernel" /><published>2021-10-02T00:00:00-05:00</published><updated>2021-10-02T00:00:00-05:00</updated><id>https://jackhmiller.github.io/My-DS-Blog/2021/10/02/NTK</id><author><name></name></author><category term="DL_theory/Neural_tangent_kernel" /><summary type="html">Introduction Much of the research on deep learning theory over the past few years addresses the common theme of analyzing neural networks in the infinite-width limit. At first, this limit may seem impractical and even pointless to study. However, it turns out that neural networks in this regime simplify to linear models with a kernel called the neural tangent kernel. These results are significant as they give a way of understanding why neural networks converge to a optimal solution. Gradient descent is therefore very simple to study, leads to a proof of convergence of gradient descent to 0 training loss. Neural networks are know to be highly non-convex objects and so understanding their convergence under training is highly non-trivial.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://jackhmiller.github.io/My-DS-Blog/images/ntk.png" /><media:content medium="image" url="https://jackhmiller.github.io/My-DS-Blog/images/ntk.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">The Log-Sum-Exp Trick</title><link href="https://jackhmiller.github.io/My-DS-Blog/2021/03/01/Log-Sum-Exp-Trick.html" rel="alternate" type="text/html" title="The Log-Sum-Exp Trick" /><published>2021-03-01T00:00:00-06:00</published><updated>2021-03-01T00:00:00-06:00</updated><id>https://jackhmiller.github.io/My-DS-Blog/2021/03/01/Log-Sum-Exp-Trick</id><author><name></name></author><category term="" /><summary type="html">The Log-Sum-Exp Trick</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://jackhmiller.github.io/My-DS-Blog/images/floatingpoint.png" /><media:content medium="image" url="https://jackhmiller.github.io/My-DS-Blog/images/floatingpoint.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Proving Proximal Gradient Method’s Convergence Rate and a Code Demonstration</title><link href="https://jackhmiller.github.io/My-DS-Blog/2020/08/05/PGD.html" rel="alternate" type="text/html" title="Proving Proximal Gradient Method’s Convergence Rate and a Code Demonstration" /><published>2020-08-05T00:00:00-05:00</published><updated>2020-08-05T00:00:00-05:00</updated><id>https://jackhmiller.github.io/My-DS-Blog/2020/08/05/PGD</id><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://jackhmiller.github.io/My-DS-Blog/images/PGD.png" /><media:content medium="image" url="https://jackhmiller.github.io/My-DS-Blog/images/PGD.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Alternating Method of Multipliers- Theory and Industry Example Application</title><link href="https://jackhmiller.github.io/My-DS-Blog/2020/04/20/ADMM.html" rel="alternate" type="text/html" title="Alternating Method of Multipliers- Theory and Industry Example Application" /><published>2020-04-20T00:00:00-05:00</published><updated>2020-04-20T00:00:00-05:00</updated><id>https://jackhmiller.github.io/My-DS-Blog/2020/04/20/ADMM</id><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://jackhmiller.github.io/My-DS-Blog/images/convex_opt.png" /><media:content medium="image" url="https://jackhmiller.github.io/My-DS-Blog/images/convex_opt.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>