<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://jackhmiller.github.io/My-DS-Blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://jackhmiller.github.io/My-DS-Blog/" rel="alternate" type="text/html" /><updated>2024-05-27T13:10:57-05:00</updated><id>https://jackhmiller.github.io/My-DS-Blog/feed.xml</id><title type="html">Concept Drift</title><subtitle>050-709-2944 | Jack.harris.miller@gmail.com</subtitle><entry><title type="html">Stanford’s DSPy LLM Framework</title><link href="https://jackhmiller.github.io/My-DS-Blog/2024/02/10/DSPy.html" rel="alternate" type="text/html" title="Stanford’s DSPy LLM Framework" /><published>2024-02-10T00:00:00-06:00</published><updated>2024-02-10T00:00:00-06:00</updated><id>https://jackhmiller.github.io/My-DS-Blog/2024/02/10/DSPy</id><author><name></name></author><summary type="html">Setting up the Foundational Model Landscape The current LLM/foundational model landscape can be defined by dedication to task-specific design, that prioritize flexibility for performance. These tasks include: Adaptation Prompting- zero-shot, few-shot, kNN Finte-tuning- PEFT, LoRa Reasoning Chain-of-thought Agent loops Augmentation Retrieval augmentation, multi-model augmentation Calling external tools Retrieval modeling</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://jackhmiller.github.io/My-DS-Blog/images/dspy.png" /><media:content medium="image" url="https://jackhmiller.github.io/My-DS-Blog/images/dspy.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Chunking in RAG</title><link href="https://jackhmiller.github.io/My-DS-Blog/2023/08/25/RAG.html" rel="alternate" type="text/html" title="Chunking in RAG" /><published>2023-08-25T00:00:00-05:00</published><updated>2023-08-25T00:00:00-05:00</updated><id>https://jackhmiller.github.io/My-DS-Blog/2023/08/25/RAG</id><author><name></name></author><category term="LLMs" /><category term="chunking" /><summary type="html">Introduction Language models have become an important and flexible building block in a variety of user-facing language technologies, including conve2024-02-05rsational interfaces, search and summarization, and collaborative writing. These models perform downstream tasks primarily via prompting: all relevant task specification and data to process is formatted as a textual input context, and the model returns a generated text completion. These input contexts can contain thousands of tokens, especially when language models are used to process long documents (e.g., legal or scientific documents, conversation histories, etc.) or when language models are augmented with external information.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://jackhmiller.github.io/My-DS-Blog/images/chunk_thumb.png" /><media:content medium="image" url="https://jackhmiller.github.io/My-DS-Blog/images/chunk_thumb.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">PEFT with LoRA and QLoRA</title><link href="https://jackhmiller.github.io/My-DS-Blog/2023/06/22/PEFT-LORA-QLORA.html" rel="alternate" type="text/html" title="PEFT with LoRA and QLoRA" /><published>2023-06-22T00:00:00-05:00</published><updated>2023-06-22T00:00:00-05:00</updated><id>https://jackhmiller.github.io/My-DS-Blog/2023/06/22/PEFT-LORA-QLORA</id><author><name></name></author><category term="LLMs, LoRA, PEFT, QLoRA, finished_post" /><summary type="html">Introduction Since the inception of transfer learning, dozens of works have sought to make model adaptation more parameter- and compute-efficient. Yet it is still the case that one of main drawbacks for LLMs is that have to be fine-tuned for each downstream task, learning a different set of parameters.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://jackhmiller.github.io/My-DS-Blog/images/LoRA.png" /><media:content medium="image" url="https://jackhmiller.github.io/My-DS-Blog/images/LoRA.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">ColBERT</title><link href="https://jackhmiller.github.io/My-DS-Blog/2022/03/07/COLBERT.html" rel="alternate" type="text/html" title="ColBERT" /><published>2022-03-07T00:00:00-06:00</published><updated>2022-03-07T00:00:00-06:00</updated><id>https://jackhmiller.github.io/My-DS-Blog/2022/03/07/COLBERT</id><author><name></name></author><summary type="html">Introduction- where ColBERT fits Vector search has experienced explosive growth in recent years, especially after the advent of large language models (LLMs). that employ embedding-based representations of queries and documents and directly model local interactions (i.e., fine-granular relationships) between their contents. This popularity has prompted a significant focus in academic research on enhancing embedding models through expanded training data, advanced training methods, and new architectures. However, these new embedding-based representations come at a steep computational cost.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://jackhmiller.github.io/My-DS-Blog/images/colbert_title.jpg" /><media:content medium="image" url="https://jackhmiller.github.io/My-DS-Blog/images/colbert_title.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Rotary Position Embeddings (RoPE)</title><link href="https://jackhmiller.github.io/My-DS-Blog/2022/02/19/ROPE.html" rel="alternate" type="text/html" title="Rotary Position Embeddings (RoPE)" /><published>2022-02-19T00:00:00-06:00</published><updated>2022-02-19T00:00:00-06:00</updated><id>https://jackhmiller.github.io/My-DS-Blog/2022/02/19/ROPE</id><author><name></name></author><summary type="html">The Problem Since Vaswani et al., 2017 [16] there have been many schemes introduced for encoding positional information in transformers. When applying self-attention to a given domain, the choice of position encoding typically involves tradeoffs between simplicity, flexibility, and efficiency. For example, learned absolute positional encoding is very simple, but may not generalize and are not always particularly meaningful due to the common practices of packing short sentences and phrases together in a single context and breaking up sentences across contexts.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://jackhmiller.github.io/My-DS-Blog/images/rope.PNG" /><media:content medium="image" url="https://jackhmiller.github.io/My-DS-Blog/images/rope.PNG" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>