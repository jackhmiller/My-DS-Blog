<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>A Detailed Guide to 5 Loss Functions for Machine Learning Algorithms with Python Code | Not Another Data Science Blog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="A Detailed Guide to 5 Loss Functions for Machine Learning Algorithms with Python Code" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This post covers multiple loss functions, where they work, and how you can code them in Python" />
<meta property="og:description" content="This post covers multiple loss functions, where they work, and how you can code them in Python" />
<link rel="canonical" href="https://jackhmiller.github.io/My-DS-Blog/2017/11/13/Losses.html" />
<meta property="og:url" content="https://jackhmiller.github.io/My-DS-Blog/2017/11/13/Losses.html" />
<meta property="og:site_name" content="Not Another Data Science Blog" />
<meta property="og:image" content="https://jackhmiller.github.io/My-DS-Blog/images/lf.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2017-11-13T00:00:00-06:00" />
<script type="application/ld+json">
{"url":"https://jackhmiller.github.io/My-DS-Blog/2017/11/13/Losses.html","@type":"BlogPosting","headline":"A Detailed Guide to 5 Loss Functions for Machine Learning Algorithms with Python Code","dateModified":"2017-11-13T00:00:00-06:00","datePublished":"2017-11-13T00:00:00-06:00","image":"https://jackhmiller.github.io/My-DS-Blog/images/lf.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://jackhmiller.github.io/My-DS-Blog/2017/11/13/Losses.html"},"description":"This post covers multiple loss functions, where they work, and how you can code them in Python","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/My-DS-Blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://jackhmiller.github.io/My-DS-Blog/feed.xml" title="Not Another Data Science Blog" /><link rel="shortcut icon" type="image/x-icon" href="/My-DS-Blog/images/nn_JFc_icon.ico">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/My-DS-Blog/">Not Another Data Science Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/My-DS-Blog/CV/">My CV</a><a class="page-link" href="/My-DS-Blog/about/">About Me</a><a class="page-link" href="/My-DS-Blog/search/">Search</a><a class="page-link" href="/My-DS-Blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">A Detailed Guide to 5 Loss Functions for Machine Learning Algorithms with Python Code</h1><p class="page-description">This post covers multiple loss functions, where they work, and how you can code them in Python</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2017-11-13T00:00:00-06:00" itemprop="datePublished">
        Nov 13, 2017
      </time>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      9 min read
    
</span></p>

    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/jackhmiller/My-DS-Blog/tree/master/_notebooks/2017-11-13-Losses.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/My-DS-Blog/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/jackhmiller/My-DS-Blog/master?filepath=_notebooks%2F2017-11-13-Losses.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/My-DS-Blog/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/jackhmiller/My-DS-Blog/blob/master/_notebooks/2017-11-13-Losses.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/My-DS-Blog/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2017-11-13-Losses.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In supervised machine learning algorithms, we want to minimize the error for each training example during the learning process. This is done using some optimization strategies like gradient descent. And this error comes from the loss function. Similat to a loss function, we also use a cost function. However, although cost function and loss function are synonymous and used interchangeably, they are different. A loss function is for a single training example. It is also sometimes called an error function. A cost function, on the other hand, is the average loss over the entire training dataset. The optimization strategies aim at minimizing the cost function.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br /></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Regression-Loss-Functions">Regression Loss Functions<a class="anchor-link" href="#Regression-Loss-Functions"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>You must be quite familiar with linear regression at this point. It deals with modeling a linear relationship between a dependent variable, Y, and several independent variables, X_iâ€™s. Thus, we essentially fit a line in space on these variables $Y = a0 + a1 * X1 + a2 * X2 + ....+ an * Xn$. We will use the given data points to find the coefficients a0, a1, â€¦, an.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We will use the famous Boston Housing Dataset for understanding this concept. And to keep things simple, we will use only one feature â€“ the Average number of rooms per dwelling (X) â€“ to predict the dependent variable â€“ Median Value (Y) of houses in USD 1000â€² s. We will use Gradient Descent as an optimization strategy to find the regression line. Just as a refresher, the gradient descent update is as follows:
$$
\text{Repeat until convergence}\left\{ \theta_j \gets \theta_j - \alpha \frac{\partial }{\partial \theta_j}J(\theta) \right\}
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here, $\theta_j$ is the weight to be updated, alpha is the learning rate and J is the cost function. The cost function is parameterized by theta. Our aim is to find the value of theta which yields minimum overall cost</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For each loss function we will follow the following steps:</p>
<ol>
<li>Write the expression for our predictor function, f(X) and identify the parameters that we need to find</li>
<li>Identify the loss to use for each training example</li>
<li>Find the expression for the Cost Function â€“ the average loss on all examples</li>
<li>Find the gradient of the Cost Function with respect to each unknown parameter</li>
<li>Decide on the learning rate and run the weight update rule for a fixed number of iterations</li>
</ol>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;housing.csv&#39;</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;RM&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;MEDV&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br /></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="1.-Squared-Error-Loss">1. Squared Error Loss<a class="anchor-link" href="#1.-Squared-Error-Loss"> </a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Squared Error loss for each training example, also known as L2 Loss, is the square of the difference between the actual and the predicted values: $ L = (y-f(x))^2$. The corresponding cost function is the Mean of these Squared Errors (MSE).  It is a positive quadratic function (of the form ax^2 + bx + c where a &gt; 0). A quadratic function only has a global minimum. Since there are no local minima, we will never get stuck in one. Hence, it is always guaranteed that Gradient Descent will converge.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">squared_loss_update</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
    <span class="c1"># intitialize at zero</span>
    <span class="n">m_deriv</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">b_deriv</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
        <span class="c1"># calculate partial derivatives for loss function (y-mx-b)^2</span>
        <span class="c1"># -2x(y-mx-b)</span>
        <span class="n">m_deriv</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="p">(</span><span class="n">m</span><span class="o">*</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">-</span><span class="n">b</span><span class="p">)</span>
        
        <span class="c1"># -2(y-mx-b)</span>
        <span class="n">b_deriv</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">m</span><span class="o">*</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="n">b</span><span class="p">)</span>
        
        <span class="n">m</span> <span class="o">-=</span> <span class="p">(</span><span class="n">m_deriv</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="n">N</span><span class="p">))</span> <span class="o">*</span> <span class="n">learning_rate</span>
        <span class="n">b</span> <span class="o">-=</span> <span class="p">(</span><span class="n">b_deriv</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="n">N</span><span class="p">))</span> <span class="o">*</span> <span class="n">learning_rate</span>
        
    <span class="k">return</span> <span class="n">m</span><span class="p">,</span> <span class="n">b</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">MSE</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">m</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="n">b</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.0001</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">500</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
        <span class="n">loss</span> <span class="o">+=</span> <span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">m</span><span class="o">*</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">loss</span> <span class="o">/=</span> <span class="nb">float</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
    <span class="n">MSE</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">squared_loss_update</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">lr</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">index</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">MSE</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;alpha = 0.0001&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[&lt;matplotlib.lines.Line2D at 0x2cc24c9f730&gt;]</pre>
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
" />
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br /></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="2.-Mean-Absolute-Error-Loss">2. Mean Absolute Error Loss<a class="anchor-link" href="#2.-Mean-Absolute-Error-Loss"> </a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Absolute Error for each training example is the distance between the predicted and the actual values, irrespective of the sign. Absolute Error is also known as the L1 loss: $L = \left| y-f(x) \right|$. The cost is the Mean of these Absolute Errors (MAE), and it is more robust to outliers than MSE. Lets code the weights update for MAE. Recall, for the purposes of calculating the partial derivatives that $\frac{\partial }{\partial x_j}(x\to \left\| x_1 \right\|) = \frac{x_j}{\left| x_j \right|}$.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">MAE_loss_update</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
    <span class="n">m_deriv</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">b_deriv</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
        <span class="c1"># calculate partial derivatives</span>
        <span class="c1"># -x(y-mx-b) / |mx +b|</span>
        <span class="n">m_deriv</span> <span class="o">+=</span> <span class="p">(</span><span class="o">-</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">m</span><span class="o">*</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span><span class="n">b</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">m</span><span class="o">*</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="n">b</span><span class="p">))</span>
        
        <span class="c1"># -(y-mx-b) / |mx +b|</span>
        <span class="n">b_deriv</span> <span class="o">+=</span> <span class="o">-</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">m</span><span class="o">*</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span><span class="n">b</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">m</span><span class="o">*</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="n">b</span><span class="p">))</span>
    
    <span class="n">m</span> <span class="o">-=</span> <span class="p">(</span><span class="n">m_deriv</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">N</span><span class="p">))</span> <span class="o">*</span> <span class="n">learning_rate</span>
    <span class="n">b</span> <span class="o">-=</span> <span class="p">(</span><span class="n">b_deriv</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">N</span><span class="p">))</span> <span class="o">*</span> <span class="n">learning_rate</span>

    <span class="k">return</span> <span class="n">m</span><span class="p">,</span> <span class="n">b</span>   
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">MAE</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">m</span> <span class="o">=</span> <span class="mf">0.0001</span>
<span class="n">b</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">500</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
        <span class="n">loss</span> <span class="o">+=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span><span class="n">m</span><span class="o">*</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span><span class="n">b</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">/=</span> <span class="nb">float</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
    <span class="n">MAE</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">MAE_loss_update</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">lr</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br /></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="2.-Mean-Absolute-Error-Loss">2. Mean Absolute Error Loss<a class="anchor-link" href="#2.-Mean-Absolute-Error-Loss"> </a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The Huber loss combines the best properties of MSE and MAE. It is quadratic for smaller errors and is linear otherwise (and similarly for its gradient). It is identified by its delta parameter:
$$
L_\delta = \left\{\begin{matrix}
frac{1}{2}(y-f(x))^2,  &amp; if \left | y-f(x) \right | \leq \delta\\ 
\delta\left | y-f(x) \right | -\frac{1}{2}\delta^2 &amp; otherwise 
\end{matrix}\right.
$$</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">Huber_update</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">delta</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
    <span class="n">m_deriv</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">b_deriv</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
        <span class="c1"># derivative of quadratic for small values and of linear for large values</span>
        <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">m</span><span class="o">*</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">delta</span><span class="p">:</span>
            <span class="n">m_deriv</span> <span class="o">+=</span> <span class="o">-</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="p">(</span><span class="n">m</span><span class="o">*</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span>
            <span class="n">b_deriv</span> <span class="o">+=</span> <span class="o">-</span> <span class="p">(</span><span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="p">(</span><span class="n">m</span><span class="o">*</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">m_deriv</span> <span class="o">+=</span> <span class="n">delta</span> <span class="o">*</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="p">((</span><span class="n">m</span><span class="o">*</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span> <span class="o">-</span> <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">/</span> <span class="nb">abs</span><span class="p">((</span><span class="n">m</span><span class="o">*</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span> <span class="o">-</span> <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">b_deriv</span> <span class="o">+=</span> <span class="n">delta</span> <span class="o">*</span> <span class="p">((</span><span class="n">m</span><span class="o">*</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span> <span class="o">-</span> <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">/</span> <span class="nb">abs</span><span class="p">((</span><span class="n">m</span><span class="o">*</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span> <span class="o">-</span> <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    
    <span class="c1"># We subtract because the derivatives point in direction of steepest ascent</span>
    <span class="n">m</span> <span class="o">-=</span> <span class="p">(</span><span class="n">m_deriv</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">N</span><span class="p">))</span> <span class="o">*</span> <span class="n">learning_rate</span>
    <span class="n">b</span> <span class="o">-=</span> <span class="p">(</span><span class="n">b_deriv</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">N</span><span class="p">))</span> <span class="o">*</span> <span class="n">learning_rate</span>

    <span class="k">return</span> <span class="n">m</span><span class="p">,</span> <span class="n">b</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">Huberloss</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">m</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="n">b</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.0001</span>
<span class="n">delta</span> <span class="o">=</span> <span class="mi">20</span>
<span class="k">for</span> <span class="nb">iter</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">500</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">m</span><span class="o">*</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">delta</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">+=</span> <span class="p">((</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">m</span> <span class="o">*</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">+=</span> <span class="n">delta</span> <span class="o">*</span> <span class="nb">abs</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">m</span> <span class="o">*</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">delta</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">N</span>
        
    <span class="n">loss</span> <span class="o">/=</span> <span class="nb">float</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
    <span class="n">Huberloss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">Huber_update</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">delta</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Binary-Classification-Loss-Functions">Binary Classification Loss Functions<a class="anchor-link" href="#Binary-Classification-Loss-Functions"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Binary Classification refers to assigning an object into one of two classes. This classification is based on a rule applied to the input feature vector. For example, classifying an email as spam or not spam based on, say its subject line, is binary classification. Lets illustrate these binary classification loss functions on the Breast Cancer dataset. A greater value of entropy for a probability distribution indicates a greater uncertainty in the distribution. Likewise, a smaller value indicates a more certain distribution. This makes binary cross-entropy suitable as a loss function â€“ you want to minimize its value. We use binary cross-entropy loss for classification models which output a probability p. Then, the cross-entropy loss for output label y (can take values 0 and 1) and predicted probability p is defined as:
$$
L = -y*log(p) - (1-y)*log(1-p) = \left\{\begin{matrix}
-log(1-p) &amp; y=0\\ 
-log(p) &amp;  y=1
\end{matrix}\right.
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This is also called Log-Loss. To calculate the probability p, we can use the sigmoid function. Here, z is a function of our input features: $S(z) = \frac{1}{1+e^{-z}}$</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">math</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_breast_cancer</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MinMaxScaler</span>

<span class="n">cancer_dataset</span> <span class="o">=</span> <span class="n">load_breast_cancer</span><span class="p">()</span>
<span class="n">data1</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">cancer_dataset</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">cancer_dataset</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>
<span class="n">data1</span><span class="p">[</span><span class="s1">&#39;Class&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">cancer_dataset</span><span class="o">.</span><span class="n">target</span>
<span class="n">X1</span> <span class="o">=</span> <span class="n">data1</span><span class="p">[</span><span class="s1">&#39;worst area&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">X2</span> <span class="o">=</span> <span class="n">data1</span><span class="p">[</span><span class="s1">&#39;mean symmetry&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">data1</span><span class="p">[</span><span class="s1">&#39;Class&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">N1</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X1</span><span class="p">)</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>
<span class="n">X1</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X1</span><span class="p">)</span>
<span class="n">X2</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="1.-Binary-Cross-Entropy-Loss">1. Binary Cross Entropy Loss<a class="anchor-link" href="#1.-Binary-Cross-Entropy-Loss"> </a></h4>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">BCE_update</span><span class="p">(</span><span class="n">m1</span><span class="p">,</span> <span class="n">m2</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
    <span class="n">m1_deriv</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">m2_deriv</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">b_deriv</span> <span class="o">=</span> <span class="mi">0</span>
    
    <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X1</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
        <span class="n">m1_deriv</span> <span class="o">+=</span> <span class="o">-</span><span class="n">X1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">s</span> <span class="o">-</span> <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="n">m2_deriv</span> <span class="o">+=</span> <span class="o">-</span><span class="n">X2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">s</span> <span class="o">-</span> <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="n">b_deriv</span> <span class="o">+=</span> <span class="o">-</span><span class="p">(</span><span class="n">s</span> <span class="o">-</span> <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    
    <span class="n">m1</span> <span class="o">-=</span> <span class="p">(</span><span class="n">m1_deriv</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="n">n</span><span class="p">))</span> <span class="o">*</span> <span class="n">learning_rate</span>
    <span class="n">m2</span> <span class="o">-=</span> <span class="p">(</span><span class="n">m2_deriv</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="n">n</span><span class="p">))</span> <span class="o">*</span> <span class="n">learning_rate</span>
    <span class="n">b</span> <span class="o">-=</span> <span class="p">(</span><span class="n">b_deriv</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="n">n</span><span class="p">))</span> <span class="o">*</span> <span class="n">learning_rate</span>
    
    <span class="k">return</span> <span class="n">m1</span><span class="p">,</span> <span class="n">m2</span><span class="p">,</span> <span class="n">b</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">BCE</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">m1</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">m2</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">b</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.0001</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">N1</span><span class="p">):</span>
        <span class="n">p</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">m1</span><span class="o">*</span><span class="n">X1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">m2</span><span class="o">*</span><span class="n">X2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">b</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">+=</span> <span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">p</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">+=</span> <span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">/=</span> <span class="nb">float</span><span class="p">(</span><span class="n">N1</span><span class="p">)</span>
    <span class="n">BCE</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="n">m1</span><span class="p">,</span> <span class="n">m2</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">BCE_update</span><span class="p">(</span><span class="n">m1</span><span class="p">,</span> <span class="n">m2</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">lr</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="2.-Hinge-Loss">2. Hinge Loss<a class="anchor-link" href="#2.-Hinge-Loss"> </a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Hinge loss is primarily used with Support Vector Machine (SVM) Classifiers with class labels -1 and 1. So make sure you change the label of the â€˜Malignantâ€™ class in the dataset from 0 to -1. Hinge Loss not only penalizes the wrong predictions but also the right predictions that are not confident. Hinge loss for an input-output pair (x, y) is given as:
$$
L = max(0, 1-y*f(x))
$$</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">Hinge_update</span><span class="p">(</span><span class="n">m1</span><span class="p">,</span> <span class="n">m2</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
    <span class="n">m1_deriv</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">m2_deriv</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">b_deriv</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X1</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="n">m1</span><span class="o">*</span><span class="n">X1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">m2</span><span class="o">*</span><span class="n">X2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">m1_deriv</span> <span class="o">+=</span> <span class="o">-</span><span class="n">X1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">m2_deriv</span> <span class="o">+=</span> <span class="o">-</span><span class="n">X2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">b_deriv</span> <span class="o">+=</span> <span class="o">-</span><span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    
    <span class="n">m1</span> <span class="o">-=</span> <span class="p">(</span><span class="n">m1_deriv</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">N</span><span class="p">))</span> <span class="o">*</span> <span class="n">learning_rate</span>
    <span class="n">m2</span> <span class="o">-=</span> <span class="p">(</span><span class="n">m2_deriv</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">N</span><span class="p">))</span> <span class="o">*</span> <span class="n">learning_rate</span>
    <span class="n">b</span> <span class="o">-=</span> <span class="p">(</span><span class="n">b_deriv</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">N</span><span class="p">))</span> <span class="o">*</span> <span class="n">learning_rate</span>

    <span class="k">return</span> <span class="n">m1</span><span class="p">,</span> <span class="n">m2</span><span class="p">,</span> <span class="n">b</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">Hinge</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">m1</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">m2</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">b</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1000</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">N1</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="n">m1</span><span class="o">*</span><span class="n">X1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">m2</span><span class="o">*</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">+=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="n">m1</span><span class="o">*</span><span class="n">X1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">m2</span><span class="o">*</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">/=</span> <span class="nb">float</span><span class="p">(</span><span class="n">N1</span><span class="p">)</span>
    <span class="n">Hinge</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="n">m1</span><span class="p">,</span> <span class="n">m2</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">Hinge_update</span><span class="p">(</span><span class="n">m1</span><span class="p">,</span> <span class="n">m2</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="jackhmiller/My-DS-Blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/My-DS-Blog/2017/11/13/Losses.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/My-DS-Blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/My-DS-Blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/My-DS-Blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>050-709-2944 | Jack.harris.miller@gmail.com</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/jackhmiller" target="_blank" title="jackhmiller"><svg class="svg-icon grey"><use xlink:href="/My-DS-Blog/assets/minima-social-icons.svg#github"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
