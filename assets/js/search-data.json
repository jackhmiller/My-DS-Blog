{
  
    
        "post0": {
            "title": "Proving Proximal Gradient Method's Convergence Rate and a Code Demonstration",
            "content": "Proximal Gradient Descent . A proximal algorithm is an algorithm for solving a convex optimization problem that uses the proximal operators of the objective terms. It is called such since it consists of a gradient step followed by a proximal mapping. There are three main benefits to the application of proximal algorithms: . They work under extremely general conditions, including cases where the functions are nonsmooth and extended real-valued | | They can be fast, since there can be simple proximal operators for functions that are otherwise challenging to handle in an optimization problem | | They are amenable to distributed optimization, so they can be used to solve very large scale problems | | . The proximal operator is defined as $$ prox_f(x) = argmin left { f(u) + frac{1}{2} left | u-x right |^2: u in mathbb{R}^n right }, forall x in mathbb{R}^n $$ with the goal being to $$minimize left { f(u) + h(u): u in mathbb{R}^n right }$$ where h is a proper lower semi-continuous function and f is a smooth convex function on dom(h). . Some important assumptions before we begin: . We assume that f has L-Lipschitz continuous gradient, i.e., $$ left | bigtriangledown f(x) - bigtriangledown f(y) right | leq L left | x-y right |, forall x, y in dom(h)$$ and hence for every $x, y in dom(h)$, $$ f(x) leq l_f(x; y) + frac{L}{2} left | x-y right |^2$$ where $l_f(x; y) := f(y) + left langle bigtriangledown f(y), x-y right rangle$. . Recal that PGM with a constant prox stepsize is recursive in nature and iterates according to : $$x_{k+1}=prox_{ lambda h}(x_k- lambda nabla f(x_k)).$$ . Let&#39;s get started! . First, we will derive a single iteration of PGM and prove that it is strong convex. . $$ x_{k+1} = argmin left { h(u) + frac{1}{2} left | u-(x_k- bigtriangledown f(x_k)) right |^2 right }$$ . $$ x_{k+1} = argmin left { f(x_k) + left langle bigtriangledown f(x_k), u-x_k right rangle +h(u) + frac{1}{2} left | x-x_k right |^2 right }$$ . $$x_{k+1}= argmin left { ell_f(u;x_k)+h(u) + frac{1}{2 lambda}||u-x_k||^2 right }, $$ . And proving strong convexity $ left langle bigtriangledown h(u) - bigtriangledown h(x), u - x right rangle geq lambda left | u-x right |^{2}$: . $$ left langle prox_{ lambda h}(u) - prox_{ lambda h}(x), (u- frac{1}{ lambda} bigtriangledown h(u)) -(x- frac{1}{ lambda} bigtriangledown h(x)) right rangle geq left | prox_{ lambda h}(u)-prox_{ lambda h}(x) right |^{2} $$ . $$ left langle (u - frac{1}{ lambda} bigtriangledown h(u)) - (x - frac{1}{ lambda} bigtriangledown h(x)), (u- frac{1}{ lambda} bigtriangledown h(u)) -(x- frac{1}{ lambda} bigtriangledown h(x)) right rangle geq left | (u - frac{1}{ lambda} bigtriangledown h(u)) - (x - frac{1}{ lambda} bigtriangledown h(x)) right |^{2} $$ . Using the definition of $x_{k+1}$ and the strong convexity, we obtain upon rearranging terms that: . $$ h(x_{k+1}) leq h(x) + left langle - bigtriangledown f(u), x^{k+1}-x right rangle + frac{1}{2} left | u-x right |^2 - frac{1}{2} left | u-x^{k+1} right |^2 - frac{1}{2} left | x^{k+1}-x right |^2 $$ . Due to the Lipschitz continuity: $$ f(x_{k+1}) leq f(u) + left langle - bigtriangledown f(u), u-x^{k+1} right rangle + frac{1}{2} left | u-x_{k+1} right |^2 $$ . Adding the two: $$ f(x_{k+1}) + h(x_{k+1}) leq f(u) + h(x) + left langle bigtriangledown f(u), u-x right rangle - frac{1}{2} left | x_{k+1}-x right |^2 + frac{1}{2} left | u-x right |^2 $$ . Using definition for $ ell_f(u;x_k)$ $$ ell_f(u;x_k)+h(u) + frac{1}{2}||u-x_k||^2 geq ell_f(x_{k+1};x_k)+h(x_{k+1})+ frac{1}{2}||x_{k+1}-x_k||^2 + frac{1}{2}||u-x_{k+1}||^2$$ . Similarly for any x in int(dom(f)): $$ f(x_*) leq f(x) + left langle bigtriangledown f(x), x_*-x right rangle + frac{1}{2 lambda} left | x_*-x right |^2 $$ It holds that $$ (f+h)(x)-(f+h)(x_*) geq frac{1}{2 lambda} left | x-x_* right |^2 $$ . Consider $$ g(u) = f(x_{k+1})+ left langle bigtriangledown f(x_{k+1}), u-x_{k+1} right rangle + g(u)+ frac{1}{2 lambda} left | u-x_{k+1} right |^2$$ . $ x_* = argmin_g(u) $ $$ g(x)-g(x_*) geq frac{1}{2 lambda} left | x-x_* right |^2 $$ . Since $$ g(x_*) = f(x_{k+1}) + left langle bigtriangledown f(x_{k+1}),x_*-x_{k+1} right rangle + frac{1}{2 lambda} left | x_*-x_{k+1} right |^2 + h(x_*) $$ $$ geq f(x_*)+h(x_*) = (f+h)(x_*) $$ . This implies that $$ h(x_{k+1})-(f+h)(x_*) geq frac{1}{2 lambda} left | x_{k+1}-x_* right |^2 $$ . Plugging for g(u) into above inequality . $$ f(x_{k+1}) + left langle bigtriangledown f(x_{k+1}), x-x_{k+1} right rangle + h(x)+ frac{1}{2 lambda} left | x-x_{k+1} right |^2 -(f+h)(x_*) geq frac{1}{2 lambda} left | x-x_* right |^2$$ . Which is equal to . $$ (f+h)(x_{k+1})-(f+h)(x_*) geq frac{1}{2 lambda} left | x_{k+1}-x_* right |^2 - frac{1}{2 lambda} left | x-x_{k+1} right |^2 +f(x_{k+1}) + ell_f(x;x_{k+1}) $$ . $$(f+h)(x_*)+ frac{1}{2 lambda}||x_k-x_*||^2 geq (f+h)(x_{k+1})+h(x_{k+1})+ frac{1}{2 lambda}||x_{k+1}-x_*||^2$$ . Using $$ frac{1}{2 lambda}((f+h)(x_*)-(f+h)(x_{k+1})) geq left | x_*-x_{k+1} right |^2 - left | x_*-x_k right |^2 + frac{1}{2 lambda} ell_f(x_*,x_k)$$ . $$ frac{1}{2 lambda}((f+h)(x_*)-(f+h)(x_{k+1})) geq left | x_*-x_{k+1} right |^2 - left | x_*-x_k right |^2 $$ . Sum over all n from 0 to k to obtain: $$ frac{1}{2 lambda} sum_{}^{k}(f+h)(x_*)-(f+h)(x_{k+1}) geq left | x_*-x_k right |^2- left | x_*-x_0 right |^2 $$ . Thus $$ sum_{}^{k}((f+h)(x_{k+1})-(f+h)(x_*)) leq frac{1}{2 lambda} left | x_*-x_0 right |^2- frac{1}{2 lambda} left | x_*-x_k right |^2 leq frac{1}{2 lambda} left | x_*-x_0 right |^2 $$ . Given the monotonicity of $(f+h)(x_n)$ for $n geq 0$ $$ k((f+h)(x_k)-(f+h)(x_*)) leq sum_{}^{k}((f+h)(x_{k+1})-(f+h)(x_*)) leq frac{1}{2 lambda} left | x_*-x_0 right |^2 $$ . Thus $$ sum_{i=1}^k (f+h)(x_i)-k(f+h)(x_*) leq frac{||x_0-x_*||^2}{2 lambda} $$ . Proving PGM has the descent property: . $$(f+h)(x_k) geq (f+h)(x_{k+1}), forall k geq 0 $$ . $$ frac{1}{2 lambda}((f+h)(x_*)-(f+h)(x_{k+1})) geq left | x_*-x_{k+1} right |^2 - left | x_*-x_k right |^2 + frac{1}{2 lambda} ell_f(x_*,x_k)$$ . Along with the relationship: $$ left | x_{k+1} -x_* right | leq left | x_k-x_* right |$$ . It follows that: $$ (f+h)(x_*)-(f+h)(x_{k+1}) leq (f+h)(x_*)-(f+h)(x_{k}) leq 0$$ . Thus for all k $ geq 0$ $$(f+h)(x_{k+1}) leq (f+h)(x_{k}) $$ . Finally, given the above: $$ k((f+h)(x_k)-(f+h)(x_*)) leq sum_{}^{k}((f+h)(x_{k+1})-(f+h)(x_*)) leq frac{1}{2 lambda} left | x_0-x_* right |^2 $$ Consequently $$ (f+h)(x_i)-(f+h)(x_*) leq frac{1}{k2 lambda}||x_0-x_*||^2 $$ . Hence we obtain the $O( frac{1}{k})$ convergence rate . . Code Example . Here we will employ proximal gradient descent with stochastic schemes. In general, when the loss function we are trying to minimize can be wwritten in the form $ sum_{i=1}^{m}g_i( theta )$ where each $g_i( theta)$ is the loss sample at i, and the training time is long, then stochastic schemes should be considered. We will optimize $$f( theta) = underset{ theta in mathbb{R}^d}{min} frac{1}{m} sum_{i=1}^{m} left [ log(1+exp(x_i theta)) -y_ix_i theta right ] + lambda left | theta right |_1$$ We decompose $f( theta)$ into a convex and differentiable function g and a convex but not differentiable function h: $$ g( theta) = frac{1}{m} sum_{i=1}^{m}log(1+exp(x_i theta)) $$ $$ h( theta) = frac{1}{m} sum_{i=1}^{m} -y_ix_i theta + lambda left | theta right |_1$$ . The data we are using is from the classic MNIST machine learning dataset. There are two classes, 0 and 1, and we have a total of 14,780 images; a training set of 12,665 and a test set of 2,115. Each image is 28x28. Each image is vectorized and stacked to form a training and test matrix, with the label appended to the last column of each matrix. Thus, our classifier will learn $ theta$ on the train set to predict the labels for the test set. . import numpy as np from sklearn.metrics import accuracy_score . x_train = train[:, :-1] y_train = train[:, -1 :] x_test = test[:, :-1] y_test = test[:, -1 :] x = x_train y = y_train . def predict_labels(X, weights): return 1/(1+np.exp(-X.dot(weights))) def soft_threshold(x,t): pos = np.maximum(x - t, 0) neg = np.minimum(x + t, 0) return pos+neg def log_loss(X, theta): return np.sum(np.log(1 + np.exp(X.dot(theta)))) / X.shape[0] def h(X, y, lam=10, lr=0.01): return (1/len(X))*(-y.T.dot(X)) + lam*lr def evaluate_gradient(X, theta, y=None): return np.sum((X*np.exp(X.dot(theta))) / (1 + np.exp(X.dot(theta))), axis=0)/m . n = 100 lam = 10 lr= 0.01 max_iters=1000 tol= 1e-3 N, D = x.shape theta_current = np.zeros(shape=(D, 1)) losses = [log_loss(x, theta_current)] thetas = [theta_current] iterations = 1 while (loss &gt; tol) or (iterations &gt; max_iters): theta_current = thetas[-1] # Stochastic number_of_rows = x.shape[0] random_indices = np.random.choice(number_of_rows, size=n, replace=False) x_temp, y_temp = x[random_indices, :], y[random_indices, :] for it in range(n): # Proximal GD grad = evaluate_gradient(x_temp, theta_current).reshape(-1,1) theta_new_grad = theta_current - (lr * grad) theta_new = soft_threshold(theta_new_grad, h(x_temp, y_temp)) theta_current = theta_new loss = log_loss(x, theta_current) losses.append(loss) thetas.append(theta_current) iterations += 1 . n = 100 lam = 10 lr= 0.01 max_iters=1000 tol= 1e-5 N, D = x.shape theta_current = np.zeros(shape=(D, 1)) loss_1 = log_loss(x, theta_current) losses = [loss_1] thetas = [theta_current] iterations = 1 #while losses[-1] &gt; tol: for i in range(200): theta_current = thetas[-1] grad = evaluate_gradient(x, theta_current).reshape(-1,1) theta_new_grad = theta_current - (lr * grad) theta_new = soft_threshold(theta_new_grad, h(x, y).T) theta_current = theta_new loss = log_loss(x, theta_current) losses.append(loss) thetas.append(theta_current) #iterations += 1 . predict_labels(x, thetas[-1]) accuracy_score(y_test, predict_labels(x_test, thetas[-1])) . Overall, this stochastic implementation achieves an accuracy of 93.76 on the training set. .",
            "url": "https://jackhmiller.github.io/My-DS-Blog/2020/08/05/PGD.html",
            "relUrl": "/2020/08/05/PGD.html",
            "date": " • Aug 5, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Alternating Method of Multipliers- Theory and Industry Example Application",
            "content": "In this blog post we will solve the following optimization problem using the scaled form of alternating direction method of multipliers (ADMM). . $$ min frac{1}{2} left | Ax-b right |_{2}^{2} + lambda_1 left | x right |_1 + frac{ lambda_2}{2} left | x right |_{2}^{2}min frac{1}{2} left | Ax-b right |_{2}^{2} + lambda_1 left | x right |_1 + frac{ lambda_2}{2} left | x right |_{2}^{2}$$ . Background on ADMM . The alternating direction method of multipliers (ADMM) is an algorithm that solves convex optimization problems by breaking them into smaller pieces, each of which are then easier to handle. Namely, it is intended to blend the decomposability of dual ascent with the superior convergence properties of the method of multipliers. The original paper can be found here: https://stanford.edu/~boyd/papers/pdf/admm_distr_stats.pdf. . ADMM received lots of attention a few years ago due to the tremendous proliferation and subsequentdemand from large-scale and data-distributed machine learning applications. ADMM a fairly simple computational method for optimization proposed in 1970s. It stemmed from the augmented Lagrangian method (also known as the method of multipliers) dating back to late 1960s. The theoretical aspects of ADMM have been studied since the 1980s, and its global convergence was established in the literature (Gabay, 1983; Glowinski &amp; Tallec, 1989;Eckstein &amp; Bertsekas, 1992). As reviewed in the comprehensive paper (Boyd et al., 2010), with the ability of dealing with objective functions separately and synchronously , ADMM turned out to be a natural fit in the field of large-scale data-distributed machine learning and big-data related optimization, and therefore received significant amount of attention beginning in 2015. Considerable work was conducted thereafter. . On the theoretical side, ADMM was shown to have an O(1/N) rate of convergence for convex problems. . The algorithm solves the problem in the form: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; minimize $f(x) + g(z)$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; subject to $Ax+ Bz = c$ . with variables $x in R^n$ and $z in R^m$, where $A in R^{pxn}$, $B in R^{pxm}$ and $C in R^{p}$. . The only difference from the general linear equality-constrained problem is that the variable x has been split into two parts, called x and z, with the objective function separable across this splitting. The optimal value of the problem is now denoted by: $$ p^* = inf left { f(x) + g(x) | Ax + Bz = c right } $$ Which forms the augmented Lagrangian: $$ L_p(x, z, y) = f(x) + g(z) + y^T(Ax+Bz-c) + frac{ rho}{2} left | Ax + Bz -c right |_{2}^{2} $$ . Finally, we have our ADMM which consists of the following iterations: $$ x^{k+1} = underset{x}{argmin} L_ rho(x, z^k, y^k)$$ $$ z^{k+1} = underset{x}{argmin} L_ rho(x^{k+1}, z, y^k) $$ $$ y^{k+1} = y^k + rho(Ax^{k+1} +Bz^{k+1}-c) $$ $$s.t. rho&gt;0$$ . The algorithm is very similar to dual ascent and the method of multipliers: it consists of an x-minimization step, a z-minimization step, and a dual variable update. As in the method of multipliers, the dual variable update uses a step size equal to the augmented Lagrangian parameter. However, while with the method of multipliers the augmented Lagrangian is minimized jointly with respect to the two primal variables, in ADMM, on the other hand, x and z are updated in an alternating or sequential fashion, which accounts for the term alternating direction. . Simple examples show that ADMM can be very slow to converge to high accuracy. However, it is often the case that ADMM converges to modest accuracy—sufficient for many applications—within a few tens of iterations. This behavior makes ADMM similar to algorithms likethe conjugate gradient method, for example, in that a few tens of iterations will often produce acceptable results of practical use. However, the slow convergence of ADMM also distinguishes it from algorithms such as Newton’s method (or, for constrained problems, interior-point methods), where high accuracy can be attained in a reasonable amount of time. While in some cases it is possible to combine ADMM with a method for producing a high accuracy solution from a low accuracy solution, in the general case ADMM will be practically useful mostly in cases when modest accuracy is sufficient. Fortunately, this is usually the case for large-scale industrial applications. Also, in the case of machine learning problems, solving a parameter estimation problem to very high accuracy often yields little to no improvement in actual prediction performance, the real metric of interest in applications. . Our Optimization Problem . First we will write the augmented Lagrangian function (the scaled form) and drive the ADMM updates. . Scaled form of the augmented Lagrangian . $$ L(x, z, u: rho) = min frac{1}{2} left | Ax-b right |_{2}^{2} + lambda_{1} left | z right |_{1} + frac{ lambda_{2}}{2} left | z right |_{2}^{2} + frac{ rho}{2} left | x-z+ w right |_{2}^{2} + frac{ rho}{2} left | w right |_{2}^{2} $$ . $$ x_{k} = underset{x}{argmin} frac{1}{2} left | Ax-b right |_{2}^{2} + frac{ rho}{2} left | x-z_{k-1}+ w_{k-1} right |_{2}^{2} $$ . $$ x_k = ((A^TA+ rho I))^{-1}( rho(z_{k-1}-w_{k-1})+A^Tb) $$ . $$ z_{k} = underset{z}{argmin} ( lambda_{1} left | z right |_{1} + frac{ lambda_{2}}{2} left | z right |_{2}^{2}) + frac{ rho}{2} left | x_{k}-z+ w_{k-1} right |_{2}^{2} $$ . if $z&gt;0$ $$ z_{k} = frac{ rho(w_{k-1}+x_{k})}{ lambda_2- rho}- lambda_1$$ . if $z&lt;0$ $$ z_{k} = frac{ rho(w_{k-1}+x_{k})}{ lambda_2- rho}+ lambda_1$$ . $$ w_{k} = w_{k-1} + x_k -z_k $$ . Practical Application . Now, we will implement a regression algorithm using our augmented lagrangian. The dataset is the performance decay over time of a ship&#39;s Gas Turbine (GT) compressor. We split our test and train data 20:80. The range of decay of compressor has been sampled with a uniform grid of precision 0.001 so to have a good granularity of representation. For the compressor decay state discretization the kMc coefficient has been investigated in the domain [0.95,1]. Ship speed has been investigated sampling the range of feasible speed from 3 knots to 27 knots with a granularity of representation equal to tree knots. A series of measures (13 features) which indirectly represents of the state of the system subject to performance decay has been acquired and stored in the dataset over the parameter’s space. . The A 13-feature vector containing the GT measures at steady state of the physical asset: . Lever position (lp) | Ship speed (v) | Gas Turbine (GT) shaft torque (GTT) | GT rate of revolutions (GTn) | Gas Generator rate of revolutions (GGn) | Port Propeller Torque (Tp) | Hight Pressure (HP) Turbine exit temperature (T48) | GT Compressor outlet air temperature (T2) | HP Turbine exit pressure (P48) | GT Compressor outlet air pressure (P2) | GT exhaust gas pressure (Pexh) | Turbine Injection Control (TIC) | Fuel flow (mf) | GT Compressor decay state coefficient | . import pandas as pd import numpy as np import matplotlib.pyplot as plt %matplotlib inline from scipy.io import loadmat . ship_test = pd.read_csv(&quot;Shiptest-2.csv&quot;, header=None) ship_train = pd.read_csv(&quot;Shiptrain-2.csv&quot;, header=None) . X_train = ship_train.iloc[:,:-1] y_train = ship_train.iloc[:,-1:] X_test = ship_test.iloc[:,:-1] y_test = ship_test.iloc[:,-1:] . lambda_1 = 0.1 lambda_2 = 0.9 . iterations = 100 rho = 0.1 w = 0 z = np.random.rand(13).reshape(-1,1) A = X_train.values b = y_train.values loss = [] for i in range(iterations): x = (np.linalg.inv(np.dot(A.T,A) + (rho*np.eye(13)))).dot(rho*(z-w) + np.dot(A.T, b)) for i in range(len(z)): if np.sign(z[i])&gt;0: z = rho*(w+x)/(lambda_2-rho) - lambda_1 else: z = rho*(w+x)/(lambda_2-rho) + lambda_1 w = w + rho*(x-z) loss.append(np.sum(0.5*(np.dot(A,x)-b)**2)) . plt.figure(figsize=(10,5)) plt.plot(loss) plt.title(&#39;Obj function vs iterations&#39;) plt.xlabel(&#39;Number of iterations&#39;) plt.ylabel(&#39;Obj&#39;) . Text(0, 0.5, &#39;Obj&#39;) . print(&quot;Coefficients&quot;) x . Coefficients . array([[-5.05840315e-02], [ 1.03456745e-02], [ 1.21482112e-05], [-2.86479882e-04], [-1.49933306e-05], [-1.52302544e-03], [-9.13559793e-04], [ 1.36474482e-03], [ 1.85080531e-01], [ 5.00636758e-02], [ 6.72727845e-01], [-1.62063399e-04], [-1.76737055e-01]]) . Sum absolute errors . abs_ms_errors = [] abs_errors = [] for A, b in zip(X_test.values, y_test.values): abs_ms_errors.append(abs(np.sum(0.5*(np.dot(A,x)-b)**2))) abs_errors.append(abs(np.sum(np.dot(A,x)-b))) print(&quot;Sum absolute mean-squared errors:&quot;, round(sum(abs_ms_errors), 2)) print(&quot;Sum absolute errors:&quot;, round(sum(abs_errors), 2)) . Sum absolute mean-squared errors: 0.14 Sum absolute errors: 21.1 .",
            "url": "https://jackhmiller.github.io/My-DS-Blog/2020/04/20/ADMM.html",
            "relUrl": "/2020/04/20/ADMM.html",
            "date": " • Apr 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "The Hypothesis Space and Representer Theorem",
            "content": "Introduction . When confronted with a machine learning task, probably some of the first questions a data scientist will ponder is: 1) What is our instance space? What is our label space? How do we define success? Are there computational issues associated with the task/data? How can we achieve generalization without over-fitting? What kind of features are we going to use? After answering these questions, a data scientist or team will settle on a learning algorithm and an appropriate loss function/evaluation metric. Unfortunately, in my experience, what is lacking from the initial discussion regarding data science tasks is a a seemingly trivial discuss of the hypothesis space. Since it is more of a theoretical framework as opposed to a tangible manifestation of a learning algorithm or code base, it is not discussed or included in a project workplan. However, ultimately a thorough discussion of the hypothesis space and its constraints is a vital component in initially framing a modeling task, whether machine learning or deep learning, and driving a project towards successful and reproducible results. When learning a model $g(x)$, we must choose which find of function we expect $g(x)$ to be. For example, consider an unput with four binary features $(x = left [ x_1x_2x_3x_4 right ]; x in left { 0,1 right })$ and an unknown function $f(x)$ that returns y. For four features there are 16 possible instances. Thus in the binary classification task there are $2^16$ possible functions to describe our data. Therefore, we are confronted with two issues. First, without without restrictions on the set of functinos $g(x)$ learning is not feasible. Second, even if it were feasible, assuming there is a deterministic target hypothesis $g_0$ relating x to y, chosing a $g$ from a large enough space of functions, we certainly would achieve very good performance on training data. If fact, if we wanted to de could always make the training error exactly zero. A simple hypothesis would do just that: $$ g(x) = left { begin{matrix} +1 &amp; i in left { 1, 2, ...m right } x_i = x, y_i =1 -1 &amp; otherwise end{matrix} right. $$ Obviously however this hypothesis would not generalize to data outside of our test set. Rather, we just created a model that simply memorizes labels. So all we ended up doing is minimizing the empirical error at the expense of the true error, which is called overfitting. What a good model does is minimize the true error $ Re (g) = E_D left [ L(g(x), y) right ]$ where the expectation is taken over (x,y) pairs drawn from some distribution D. . The Formal Model . The above considerations give rise to the formal model of our learning prolem. Let $X$ be the input space, $y$ be the output space, D an unknown probability distribution on $Xxy$ and let $g$ our hypothesis space be a class of functions $g: X to y $. For many practical algorithms, it is often observed that the true error is not too far from the empirical error. Real algorithms are not as ill behaved as the label memorization algorithm above, which was an extreme example. The key observation is the dicrepancy between the $R_D$ and $R_{emp}$ is related to the size of $g$, that is, how flexible our model is in terms of the size and shape of the hypothesis space. What made the label memorization algorithm so bad was that the class of possible hypotheses was so huge, permitting us to fit anything we wanted. That is an invitation for disastrous overfitting. Learning algorithms used in practice usually have access to a much more limited set of hyoptheses (such as linear discriminators in the case of the perceptron, SVM etc..), so they have less opportunity to overfit. One way to do this is by explicitly restricting the hypothesis space $g$ to “simple” hypotheses, as in Structural Risk Minimization.Another way is to introduce a penalty functional $ Omega $ that somehow measures the complexity of each hypothesis f, and to minimize the sum $$ R_reg(f) = R_{emp}(f) + Omega (f)$$. We can now begin to construct our hypothesis class g. Naturally we want G to be a linear function space in te sense that for any $f in G$ and any real number $ lambda$, $ lambda f$ is in G. Also, for any $f_1, f_2 in g$ the sum of $f_1 + f_2$ is in G. We also want G to be related to te regularizer $ Omega$. We define a norm on g and set $ Omega left [ f right ] = left | f right |^2$. We further require that the norm be derived from an inner product. This leads us to the notion of a Hilbert Space, or a linear inner product space, which will allow us to make the connection between the abstract structure of our hypothesis space g and what the elements of g actually are (discriminant functions or regressors). Within a Hilbert Space, for any such $ f in g$ any x has a corresponding $f_x in g$ such that x(f) = $ left langle f_x, x right rangle$. Therefore, for any $x in X$ we have a special function $k_x$ in our Hilbert Space called the representer and satisfying $f(x) = left langle k_x, f right rangle forall f in g$. Ultimately, we can rewrite our entire regularized risk minimization problem as: $$ hat{f} = arg underset{f in g}{min} left [ frac{1}{m} sum_{i=1}^{m}L( left langle k_x,f right rangle, y_i) + left langle f, f right rangle right ] $$ The hypothesis f only features in this equation in the form of inner producs with other functions in hypothesis space g. Once we know the form of the inner product and what the $k_x$ are, we can do everything we want with simple math. Moreover, anything outside of the span of $ left { k_x right }x in X$ is uninteresting since it does not affect what $f in g$ evaluated at any point of the input space is, so we can leave it out of the hypothesis space altogether. The result of the whole construction is just driven by the inner product $k(x, x&#39;) = left langle k_x, k_{x&#39;} right rangle$. This is called the kernel. In fact, we can reverse the whole procedure and construct g starting from the kernel. Given a positive definite function k on the input space X, we define g to be the minimal complete space of functions that includes all $ left { k_x right }x in X$ and that has an inner product define in a fashion above. This defines g uniquely, and formaly g is called the Reproducing Kernel Hilbert Space associated with kernel k. Finally. we have reduced the learning problem to that of defining the loss function Representer L and the kernel k. One more interesting point. By looking at our new formulation of our regularized risk minimization problem above, it is clear that $ hat{f}$ is going to be in the space of representers of the training data $k_1, k_2, k_3, ..., k_m$. We can tell because the loss term only depends on the inner products of f with $k_{x_1}, k_{x_2}, k_{x_3}, ..., k_{x_m}$ while the regularization term penalizes f in all directions. If f has any component orthogonal to the subspace spanned by $k_{x_1}, k_{x_2}, k_{x_3}, ..., k_{x_m}$, the loss term is not going to be sensitive to that component, but the regularization term will still penalize it. Hence ,the optimal f will be entirely contained in the subspace spanned by the representers. This is the meaning of the representer teorem and it means that the optimal hypothesis $ hat{f}$ can be expressed as $$ hat{f} = b + sum_{i=1}^{m} alpha_ik_{x_i} $$ for some real coefficients $ alpha_1, alpha_2, alpha_3... alpha_m$ and bais b. Or as it is better known, $$ hat{f} = b + sum_{i=1}^{m} alpha_ik(x_i, x) $$ .",
            "url": "https://jackhmiller.github.io/My-DS-Blog/2020/02/23/RKHS.html",
            "relUrl": "/2020/02/23/RKHS.html",
            "date": " • Feb 23, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Group Lasso",
            "content": "It is frequently the case when dealing with high dimensional datasets that there are more variables than observations but we only expect a small fragment of the variables to be truly meaningful. To compensate for such occurances and enhance our ability to generalize our resulting model, it is common to employ regularization techniques which are also used to reduce overfitting in other settings. The most common examples are L1 (Lasso) and L2 (Ridge) regularization. Lasso, in particular, causes sparsity for weights. It provides sparse solutions, because it will send to zero some of the β coefficients (the least related with the response variable). The effect of this penalization can be controlled using the λ parameter. A large λ value provides solutions where the penalization has a greater importance, and thus there are more zeros among the β coefficients. . However, there are many regression problems dealing with high dimensional data in which the covariates have a natural group structure, and it is desirable to have all coefficients within a group become nonzero (or zero) simultaneously. For example, in biostatistics it is common to deal with genetic datasets in which predictors are grouped into genetical pathways. In stock market analysis one can group companies from the same business segment. In climate data one can group different regions… And lasso provides individual sparse solutions, not group sparse. A more general leading example is when we have qualitative factors among our predictors. We typically code their levels using a set of dummy variables or contrasts, and would want to include or exclude this group of variables together. For such a scenario, we have a technique called the Group Lasso, which accounts for a natural grouped structure of predictors while causing sparsity for weights. . In the image above, we can see a visual depiction of the contours of the L1, group lasso, and L2 penalties respectively (Source: Ming Yuan, &quot;Model selection and estimation in regression with grouped variables,&quot; Statistical Methodology 68:1 (Feb 2006). Specifically, we can see how the group lasso (middle image) incorporates elements of both the L1 and L2 penalties. . Lets get into the math of the Group Lasso. Consider a linear regression model involving J groups of covariates, where j = 1,..., J, and the vector $ mathbb{Z}_j in mathbb{R}^{pj}$ represents the covariates of group j. Our goal is to predict a real valued response $Y in mathbb{R}$ based on the collection of covariates ($ mathbb{Z}_1, ..., mathbb{Z}_J $. A linear model for the regression function $ mathbb{E}(Y| mathbb{Z})$ $ sum_{j=1}^{J} mathbb{Z}_{j}^{T} theta_j$ where $ theta_j in mathbb{R}^{pj}$ represents a group of $p_j$ regression coefficients. . Given a collection of N samples $ left { (y_i, z_{i1}, z_{i2},..., z_{iJ}) right }_{i=1}^{N}$ the group lasso solves the convex problem: $$ underset{ theta_j in mathbb{R}^{pj}}{min} frac{1}{2} left | y- sum_{j=1}^{J} mathbb{Z}_{j}^{T} theta_j right |_{2}^{2} + lambda sum_{j=1}^{J} left | theta_j right |_2 $$ where $ left | theta_j right |_2$ is the euclidean norm of the vector $ theta_j$. . This is a group generalization of the lasso, with the properties: . depending on $ lambda geq 0$, either the entire vector $ theta_j$ will be zero, or all its elements will be nonzero | when $p_j=1$ (continuous variables), then we have $ left | theta_j right |_2 = left | theta_j right |$, so if all of the groups are singletons, the optimization problem reduces to ordinary lasso. | . We can solve the group lasso problem using block coordinate descent. Here is a proof showing that we can solve it iteratively, for $j = 1 · · · , J$: . We start with our optimization problem: $$ underset{ theta_j in mathbb{R}^{pj}}{min} frac{1}{2} left | r_j- mathbb{Z}_j^T theta_j right |_2^2 + lambda left | theta_j right |_2 $$ Where $r_j = y - sum_{k neq j}^{}Z_k^T theta_k$ . With a bit of manipulation we get $$ -Z_j^T(y- sum_{j=1}^{J}Z_j theta_j)+ lambda(s( frac{ theta_j}{ left | theta_j right |_{2}})) $$ . Using the definition of $r_j$ we can solve for $ theta$: $$ theta_j = (Z_j^TZ_j + frac{ lambda}{ left | theta_j right |_2})^{-1}Z_j^Tr_j $$ . Or $$ theta_j = S_{ frac{ lambda}{ left | theta_j right |^2}}( frac{Z_{j}^{T}r_j}{Z_{j}^{T}Z_j}) $$ . . We will now implement the block coordinate proximal gradient descent to solve the group lasso problem presented above. We will be using the kaggle classic boston housing dataset, where our independent variable is price, and our dependent variables are a combination of categorical (number of bed/bath) and continous (price/sq feet) features. Our first step is to create dummy variables corresponding to the categorical variables. To avoid multicollinearity issues we use 0 bedrooms, 1 bathroom, and short sale as baselines, respectively. To improve results we standardize our data, and will use a $ lambda$ value of 0.012. . from sklearn.preprocessing import MinMaxScaler import itertools . bh = pd.read_csv(&quot;boston_housing.csv&quot;) . Create dummy variables . rooms = pd.get_dummies(bh.Bedrooms, prefix=&#39;Bedrooms&#39;) baths = pd.get_dummies(bh.Bathrooms, prefix=&#39;Bath&#39;) status = pd.get_dummies(bh.Status, prefix=&#39;Status&#39;) . df = pd.concat([bh, rooms, baths, status], axis=1) . df.drop([&#39;Bathrooms&#39;, &#39;Bedrooms&#39;, &#39;Status&#39;], axis=1, inplace=True) . Normalize data . scaler = MinMaxScaler() df[&#39;Price&#39;] = scaler.fit_transform(df[&#39;Price&#39;].values.reshape(-1,1)) df[&#39;PriceSF&#39;] = scaler.fit_transform(df[&#39;PriceSF&#39;].values.reshape(-1,1)) . df.head() . Price PriceSF Bedrooms_0 Bedrooms_1 Bedrooms_2 Bedrooms_3 Bedrooms_4 Bath_1 Bath_2 Bath_3 Bath_4 Status_1 Status_2 Status_3 . 0 0.389410 | 0.280785 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | . 1 0.188751 | 0.108646 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | . 2 0.262731 | 0.142556 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | . 3 0.447175 | 0.211009 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | . 4 0.042260 | 0.061014 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | . y = df[&#39;Price&#39;].values . features = df.drop(&#39;Price&#39;, axis=1) . groups = [&#39;PriceSF&#39;, &#39;Bed&#39;, &#39;Bath&#39;, &#39;Status&#39;] . Defining our soft thresholding function for PGM, and our loss function. . def soft_threshold(x, gamma=lamda): for i, val in enumerate(x): if val &gt; gamma: x[i] = 1-(lamda/abs(val-gamma)) elif val &lt; gamma: x[i] = 1-(lamda/abs(val+gamma)) elif (val &lt;= gamma) and (val&gt;= -gamma): x[i] = 0 return x def loss(b, l=0.012): temp_coeffs = [[beta]*i for beta, i in zip(b, group_lengths)] coeff_vector = np.array(list(itertools.chain(*temp_coeffs))) f_x = np.sum(0.5*(y - np.dot(features.values,coeff_vector))**2) penalty = l*sum([i**2 for i in b]) return f_x + penalty def create_b_not_vector(b, i): not_group_lengths = [j for k, j in enumerate(group_lengths) if k != i] temp_coeffs = [[beta]*i for beta, i in zip(b, not_group_lengths)] return np.array(list(itertools.chain(*temp_coeffs))) . slices = [(0,1), (1, 6), (6, 10), (10, 13)] b = np.random.rand(len(features.columns)) lamda = 0.012 losses = [loss(b)] iterations = 1 for iteration in range(200): for sliced in slices: Z = features[features.columns[sliced[0]:sliced[1]]] Z_cols = Z.columns Z_not = features.loc[:, [feat for feat in features.columns if feat not in Z_cols]] b_not = [i for j, i in enumerate(b) if j not in range(sliced[0], sliced[1])] r = y - np.dot(Z_not, b_not) a = b[sliced[0]:sliced[1]] - np.sum((-Z.values.T*(r-np.dot(Z.values, b[sliced[0]:sliced[1]]))), axis=1) b[sliced[0]:sliced[1]] = soft_threshold(a) f_x = np.sum(0.5*(y - np.dot(features.values,b))**2) penalty = lamda*sum([i**2 for i in b]) losses.append(f_x + penalty) iterations += 1 . And there you have it .",
            "url": "https://jackhmiller.github.io/My-DS-Blog/2020/01/23/GL.html",
            "relUrl": "/2020/01/23/GL.html",
            "date": " • Jan 23, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://jackhmiller.github.io/My-DS-Blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
  
    
        ,"post6": {
            "title": "Coordinate Descent is Fast!",
            "content": "Coordinate descent (CD) algorithms solve optimization problems by successively performing approximate minimization along coordinate directions or coordinate hyperplanes. They have been used in applications for many years, and their popularity continues to grow because of their usefulness in data analysis, machine learning, and other areas of current interest. . CD methods are the archetype of an almost universal approach to algorithmic optimization: solving an optimization problem by solving a sequence of simpler optimization problems. The obviousness of the CD approach and its acceptable performance in many situations probably account for its longstanding appeal among practitioners. Paradoxically, the apparent lack of sophistication may also account for its unpopularity as a subject for investigation by optimization researchers, who have usually been quick to suggest alternative approaches in any given situation. . Various applications (including several in computational statistics and machine learning) have yielded problems for which CD approaches are competitive in performance with more reputable alternatives. The properties of these problems (for example, the low cost of calculating one component of the gradient, and the need for solutions of only modest accuracy) lend themselves well to efficient implementations of CD, and CD methods can be adapted well to handle such special features of these applications as nonsmooth regularization terms and a small number of equality constraints. At the same time, there have been improvements in the algorithms themselves and in our understanding of them. Besides their extension to handle the features just mentioned, new variants that make use of randomization and acceleration have been introduced. Parallel implementations that lend themselves well to modern computer architectures have been implemented and analyzed. Perhaps most surprisingly, these developments are relevant even to the most fundamental problem in numerical computation: solving the linear equations Aw = b. . So let&#39;s explore Coordinate Descent with the simplest of cases: plain old linear regression using some of the ever so used mtcars dataset. Recall that the sum of squared residuals is: . $$RSS = sum left(y_i - sum x_{ij} beta_j right)^2$$ . Quickly deriving our objective function: . $$ f( beta ) = argmin left | y_i-x_{ij} beta_j right |^2_2 $$ . $$ =(y-x beta)^T(y-x beta) $$ . $$ hat{ beta} = (x^Tx)^{-1}(x^Ty) $$aka OLS. . Now let&#39;s do our update steps: . $$ 0 = bigtriangledown_i f(x) = beta_j^T( beta_j x_{ij} + beta_{j}x_{i-1,j}-y_i) $$ . $$ x_{ij} = frac{ beta_j^T(y_i- beta_jx_{i-1, j})}{ beta_j^T beta_j} $$ $$ x_{ij} = frac{ beta_j^T(residual_{i-1,j})}{ left | beta_j right |^2} $$ . Loading the data and off we go. . cd_data = loadmat(&#39;mtcars.mat&#39;)[&#39;data&#39;] cd_df = pd.DataFrame(cd_data) . y = cd_df[0].values x = cd_df.loc[:, 1:].values . inter = np.ones(x.shape[0]) X = np.column_stack((inter, x)) X_Normalized = X / np.sqrt(np.sum(np.square(X), axis=0)) . Define our loss function . def loss(b): return sum((y - X_Normalized@b)**2) . Coding the descent from scratch using numpy . b = np.zeros(X_Normalized.shape[1]) losses = [loss(b)] iterations = 1 for iteration in range(100): r = y - X_Normalized.dot(b) for j in range(len(b)): r = r + X_Normalized[:, j] * b[j] b[j] = X_Normalized[:, j].dot(r) r = r - X_Normalized[:, j] * b[j] losses.append(loss(b)) iterations += 1 . array([193.44428449, -44.7866386 , -27.77448248, -13.3819371 ]) . plt.plot(losses) plt.title(&#39;CD Loss&#39;) plt.xlabel(&#39;Iteration&#39;) . Text(0.5, 0, &#39;Iteration&#39;) . Our loss function converges almost instantly! . print(&quot;MSE:&quot;, sum((y - X_Normalized@b)**2) ) . MSE: 261.3695510665015 . Just as a sanity check, lets make sure our from scratch implementation produced the same results as sklearn&#39;s implementation of linear regression . from sklearn import linear_model reg = linear_model.LinearRegression() . reg.fit (X_Normalized, y) reg.coef_ . array([ 0. , -44.66189706, -27.81172298, -13.40793129]) .",
            "url": "https://jackhmiller.github.io/My-DS-Blog/2019/12/04/CD.html",
            "relUrl": "/2019/12/04/CD.html",
            "date": " • Dec 4, 2019"
        }
        
    
  
    
        ,"post7": {
            "title": "Accelerated Gradient Descent and Newton's Method",
            "content": "Nonconvex optimization problems are ubiquitous in modern machine learning. While it is NP-hard to find global minima of a nonconvex function in the worst case, in the setting of machine learning it has proved useful to consider a less stringent notion of success, namely that of convergence to a first-order stationary point. However, architectures such as deep neural networks induce optimization surfaces that can be teeming with highly suboptimal saddle points. . In this setting, the glaring limitation of gradient descent is that it can get stuck in flat areas or bounce around if the objective function returns noisy gradients. Therefore second-order descent methods are needed for optimization. Momentum is an approach that accelerates the progress of the search to skim across flat areas and smooth out bouncy gradients. In some cases, the acceleration of momentum can cause the search to miss or overshoot the minima at the bottom of basins or valleys. Nesterov momentum is an extension of momentum that involves calculating the decaying moving average of the gradients of projected positions in the search space rather than the actual positions themselves. This has the effect of harnessing the accelerating benefits of momentum whilst allowing the search to slow down when approaching the optima and reduce the likelihood of missing or overshooting it. Further, Nesterov’s accelerated gradient descent (AGD), an instance of the general family of “momentum methods,” provably achieves faster convergence rate than gradient descent (GD) in the convex setting. . $$ L( alpha, beta|y_1,...,y_n) = prod_{i=1}^{n} frac{ Gamma ( alpha+ beta)}{ Gamma( alpha) Gamma( beta)}y_i^{ alpha-1}(1-y_i)^{ beta-1} $$ . Given the above likelihood function, where $ Gamma$ is the gamma function, and $ alpha$ and $ beta$ are greater than zero. First, we convert our likelihood function to the log likelihood. . $$ log(L( alpha, beta | y_{1}, ..., y_{n})) = nlog( Gamma(a+b)) -nlog( Gamma(a)) -nlog( Gamma(b)) +(a-1) sum_{i=1}^{n}log(y_{i}) +(b-1) sum_{i=1}^{n}log(1-y_{i}) $$ . And its maximum likelihood formulation. . $$ widehat{ alpha}, widehat{ beta} = underset{i}{argmax}(log( widehat{L}( alpha, beta | y_{1}, ..., y_{n}))) $$ . And our gradient and Hessian from the log likelihood function: . &emsp; Gradient . $$ bigtriangledown log(L)) = begin{bmatrix} n psi( alpha+ beta) -n psi( alpha) + sum_{i=1}^{n}log(y_{i}) n psi( alpha+ beta) -n psi( beta) + sum_{i=1}^{n}log(1-y_{i}) end{bmatrix} $$ &emsp; Hessian . $$ bigtriangledown^{2} log(L) = begin{bmatrix} n psi&#39;( alpha+ beta) -n psi&#39;( alpha) &amp; n psi&#39;( alpha+ beta) n psi&#39;( alpha+ beta) &amp; n psi&#39;( alpha+ beta) -n psi&#39;( beta) end{bmatrix} $$ Let&#39;s code it up. . For digamma $ psi(x) = frac{ Gamma&#39;(x)}{ Gamma(x)}$ and trigamma $ psi&#39;(x)$, we will cheat and use built-in functions from scipy. . import matplotlib.pyplot as plt import numpy as np import pandas as pd import math from scipy.io import loadmat import seaborn as sns import scipy from sklearn.model_selection import KFold, cross_val_score from random import randrange from scipy.interpolate import BSpline from scipy.ndimage import gaussian_filter %matplotlib inline from scipy.special import digamma, polygamma . Loading the data and coding our objective function, gradient, and Hessian. . Y = loadmat(&#39;Sample_data.mat&#39;)[&#39;y&#39;] . def objective(data, x): n = len(data) return n*math.log(math.gamma(x[0]+x[1]))-n*math.log(math.gamma(x[0]))-n*math.log(math.gamma(x[1])) + (x[0]-1)*sum([math.log(i) for i in data]) + (x[1]-1)*sum([math.log(1-i) for i in data]) def gradient(data, x): n = len(data) return np.array([n* digamma(x[0]+x[1]) - n*digamma(x[0]) + sum([math.log(i) for i in data]), n* digamma(x[0]+x[1]) - n*digamma(x[1]) + sum([math.log(1-i) for i in data])]) def Hessian(data, x): n = len(data) return np.array([[n* polygamma(1, x[0]+x[1]) - n*polygamma(1, x[0]), n* polygamma(1, x[0]+x[1])],[n* polygamma(1, x[0]+x[1]), n* polygamma(1, x[0]+x[1]) - n*polygamma(1, x[1])]]) . &emsp; Newton&#39;s Method . alpha = 1 lam = 1e-10 epsilon = 0.01 x0 = np.random.rand(2) f0 = objective(Y, x0) # initialize fL = [f0] g0 = gradient(Y, x0) # initialize gL = [g0] H0 = Hessian(Y, x0) # initialize Omega = -np.linalg.solve(H0 + lam*np.eye(2),g0) stopping = np.linalg.norm(Omega) iterations = 1 params = [] while stopping &gt; epsilon: x = x0 + alpha*Omega fval = objective(Y, x) while fval &lt; f0: alpha *= 0.1 x = x0 + alpha*Omega fval = objective(Y, x) alpha = alpha**0.5 params.append(x) x0 = x f0 = fval fL.append(fval) g0 = gradient(Y, x0) Omega = -np.linalg.solve(Hessian(Y, x0)+lam*np.eye(2),g0) stopping = np.linalg.norm(Omega) iterations += 1 results = x0 iters = iterations plt.figure(figsize=(20,10)) plt.plot(fL) plt.title(&#39;Newton Method&#39;) plt.xlabel(&#39;Number of iterations&#39;) plt.ylabel(&#39;Log-Likelihood&#39;) . Text(0, 0.5, &#39;Log-Likelihood&#39;) . fig, ax = plt.subplots(figsize=(15,10)) ax.plot([x[0] for x in params], lw=3, label=&#39;Alpha&#39;) ax.plot([x[1] for x in params], lw = 3, label=&#39;Beta&#39;) plt.title(&#39;Alpha &amp; Beta Values vs Number of Iterations&#39;) plt.xlabel(&#39;Number of iterations&#39;) plt.ylabel(&#39;Parameter Value&#39;) ax.legend(loc=&#39;best&#39;) . &lt;matplotlib.legend.Legend at 0x21414a0d940&gt; . print(f&quot; Newton&#39;s Method optimized parameter values- Alpha: {round(x0[0], 3)}, Beta: {round(x0[1], 3)}&quot;) . Newton&#39;s Method optimized parameter values- Alpha: 4.97, Beta: 12.173 . &emsp; Accelerated Gradient Descent . x = np.random.rand(2) beta = np.random.rand(2) x0 = x convergence = gradient(Y, beta)@gradient(Y, beta) convergence_results = [convergence] loss = [objective(Y, x)] alpha = 0.1 epsilon = 0.01 iterations = 1 while convergence &gt; epsilon: x = beta + alpha*gradient(Y, x) beta = x - (iterations-1)/(iterations+2)*(x-x0) convergence = gradient(Y, x)@gradient(Y, x) convergence_results.append(convergence) try: loss.append(objective(Y, x)) except: pass x0 = x iterations += 1 results = x0 iters = iterations plt.figure(figsize=(20,10)) plt.plot(loss) plt.title(&#39;Accelerated Gradient Descent&#39;) plt.xlabel(&#39;Iterations&#39;) plt.ylabel(&#39;Log-Likelihood&#39;) .",
            "url": "https://jackhmiller.github.io/My-DS-Blog/2019/11/28/NM.html",
            "relUrl": "/2019/11/28/NM.html",
            "date": " • Nov 28, 2019"
        }
        
    
  
    
        ,"post8": {
            "title": "Cost-Sensitive Feature Selection",
            "content": "Motivation . Feature selection is one of the trending challenges in multi-label classification, as the proliferation of high-dimensional data has become a trend in the last few years. Datasets with a dimensionality over the tens of thousands are constantly appearing in applications such as medical image and text retrieval or genetic data. The high-dimensionality of data has an important impact in learning algorithms, since they degrade their performance when a number of irrelevant and redundant features are present. In fact, this phenomenon is known as the curse of dimensionality, because unnecessary features increase the size of the search space and make generalization more difficult. For overcoming this major obstacle in machine learning, researchers usually employ dimensionality reduction techniques. In this manner, the set of features required for describing the problem is reduced, most of the times along with an improvement in the performance of the models. . Feature selection is arguably the most famous dimensionality reduction technique. It consists of detecting the relevant features and discarding the irrelevant ones. Its goal is to obtain a subset of features that describe properly the given problem with a minimum degradation in performance, with the implicit benefits of improving data and model understanding and the reduction in the need for data storage. With this technique, the original features are maintained, contrary to what usually happens in other techniques such as feature extraction, where the generated dataset is represented by a newly generated set of features, different than the original. . Feature selection methods can be divided into wrappers, filters and embedded methods. While wrapper models involve optimizing a predictor as a part of the selection process, filter models rely on the general characteristics of the training data to select features with independence of any predictor. The embedded methods generally use machine learning models for classification, and then an optimal subset or ranking of features is built by the classifier algorithm. Wrappers and embedded methods tend to obtain better performances but at the expense of being very time consuming and having the risk of overfitting when the sample size is small. On the other hand, filters are faster and, therefore, more suitable for large datasets. They are also easier to implement and scale up better than wrapper and embedded methods. As a matter of fact, filters can be used as a pre-processing step before applying other more complex feature selection methods. . However the existing approaches assume that all the features have the same cost. This assumption may be inappropriate when the acquisition of the feature values is costly. For example in medical diagnosis each diagnostic value extracted by a clinical test is associated with its own cost. In such cases it may be better to choose a model with an acceptable classification performance but a much lower cost. . As a result, our goal of here is to obtain a trade-off between a filter metric and the cost associated to the selected features, in order to select relevant features with a low associated cost. We will try and introduce four different approaches that can be considered filter methods. In this manner, any filter metric can be modified to have into account the cost associated to the input features. . . Cost-sensitive Methods . Approach #1- Feature-cost-sensitive Random Forest (FCS-RF) . Random forest (RF) is an ensemble of decision trees. RF has a wide range of applications because of its good stability and generalization. The typical construction process of RF consists of the following procedures. First, bagging is adopted on the training dataset to produce many subsets (with differences). Then each subset is used to construct a decision tree. In the tree growth, the splitting on each node depends on the feature selected from a group of candidates that are randomly chosen from all features. Without pruning (i.e., all leaf nodes are pure), all trees grow fully and each of them functions as a base classifier. Finally, all these tree classifiers are integrated. There are two important random characteristics in growing a random forest. One is randomly sampling, and the other is randomly generating the node splitting candidates. The diversity between the trees caused by randomness is crucial to the performance of the forest. . We can extend the random forest to incorporate feature cost by incorporating a probability vector into the tree construction process. The probability vector is generated based on the cost vector and we require the probability of a feature being selected inversely proportional to its cost. It can be easily shown that the expected cost of the feature selected in probability is smaller than the expected cost of the feature selected randomly. But some randomness among the trees still remains, which is indispensable to preserve the accuracy. It should be noted that if all the cost of features are equal, the FCS-RF will degenerate to the ordinary RF. . By using the the feature cost into the construction of random forest, the high-cost features less likely to be selected and the low-cost features with larger chances of being selected. In this way, the importance score of a feature is explicitly influenced by feature cost and its distinguishing ability. As a result, the top ranked features in the rank produced through FCS-RF have larger chance to be both low-cost and informative. . . Approach #2- Non-convex Constrained Optimization . Given a loss function (negative log-likelihood function) $ mathcal{L}_k( theta_k)$, the most natural way to take cost into account would be to minimize $ mathcal{L}_k( theta_k)$ with the addition of constraint $$ sum_{j=1}^{p}c_j( left| theta_{k,j} right|&gt;0) leq t $$ . This is equivalent to finding the best model (in terms of maximization of log-likelihood function) subject to limited budget. Parameter t can be interpreted as an upper bound for the budget of the kth model. When c1 = ··· = cp, the above problem reduces to best-subset selection. In best-subset selection we maximize log-likelihood function subject to limited number of features. Note that in best-subset selection parameter t stands for the maximal possible number of features that can be used in the model, whereas in in the additional constraint $t$ denotes the maximal possible cost that can be incurred to build the model. In other words, the constraint ensures that we control the budget instead of controlling the number of features. . Minimization of the log-likelihood function subject to (4) is equivalent to finding the optimal values of parameters $ theta_k$ by solving: $$ hat{ theta}_k= underset{ theta_k epsilon R}{argmin} left { mathcal{L}_k( theta_k) + lambda sum_{j=1}^{p}c_j( left| theta_{k,j} right|&gt;0) right } $$ . More precisely, for a given t &gt; 0 there exists $ lambda$ &gt; 0 such that the two problems share the same solution, and vice versa. The second term is a penalty for the cost. Parameter $ lambda$ &gt; 0 controls the balance between goodness of fit of the model and the cost. The larger is the value of $ lambda$ the cheaper is the model. Although the above approach seems to be encouraging, the issue is that the above equation is nonconvex, which makes it difficult to understand theoretically, and especially, to solve computationally. The problem is due to employment of $ mathscr{l}_0$-type penalty and renders the approach computationally infeasible for large number of features. The popular solution is to use $ mathscr{l}_1$ (lasso) penalty instead of $ mathscr{l}_0$, i.e., to optimize: . $$ hat{ theta}_k= underset{ theta_k epsilon R}{argmin} left { mathcal{L}_k( theta_k) + lambda sum_{j=1}^{p}c_j left| theta_{k,j} right| right } $$ It is known that $ mathscr{l}_1$ regularization (lasso) encourages sparsity in parameter vector, and thus selects relevant features. We can take this approach a step further, whereby instead of using $ mathscr{l}_1$ penalty, we can consider the elastic-net penalty which is a linear combination of $ mathscr{l}_1$ and $ mathscr{l}_2$ norms. Including $ mathscr{l}_2$ (ridge) regularization stabilizes the solution. The standard elastic-net does not take into account feature costs. The key idea in our approach is to modify the elastic-net penalty in such a way that features with higher costs are assigned larger weights: $$ hat{ theta}_k= underset{ theta_k epsilon R}{argmin} left { mathcal{L}_k( theta_k) + lambda sum_{j=1}^{p}c_jP_w( theta_{k,j}) + lambda sum_{j=p+1}^{p+k-1}P_w( theta_{k,j}) right } $$ . where $P_w( theta_{k,j}) = (1-w) theta_{k,j}^2 + w left| theta_{k,j} right|$ is the elastic net penalty, and $ lambda$ is a regularization parameter. The second term depends on costs; the larger is the cost the larger is the penalty. In other words, it is more difficult to choose expensive features to the final model. The costs are normalized in such a way that they sum up to p. Such normalization is necessary, otherwise the scale can be absorbed into the value of $ lambda$. . The total cost depends on $ lambda$, large enough value of $ lambda$ sets all the coefficients exactly equal to zero which gives a total cost zero. Small value of $ lambda$ results in many non-zero coefficients which yields large total cost. A practical challenge is how to select $ lambda$ in order to get a certain number of features with non-zero coefficients or to get a certain total cost. Unfortunately, it is not possible to directly control the number of selected features, and as such the total cost. It is necessary to test different regularization parameters and observe the total cost associated with selected features, similar to tuning a hyperparameter. It is relatively easy to obtain the solutions for many different values of $ lambda$, which in turn allows to select the value of $ lambda$ best corresponding to the desired total cost. . . Approach #3- Cost-based Correlation-based Feature Selection (CCFS) . CFS (Correlation-based Feature Selection) is a multivariate subset filter algorithm. It uses a search algorithm combined with an evaluation function to estimate the merit of feature subsets. The evaluation function takes into account the usefulness of individual features for predicting the class label as well as the level of correlation among them. It is assumed that good feature subsets contain features highly correlated with the class and uncorrelated with each other. The evaluation function can be seen in the following equation: $$ M_s = frac{k overline{r_{ci}}}{ sqrt{k+k(k-1) overline{r_{ii}}}} $$ . where $M_s$ is the merit of a feature subset S that contains k features, overline{r{ci}} is the average correlation between the features of S and the class, and overline{r{ii}} is the average intercorrelation between the features of S.In fact, this function is Pearson&#39;s correlation with all variables standardized. The numerator estimates how predictive of the class S is and the denominator quantifies the redundancy among the features in S. We can modify CFS by adding a term to the evaluation function to take into account the cost of the features, as can be seen in the following equation: $$ MC_s = frac{k overline{r_{ci}}}{ sqrt{k+k(k-1) overline{r_{ii}}}} - lambda frac{ sum_{i}^{k}C_i}{k} $$ . where $MC_s$ is the merit of the subset S affected by the cost of the features, $C_i$ is the cost of the feature i, and $ lambda$ is a parameter introduced to weight the influence of the cost in the evaluation function. If $ lambda$ is 0, the cost is ignored and the method works as the regular CFS. If $ lambda$ is between 0 and 1, the influence of the cost is smaller than the other term. If $ lambda = 1$ both terms have the same influence and if $ lambda &gt; 1$, the influence of the cost is greater than the influence of the other term. . . Approach #4- Cost-based Minimal-Redundancy-Maximal-Relevance (mRMR) . mRMR (Minimal-Redundancy-Maximal-Relevance) is a multivariate ranker filter algorithm. As mRMR is a ranker, the search algorithm is simpler than CFS&#39;s.The evaluation function combines two constraints (as the name of the method indicates), maximal relevance and minimal redundancy. The former is denoted by the letter D, it corresponds to the mean value of all mutual information values between each feature $x_i$ and class c, and has the expression shown in the following equation: $$ D(S,c) = frac{1}{ left| S right|} sum_{}^{}I(x_i;c) $$ . where S is a set of features and $I(x_i;c)$ is the mutual information between feature x and class c. The constraint of minimal redundancy is denoted by R and is shown in the following equation: $$ R(S) = frac{1}{ left| S right|^2} sum_{}^{}I(x_i,x_j) $$ . The evaluation function to be maximized combines the two constraints, $D(S,c) - R(S)$. We can make a modification of mRMR by adding a term to the condition to be maximized so as to take into account the cost of the feature to be selected, as can be seen in the following equation: $$ underset{x_j epsilon X-S_{m-1}}{max} left[ left( I(x_j;c)- frac{1}{m-1} sum_{x_i}I(x_j;x_i) right) - lambda C_j right] $$ . where $C_j$ is the cost of the feature j, and $ lambda$ is a parameter introduced to weight the influence of the cost in the evaluation function, as explained in the previous subsection. . . In Practice . When using the above mentioned approaches in real machine learning projects, cost sensitive feature selection methods typically achieves higher accuracy when the budget is low. For higher budgets, the traditional methods tend to perform better. This is because for a larger budget, traditional methods can include all relevant features, which results in a large predictive power of the model. For a limited budget, cost sensitive methods select features that serve as cheaper substitutes of the relevant expensive features. The graph below shows a typical tradoff between cost sensitive and non-cost sensitive feature selection approachs. . &nbsp; . Source: Teisseyre, P., Klonecki, T. (2021). Controlling Costs in Feature Selection: Information Theoretic Approach. In: Paszynski, M., Kranzlmüller, D., Krzhizhanovskaya, V.V., Dongarra, J.J., Sloot, P.M.A. (eds) Computational Science – ICCS 2021. ICCS 2021. . We can see that until 60% of the total cost, cost sensitive method performs better. This is due to the fact that,in this case, traditional methods can only use a fraction of all original features (say 1 or 2 out of 4 original features) within the assumed budget, which deteriorates the predictive performance of the corresponding classification model. On the other hand, the cost sensitive method aims to replace the original features by their cheaper counterparts, which allows to achieve higher accuracy of the corresponding model. When the budget exceeds 60% of the total cost, the traditional feature selection method tends to perform better than the proposed method, which is associated with the fact that, in this case, traditional methods include all original features (i.e., those which constitute the minimal set allowing for accurate prediction of the target variable) which results in a large predictive power of the corresponding model. For a larger budget, cost sensitive methods include both noisy features as well as the original ones. The noisy features become redundant when considering together with the original ones. This results in slightly lower prediction accuracy of the corresponding model. As expected, the cost sensitive methods are worth considering when the assumed budget is limited. .",
            "url": "https://jackhmiller.github.io/My-DS-Blog/2018/09/07/CFR-(1).html",
            "relUrl": "/2018/09/07/CFR-(1).html",
            "date": " • Sep 7, 2018"
        }
        
    
  
    
        ,"post9": {
            "title": "Functional Linear Regression",
            "content": "Covariance estimation is a problem of great interest in many different disciplines, including machine learning, signal processing, economics and bioinformatics. In many applications the number of variables is very large, e.g., in the tens or hundreds of thousands, leading to a number of covariance parameters that greatly exceeds the number of observations. To address this problem constraints are frequently imposed on the covariance to reduce the number of parameters in the model. For example, the Glasso model of Yuan and Lin and Banerjee et al 1 imposes sparsity constraints on the covariance. The Kronecker product model of Dutilleul and Werner et al 2 assumes that the covariance can be represented as the Kronecker product of two lower dimensional covariance matrices. Here we will implement a combination of these two aproaches. . Here is our problem setting: . A combustion engine produces gas with polluting substances such as nitrogen oxides (NOx).Gas emission control regulations have been set up to protect the environment. The NOx Storage Catalyst (NSC) is an emission control system by which the exhaust gas is treated after the combustion process in two phases: adsorption and regeneration. During the regeneration phase, the engine control unit is programmed to maintain the combustion process in a rich air-to-fuel status. The average relative air/fuel ratio is the indicator of a correct regeneration phase. Our goal is to predict this value, using the information from eleven sensors. To do so, we are going to use group lasso regression. . List of on-board sensorsair aspirated per cylinder . engine rotational speed | total quantity of fuel injected | low presure EGR valve | inner torque | accelerator pedal position | aperture ratio of inlet valve | downstreem intercooler preasure | fuel in the 2nd pre-injection | vehicle velocity | . First we will write the problem that we want to solve in mathematical notation. . $$ underset{ beta_g in mathbb{R}}{armin} left | sum_{g in G} left [ X_g beta_g right ]-y right |_2^2 + lambda_1 left | beta right |_1 + lambda_2 sum_{g in G} sqrt[]{d_g} left | beta_g right |_2 $$ Where $$ $$ $ X_g in mathbb{R}^{n x d_g}$ is the data matrix for each sensor&#39;s covariates which compose group $g$, $ beta_g $ is the B spline coefficients for group $g$, $ y in mathbb{R}^{n}$ is the air/fuel ratio target, $ n$ is the number of measurements, $d_g$ is the dimensionality of group $g$, $ lambda_1 $ is the parameter-wise regularization penalty, $ lambda_2$ is the group-wise regularization penalty, $ G $ is the set of all groups for all sensors . Now on to the code. We will use group lasso to learn the B-spline coefficients. We will use B-splines with 8 knots to reduce the dimensionality of the problem. Ultimately, we want to determine which sensors are correlated with the air/fuel ratio? Also, we want to predict the air/fuel ratio for the observations in the test dataset. . from scipy import interpolate import group_lasso import sklearn.linear_model as lm from sklearn.model_selection import GridSearchCV, RandomizedSearchCV . x_train = loadmat(&#39;NSC.mat&#39;)[&#39;x&#39;] y_train = loadmat(&#39;NSC.mat&#39;)[&#39;y&#39;] x_test = loadmat(&#39;NSC.test.mat&#39;)[&#39;x_test&#39;] y_test = loadmat(&#39;NSC.test.mat&#39;)[&#39;y_test&#39;] . for i in range(len(x_train[0])): plt.figure(figsize=(15,8)) pd.DataFrame(x_train[0][i]).plot(legend=False, title=f&quot;Sensor {i}&quot;) . def transformation(data): coefficients = [] x = np.linspace(0, 203, 203) knots = np.linspace(0, 203, 10) [1:-1] for i,d in enumerate(data): t, c, k = interpolate.splrep(x, d, task=-1, t=knots, k=2) coefficients.append(np.trim_zeros(c, trim=&#39;b&#39;)[:-1]) return np.array(coefficients) def standardize(data): results = [] for i in data: temp = scaler.fit_transform(i) results.append(temp) return results . scaler = StandardScaler() Y_train = transformation(scaler.fit_transform(y_train)).ravel() Y_test = transformation(scaler.fit_transform(y_test)).ravel() X_train = np.hstack(np.array([transformation(i) for i in standardize(x_train[0])])) X_test = np.hstack(np.array([transformation(i) for i in standardize(x_test[0])])) . identity = np.identity(10) . Kronecker Products . final_train = np.kron(X_train, identity) final_test = np.kron(X_test, identity) . g = [[i]*100 for i in range(1,11)] groups = np.array([item for sublist in g for item in sublist]) . gl = group_lasso.GroupLasso( groups=groups, group_reg=0, l1_reg=0, fit_intercept=True, scale_reg=&quot;none&quot;, supress_warning=True, tol=1e-5 ) . lambdas, _, _ = lm.lasso_path(final_train, Y_train) . CV = RandomizedSearchCV(estimator=gl, param_distributions={&#39;group_reg&#39;: lambdas[::5]}, scoring=&#39;neg_mean_squared_error&#39;, n_iter=100, verbose=2) CV.fit(final_train, Y_train) . coef = gl.coef_.ravel().reshape(100, 10) coef_base = X_train@coef coef_df = pd.DataFrame(coef_base) . print(&quot;Best lambda:&quot;, CV.best_params_[&#39;group_reg&#39;]) . print(&quot;Coefficients Correlated to Target&quot;) coef_df.corrwith(pd.DataFrame(Y_train.reshape(150,10))) . It appears sensors 2 and 7 have the greatest correlation to the air fuel ration . _y = pd.DataFrame(Y_train.reshape(150,10)) for sensor in [2, 7]: plt.figure(figsize=(15,8)) plt.scatter(coef_df[sensor], _y[sensor]) plt.title(f&quot;Correlation of sensor {sensor} and air/fuel ratio&quot;) plt.xlabel(f&quot;Sensor {sensor}&quot;) plt.ylabel(&quot;Air/fuel ratio&quot;) . coef_df[2].plot(title=&#39;Coefficients for sensor 2&#39;) . coef_df[7].plot(title=&#39;Coefficients for sensor 7&#39;) . predicted = CV.predict(final_test) . print(&quot;Mean Square Prediction Error:&quot;, sum((Y_test - predicted)**2)) . . Yuan et al. &quot;Model Selection and Estimation in Regression With Grouped Variables,&quot; Journal of the Royal Statistical Society Series B. (2006): 49-67. . Tsiligkaridis et al. &quot;Convergence Properties of Kronecker Graphical Lasso Algorithms,&quot; IEEE (2013). .",
            "url": "https://jackhmiller.github.io/My-DS-Blog/2018/04/02/FLR.html",
            "relUrl": "/2018/04/02/FLR.html",
            "date": " • Apr 2, 2018"
        }
        
    
  
    
        ,"post10": {
            "title": "Sparse Linear Regression",
            "content": "There are two fundamental goals in statistical learning: ensuring high prediction accuracy and discovering relevant predictive variables. Variable selection is particularly important when the true underlying model has a sparse representation. It may be important to clarify that the expression &#39;sparse&#39; should not be confused with techniques for sparse data, containing many zero entries. Here, sparsity refers to the estimated parameter vector, which is forced to contain many zeros. A sparse representation can be manifested as a result of two common occurances. First, the number of predictors might exceed the number of observations. Such high-dimensional data settings are nowadays commonplace in operational research. Second, some data points might behave differently from the majority of the data. Such atypical data points are called outliers in statistics, and anomalies in machine learning. Traditional methods for linear regression analysis such as the ordinary Least Squares estimator (OLS) fail when these problems arise: the OLS cannot be computed or becomes unreliable due to the presence of outliers. . A regression vector is sparse if only some of its components are nonzero while the rest is set equal to zero, hereby inducing variable selection. . . Here we want to compare some different regression techniques that induce feature or input sparsity: Lasso Regression, Ridge Regression, Adaptive Lasso Regression, and Elastic Net Regression. We will calculate the optimal tuning parameters, and fit the model to aquire the coefficients obtained with the optimal parameters as well as the Mean Square Prediction Error for the test dataset. . In this demonstration our goal is to predict the concentration of carbon oxide (CO) in mg/m^3. For this purpose, we have the following information provided by air quality sensors: . Benzene (C6H6) concentration in μg/m3 | Non Metanic HydroCarbons (NMHC) concentration in μg/m3 | Nitrogen Oxides (NOx)concentration in ppb | Nitrogen Dioxide (NO2) concentration in μg/m3 | Ozone (O3) concentration in μg/m3 | Temperature (T) in Celsius degrees | Relative Humidity (RH) | Absolute Humidity (AH) | . from sklearn.model_selection import RandomizedSearchCV, GridSearchCV from scipy.stats import uniform from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression, lasso_path, LassoCV from sklearn.metrics import mean_squared_error from sklearn.preprocessing import StandardScaler from numpy import arange . scaler = StandardScaler() . train_data = pd.read_csv(&#39;train.air.csv&#39;) test_data = pd.read_csv(&#39;test.air.csv&#39;) . standardized_train = scaler.fit_transform(train_data) standardized_test = scaler.fit_transform(test_data) . train = pd.DataFrame(standardized_train, columns=train.columns) test = pd.DataFrame(standardized_test, columns=test.columns) . y_train = train[&#39;CO&#39;] x_train = train.drop(&#39;CO&#39;, axis=1) y_test = test[&#39;CO&#39;] x_test = test.drop(&#39;CO&#39;, axis=1) . Ridge . param_grid = {&#39;alpha&#39;: uniform()} model = Ridge() ridge_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=100) ridge_search.fit(x_train, y_train) print(&quot;Optimal lasso penality parameter:&quot;, round(ridge_search.best_estimator_.alpha, 3)) print(&quot;Best parameter score:&quot;, round(ridge_search.best_score_, 3)) . print(&quot;Coefficients:&quot;, ridge_search.best_estimator_.coef_) . ridge_pred = ridge_search.predict(x_test) . print(&quot;Ridge MSE for test data:&quot;, round(mean_squared_error(y_test, ridge_pred),2)) . Lasso . param_grid = {&#39;alpha&#39;: uniform()} model = Lasso() lasso_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=100) lasso_search.fit(x_train, y_train) print(&quot;Optimal lasso penality parameter:&quot;, round(lasso_search.best_estimator_.alpha, 3)) print(&quot;Best parameter score:&quot;, round(lasso_search.best_score_, 3)) . print(&quot;Coefficients:&quot;, lasso_search.best_estimator_.coef_) . lasso_pred = lasso_search.predict(x_test) . print(&quot;Lasso MSE for test data:&quot;, round(mean_squared_error(y_test, lasso_pred), 2)) . Adaptive Lasso . coefficients = LinearRegression(fit_intercept=False).fit(x_train, y_train).coef_ gamma = 2 weights = coefficients**-gamma X = x_train/weights lambdas, lasso_betas, _ = lasso_path(X, y_train) lassoCV = LassoCV(alphas=lambdas, fit_intercept=False, cv=10) lassoCV.fit(X, y_train) . print(&quot;Optimal adaptive lasso penality parameter:&quot;, lassoCV.alpha_) . print(&quot;Coefficients:&quot;, lassoCV.coef_) . adaptive_pred = lassoCV.predict(x_test/weights) . print(&quot;Adaptive Lasso MSE for test data:&quot;, round(mean_squared_error(y_test, adaptive_pred), 2)) . Elastic Net . param_grid = {&#39;alpha&#39;: uniform(), &#39;l1_ratio&#39;: arange(0, 1, 0.01)} model = ElasticNet() EN_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=100) EN_search.fit(x_train, y_train) print(&quot;Optimal parameters:&quot;, EN_search.best_params_) print(&quot;Best parameter score:&quot;, round(EN_search.best_score_, 3)) . print(&quot;Coefficients:&quot;, EN_search.best_estimator_.coef_) . EN_pred = EN_search.predict(x_test) . print(&quot;Elastic Net MSE for test data:&quot;, round(mean_squared_error(y_test, EN_pred), 2)) . Conclusion . Elastic net can be recommended without knowing the size of the dataset or the number of predictors, as it automatically handles data with various sparsity patterns as well as correlated groups of regressors. Lasso outperforms ridge for data with a small to moderate number of moderate-sized effects. In these cases, rdige will not provide a sparse model that is easy to interpret, which would lead one to use Lasso methods. On the other hand, Ridge regression performs the best with a large number of small effects.This is because the ridge penalty will prefer equal weighting of colinear variables while lasso penalty will not be able to choose. This is one reason ridge (or more generally, elastic net, which is a linear combination of lasso and ridge penalties) will work better with colinear predictors. If the data give little reason to choose between different linear combinations of colinear predictors, lasso will struggle to prioritize a predictor amongst colinears, while ridge tends to choose equal weighting. Given our dataset and number of predictors here, I would recommend Lasso. .",
            "url": "https://jackhmiller.github.io/My-DS-Blog/2018/03/17/Sparse.html",
            "relUrl": "/2018/03/17/Sparse.html",
            "date": " • Mar 17, 2018"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "My CV",
          "content": "Work Experience . Data Scientist at Sparks AB . Hod Hasharon, October 2019 – Current . ML &amp; Deep Learning for Algorithmic Trading | End-to-end data science pipelines for financial market predictions, including research, feature creation/selection, model training/optimization, and deployment architecture | Deep learning (RNN, CNN, transformers) &amp; machine learning models (gradient boosting) | Engineering using AWS Cloud suite, Jenkins, custom APIs to automate training and deployment | . Junior Data Scientist/ML Engineer at WeWork Technology . Tel Aviv, November January 2019 – October 2019 . Data Science &amp; Engineering | Automated end-to-end data pipeline engineering using AWS, Snowflake &amp; Apache Airflow | Reproducible DS environment engineering using Docker and Jupyter | Unstructured sentiment modeling and automation for global billing and payment services | Anomaly detection model for payment API monitoring &amp; security | . Senior Data Analyst at EY (Ernst &amp; Young) . Tel Aviv, November 2016 – December 2018 . Innovation and R&amp;D | Machine &amp; deep learning PoCs for Israeli and international consulting clients as solutions to predictive and analytics challenges | Sample of projects: financial anomaly detection for global bank, unsupervised learning model for drug discovery for pharma co., appraisal modeling for commercial real estate | . (Aliyah and army service October 2015 – November 2016) . Quant Analyst at McKinsey &amp; Co. . Washington D.C., January 2013-October 2015 . Quantitative Analytics and Modeling | Modeling for warfare simulations and predictions regarding autonomous robotics | Part of DoD-led study to determine the effectiveness of commercial &amp; defense machine learning technologies to solve defense needs | . Research Associate at Defense Advanced Research Projects Agency (DARPA) . Philadelphia, PA, September 2011- May 2014 . Competitive contingency estimation program | Heuristic modeling for event forecasting and classification | Sentiment analysis research for modeling of exogenous events &amp; systematic shocks | . Education . University of Pennsylvania, BA Statistics | 2010-2014 . Military Applications of Statistical Modeling | GPA: 3.9/4.0 Magna Cum Laude | . Georgia Tech, Msc Computer Science | 2020-Present . Computational Perception and Robotics | GPA: 3.8/4.0 | .",
          "url": "https://jackhmiller.github.io/My-DS-Blog/CV/",
          "relUrl": "/CV/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "About Me",
          "content": "I am a data scientist working and based in Israel, with an undergraduate degree in statistics from the University of Pennsylvania, and a masters degree in computer science Georgia Tech. . 050-709-2944 | Jack.harris.miller@gmail.com . .",
          "url": "https://jackhmiller.github.io/My-DS-Blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jackhmiller.github.io/My-DS-Blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}