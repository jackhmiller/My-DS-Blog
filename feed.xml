<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://jackhmiller.github.io/My-DS-Blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://jackhmiller.github.io/My-DS-Blog/" rel="alternate" type="text/html" /><updated>2024-02-13T08:36:20-06:00</updated><id>https://jackhmiller.github.io/My-DS-Blog/feed.xml</id><title type="html">Concept Drift</title><subtitle>050-709-2944 | Jack.harris.miller@gmail.com</subtitle><entry><title type="html">Stanford’s DSPy LLM Framework</title><link href="https://jackhmiller.github.io/My-DS-Blog/2024/02/10/DSPy.html" rel="alternate" type="text/html" title="Stanford’s DSPy LLM Framework" /><published>2024-02-10T00:00:00-06:00</published><updated>2024-02-10T00:00:00-06:00</updated><id>https://jackhmiller.github.io/My-DS-Blog/2024/02/10/DSPy</id><author><name></name></author><summary type="html">Setting up the Foundational Model Landscape The current LLM/foundational model landscape can be defined by dedication to task-specific design, that prioritize flexibility for performance. These tasks include: Adaptation Prompting- zero-shot, few-shot, kNN Finte-tuning- PEFT, LoRa Reasoning Chain-of-thought Agent loops Augmentation Retrieval augmentation, multi-model augmentation Calling external tools Retrieval modeling</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://jackhmiller.github.io/My-DS-Blog/images/dspy.png" /><media:content medium="image" url="https://jackhmiller.github.io/My-DS-Blog/images/dspy.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Chunking in RAG</title><link href="https://jackhmiller.github.io/My-DS-Blog/2023/08/25/RAG.html" rel="alternate" type="text/html" title="Chunking in RAG" /><published>2023-08-25T00:00:00-05:00</published><updated>2023-08-25T00:00:00-05:00</updated><id>https://jackhmiller.github.io/My-DS-Blog/2023/08/25/RAG</id><author><name></name></author><category term="LLMs" /><category term="chunking" /><summary type="html">Introduction Language models have become an important and flexible building block in a variety of user-facing language technologies, including conve2024-02-05rsational interfaces, search and summarization, and collaborative writing. These models perform downstream tasks primarily via prompting: all relevant task specification and data to process is formatted as a textual input context, and the model returns a generated text completion. These input contexts can contain thousands of tokens, especially when language models are used to process long documents (e.g., legal or scientific documents, conversation histories, etc.) or when language models are augmented with external information.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://jackhmiller.github.io/My-DS-Blog/images/chunk_thumb.png" /><media:content medium="image" url="https://jackhmiller.github.io/My-DS-Blog/images/chunk_thumb.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">PEFT with LoRA and QLoRA</title><link href="https://jackhmiller.github.io/My-DS-Blog/2023/06/22/PEFT-LORA-QLORA.html" rel="alternate" type="text/html" title="PEFT with LoRA and QLoRA" /><published>2023-06-22T00:00:00-05:00</published><updated>2023-06-22T00:00:00-05:00</updated><id>https://jackhmiller.github.io/My-DS-Blog/2023/06/22/PEFT-LORA-QLORA</id><author><name></name></author><category term="LLMs, LoRA, PEFT, QLoRA, finished_post" /><summary type="html">Introduction Since the inception of transfer learning, dozens of works have sought to make model adaptation more parameter- and compute-efficient. Yet it is still the case that one of main drawbacks for LLMs is that have to be fine-tuned for each downstream task, learning a different set of parameters.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://jackhmiller.github.io/My-DS-Blog/images/LoRA.png" /><media:content medium="image" url="https://jackhmiller.github.io/My-DS-Blog/images/LoRA.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Trying to Make Sense of Neural Network Generalization</title><link href="https://jackhmiller.github.io/My-DS-Blog/2021/12/11/Interpolation.html" rel="alternate" type="text/html" title="Trying to Make Sense of Neural Network Generalization" /><published>2021-12-11T00:00:00-06:00</published><updated>2021-12-11T00:00:00-06:00</updated><id>https://jackhmiller.github.io/My-DS-Blog/2021/12/11/Interpolation</id><author><name></name></author><category term="theory" /><category term="interpolation" /><summary type="html">I am sure that by now you have heard the jokes and seen the memes that AI is essentially just a bunch of if else statements. Take this tweet below:</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://jackhmiller.github.io/My-DS-Blog/images/interp.png" /><media:content medium="image" url="https://jackhmiller.github.io/My-DS-Blog/images/interp.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Neural Tangent Kernel</title><link href="https://jackhmiller.github.io/My-DS-Blog/2021/10/02/NTK.html" rel="alternate" type="text/html" title="Neural Tangent Kernel" /><published>2021-10-02T00:00:00-05:00</published><updated>2021-10-02T00:00:00-05:00</updated><id>https://jackhmiller.github.io/My-DS-Blog/2021/10/02/NTK</id><author><name></name></author><category term="DL_theory/Neural_tangent_kernel" /><summary type="html">Introduction Much of the research on deep learning theory over the past few years addresses the common theme of analyzing neural networks in the infinite-width limit. At first, this limit may seem impractical and even pointless to study. However, it turns out that neural networks in this regime simplify to linear models with a kernel called the neural tangent kernel. These results are significant as they give a way of understanding why neural networks converge to a optimal solution. Gradient descent is therefore very simple to study, leads to a proof of convergence of gradient descent to 0 training loss. Neural networks are know to be highly non-convex objects and so understanding their convergence under training is highly non-trivial.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://jackhmiller.github.io/My-DS-Blog/images/ntk.png" /><media:content medium="image" url="https://jackhmiller.github.io/My-DS-Blog/images/ntk.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>