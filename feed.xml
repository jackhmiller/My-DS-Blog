<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://jackhmiller.github.io/My-DS-Blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://jackhmiller.github.io/My-DS-Blog/" rel="alternate" type="text/html" /><updated>2023-07-12T02:38:08-05:00</updated><id>https://jackhmiller.github.io/My-DS-Blog/feed.xml</id><title type="html">Concept Drift</title><subtitle>050-709-2944 | Jack.harris.miller@gmail.com</subtitle><entry><title type="html">PEFT with LoRA and QLoRA</title><link href="https://jackhmiller.github.io/My-DS-Blog/2023/06/22/PEFT-LORA-QLORA.html" rel="alternate" type="text/html" title="PEFT with LoRA and QLoRA" /><published>2023-06-22T00:00:00-05:00</published><updated>2023-06-22T00:00:00-05:00</updated><id>https://jackhmiller.github.io/My-DS-Blog/2023/06/22/PEFT-LORA-QLORA</id><author><name></name></author><category term="LLMs, LoRA, PEFT, QLoRA, finished_post" /><summary type="html">Introduction Since the inception of transfer learning, dozens of works have sought to make model adaptation more parameter- and compute-efficient. Yet it is still the case that one of main drawbacks for LLMs is that have to be fine-tuned for each downstream task, learning a different set of parameters.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://jackhmiller.github.io/My-DS-Blog/images/LoRA.png" /><media:content medium="image" url="https://jackhmiller.github.io/My-DS-Blog/images/LoRA.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Trying to Make Sense of Neural Network Generalization</title><link href="https://jackhmiller.github.io/My-DS-Blog/2021/12/11/Interpolation.html" rel="alternate" type="text/html" title="Trying to Make Sense of Neural Network Generalization" /><published>2021-12-11T00:00:00-06:00</published><updated>2021-12-11T00:00:00-06:00</updated><id>https://jackhmiller.github.io/My-DS-Blog/2021/12/11/Interpolation</id><author><name></name></author><category term="theory" /><category term="interpolation" /><summary type="html">I am sure that by now you have heard the jokes and seen the memes that AI is essentially just a bunch of if else statements. Take this tweet below:</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://jackhmiller.github.io/My-DS-Blog/images/interp.png" /><media:content medium="image" url="https://jackhmiller.github.io/My-DS-Blog/images/interp.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Neural Tangent Kernel</title><link href="https://jackhmiller.github.io/My-DS-Blog/2021/10/02/NTK.html" rel="alternate" type="text/html" title="Neural Tangent Kernel" /><published>2021-10-02T00:00:00-05:00</published><updated>2021-10-02T00:00:00-05:00</updated><id>https://jackhmiller.github.io/My-DS-Blog/2021/10/02/NTK</id><author><name></name></author><category term="DL_theory/Neural_tangent_kernel" /><summary type="html">Introduction Much of the research on deep learning theory over the past few years addresses the common theme of analyzing neural networks in the infinite-width limit. At first, this limit may seem impractical and even pointless to study. However, it turns out that neural networks in this regime simplify to linear models with a kernel called the neural tangent kernel. These results are significant as they give a way of understanding why neural networks converge to a optimal solution. Gradient descent is therefore very simple to study, leads to a proof of convergence of gradient descent to 0 training loss. Neural networks are know to be highly non-convex objects and so understanding their convergence under training is highly non-trivial.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://jackhmiller.github.io/My-DS-Blog/images/ntk.png" /><media:content medium="image" url="https://jackhmiller.github.io/My-DS-Blog/images/ntk.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Alternating Method of Multipliers- Theory and Industry Example Application</title><link href="https://jackhmiller.github.io/My-DS-Blog/2020/12/17/ADMM.html" rel="alternate" type="text/html" title="Alternating Method of Multipliers- Theory and Industry Example Application" /><published>2020-12-17T00:00:00-06:00</published><updated>2020-12-17T00:00:00-06:00</updated><id>https://jackhmiller.github.io/My-DS-Blog/2020/12/17/ADMM</id><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://jackhmiller.github.io/My-DS-Blog/images/convex_opt.png" /><media:content medium="image" url="https://jackhmiller.github.io/My-DS-Blog/images/convex_opt.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Probabilistic Generative Models- Normalizing Flows</title><link href="https://jackhmiller.github.io/My-DS-Blog/2020/10/28/norm-flows.html" rel="alternate" type="text/html" title="Probabilistic Generative Models- Normalizing Flows" /><published>2020-10-28T00:00:00-05:00</published><updated>2020-10-28T00:00:00-05:00</updated><id>https://jackhmiller.github.io/My-DS-Blog/2020/10/28/norm-flows</id><author><name></name></author><category term="generative_model, normalizing_flows" /><summary type="html">Introduction Both GAN and VAE lack the exact evaluation and inference of the probability distribution, which often results in low-quality blur results in VAEs and challenging GAN training in GANs with challenges such as mode collapse and vanishing gradients posterior collapse, etc. VAEs specifically can sometimes suffer from blurry or incomplete reconstructions due to the limitations of the variational inference framework. This is because due to the complexity of real-world data distributions, the mapping from the latent space to the data space may not cover the entire data space, leading to incomplete coverage and potentially missing regions of the target distribution.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://jackhmiller.github.io/My-DS-Blog/images/norm_flow.png" /><media:content medium="image" url="https://jackhmiller.github.io/My-DS-Blog/images/norm_flow.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>